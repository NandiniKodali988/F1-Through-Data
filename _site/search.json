[
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Report",
    "section": "",
    "text": "Introduction\nFormula 1 (F1) is a sport that combine cutting-edge technology, strategic decision-making, and driver skill to create a dynamic and competitive environment. Every race involves millions of data points – ranging from tire temperatures to lap times – that influence the outcomes. This project aims to explore how data can be utilized to uncover hidden insights into team and driver performance, pit stop strategies, and race dynamics. The goal is to simplify complex analysis into actionable findings that highlight the factors driving success in F1 racing.\n\n\nObjective\nThe project aims to answer key questions about race performance and strategies:\n\nHow does a driver’s performance evolve when they switch teams, and what role do constructors play in shaping their success?\nCan patterns or trends in pit stop data reveal insights into team strategies and race outcomes?\nCan data-driven models predict race outcomes, such as point finishes, podium placements, or race wins, based on race and pit stop data?\nWhat influence do pit stop duration, frequency, and timing have on overall race success for drivers and teams?\nAre certain circuit types, like high-speed tracks or technical layouts, better suited to specific constructors?\n\n\n\nKey Findings\n\nImpact of Team Performance on Driver’s Success\n\n  \nA significant portion of the total championship points for most drivers has been earned while driving for top-performing teams such as Ferrari, Red Bull, and Mercedes. This highlights the crucial role that team performance plays in a driver’s success. A clear correlation is observed between the points scored in a season and the team a driver represents, emphasizing the importance of car performance, engineering excellence, and team strategy.\nAnalyzing the career trajectories of drivers like Daniel Ricciardo, Kimi Räikkönen, and Sebastian Vettel, a discernible pattern emerges: they initially excelled while driving for championship-winning teams, followed by moves to midfield teams later in their careers, often before retirement. This transition reflects the competitive nature of Formula 1 and the significance of being part of a top-performing constructor.\nAdditionally, some drivers switched teams despite achieving substantial championship points in the preceding season. This observation underscores the intense competition for seats in Formula 1, where only 20 positions are available across the grid. The combination of driver performance, team expectations, and future potential dictates these decisions, reinforcing the high-stakes environment of the sport.\n\nCyclical Nature of Team Success in Formula 1\n\n\n\n\nTeam Performance\n\n\nThe analysis of constructor performance reveals clear periods of highs and lows, emphasizing the dynamic and cyclical nature of Formula 1. Success in the sport is not guaranteed and often alternates between periods of dominance and rebuilding phases, driven by factors such as technical regulations, innovation, and strategic decisions.\n\nRed Bull Racing: A rapid rise beginning in 2009 coincided with their dominance during the Sebastian Vettel era (2010–2013), where they secured multiple championships. Following a slight dip between 2014 and 2020, Red Bull has experienced another significant upward trajectory, led by Max Verstappen in recent years.\nMercedes: Entering the sport as a constructor in 2010, Mercedes rose to dominance during the hybrid engine era starting in 2014. Their decade-long supremacy was marked by exceptional car performance and driver consistency. However, a decline observed post-2021 coincides with Red Bull’s resurgence and the challenges of adapting to new regulation changes.\nFerrari: Ferrari’s performance has been marked by noticeable fluctuations. A steep decline in 2020, attributed to car performance struggles, was followed by a sharp recovery in 2022, signaling their return to competitiveness and underscoring their resilience as a historic team.\nMcLaren: While McLaren achieved strong results in the early 2000s, their performance significantly declined after 2012, particularly during the hybrid era. However, a gradual recovery since 2018 reflects significant improvements in car design, team operations, and overall strategy.\n\nThis analysis highlights that Formula 1 success is not solely determined by driver skill. Instead, it hinges on a team’s ability to innovate, adapt, and build competitive cars under constantly evolving technical regulations.\n\nMinimizing Failures for Consistent Success\n\nOne of the key differentiating factors between top-performing teams and those struggling for competitiveness in Formula 1 lies in their ability to minimize mechanical failures and accidents, ensuring both driver performance and vehicle reliability contribute to consistent results.\n\nFerrari serves as an excellent example of a top-performing team, with 77.9% of their races completed successfully, a low 6.0% attributed to accidents, and 11.9% due to mechanical failures. This highlights Ferrari’s strong engineering capabilities and their ability to ensure reliability across seasons.\nOn the other hand, teams like Haas face significant challenges, completing only 27.8% of their races successfully, while experiencing 10.1% accidents and 16.5% mechanical failures. Such high failure rates indicate gaps in vehicle reliability and race execution, which hinder their ability to compete consistently.\nWilliams, historically a strong team, shows mixed results. While they completed 47.2% of their races successfully, they recorded 11.4% accidents and 13.1% mechanical failures. Additionally, a notable 28.4% of their races ended with drivers being lapped, emphasizing struggles with car performance and speed relative to competitors.\n\nThese findings demonstrate that success in Formula 1 is a balance of driver skill, strategic decision-making, and, crucially, vehicle reliability. Teams with lower rates of accidents and failures, like Ferrari, gain a competitive edge, whereas higher failure rates, as seen with Haas and Williams, highlight the difficulties faced by midfield and lower-tier teams in maintaining consistency and achieving race finishes.\n\nRelationship Between Pit Stop Data and Race Results\n\nAn important insight derived from the analysis is the relationship between pit stop data and race results. While pit stop duration, frequency, and timing are crucial factors that influence whether a driver scores points, the data reveals that it is challenging to determine the exact finishing position solely based on pit stop variables.\n\nPredicting Pit Stop Duration Using Race Variables\n\nThe analysis demonstrates that pit stop duration can be effectively predicted using variables such as lap time, pit stop number, and lap number. These features provide meaningful insights into the time efficiency of pit stops, allowing teams to optimize their strategy during the race. Accurate predictions of pit stop duration can further help teams reduce time loss, improve competitiveness, and ensure that drivers return to the track with minimal delays.\n\n\nConclusion\nThis project provides a comprehensive analysis of Formula 1 data, exploring the interplay between driver performance, team strategy, circuit characteristics, and pit stop efficiency. By leveraging Exploratory Data Analysis (EDA) and machine learning techniques, we uncovered critical insights into the factors influencing success in Formula 1 racing.\nKey findings emphasize that a driver’s success is heavily influenced by the team they represent, with top-performing teams such as Ferrari, Mercedes, and Red Bull enabling drivers to secure the majority of championship points. The analysis also highlights the cyclical nature of team success, with periods of dominance followed by rebuilding phases, reflecting the dynamic and evolving technical landscape of the sport.\nReliability, in terms of minimizing mechanical failures and accidents, emerged as a critical factor distinguishing top-performing teams from those struggling for competitiveness. Teams with higher completion rates and fewer incidents, such as Ferrari, showcase the importance of engineering excellence and race execution in achieving consistent results.\nThe analysis of pit stop data demonstrated its importance in determining race outcomes, revealing that while pit stops significantly influence whether a driver scores points, predicting the exact finishing position remains challenging. Additionally, we successfully showcased how pit stop duration can be modeled using race variables such as lap times and stop numbers, providing actionable insights for optimizing pit stop strategies.\n\n\nFuture Scope\nI aim to further analyze team win percentages across the different clusters of race tracks formed based on their features. By grouping tracks with similar characteristics (e.g., number of corners, straights, or full-throttle percentages), this analysis can uncover patterns that highlight which teams perform best under specific circuit conditions. Such insights can help teams tailor their strategies to maximize performance on different track types, ultimately contributing to more effective decision-making and improved race results.\nThis project serves as a foundation for deeper data-driven exploration of Formula 1, where the combination of technology, strategy, and human performance continues to define success in this high-stakes sport."
  },
  {
    "objectID": "aboutme/aboutme.html",
    "href": "aboutme/aboutme.html",
    "title": "About Me",
    "section": "",
    "text": "Nandini Kodali\n\nWith a strong foundation in mathematics and computation, I bring an analytical mindset to solving complex data problems.\nI’m currently pursuing a Master’s in Data Science and Analytics at Georgetown University, after my BTech in Computation and Mathematics at Mahindra University, India.\nMy early interest in business analytics sparked my journey into data science, where I’ve since developed skills across the full pipeline — from building ETL pipelines and developing predictive models to designing impactful visualizations. I’ve worked on several machine learning and deep learning projects, and my academic path has led me to explore advanced domains like computer vision, generative image modeling, and natural language processing.\nI’m now focused on applying this technical skillset across diverse areas, leveraging both statistical rigor and creative experimentation.\n\n\n  GitHub    Website"
  },
  {
    "objectID": "technical-details/unsupervised-learning/UnsupervisedLearning.html",
    "href": "technical-details/unsupervised-learning/UnsupervisedLearning.html",
    "title": "Introduction",
    "section": "",
    "text": "Unsupervised Learning is a branch of Machine Learning that involves using algorithms to analyze unlabeled data. Unlike supervised learning, which relies on input-output pairs, unsupervised learning does not have a predefined outcomes to predict. Instead, it focuses on discovering hidden patterns, clusters, or structures within the data without explicit instructions.\nUnsupervised Learning Algorithms:\n\nClustering: Grouping data based on their similarity\nDimensionality Reduction: reducing the number of features in the dataset while preserving key information.\nAssociation Rule Learning: Identifying relationships between variables in a dataset. In this project, Dimensionality reduction and Clustering algorithms are used, which will be explored in detail in their respective section.\n\nImportance\n\nUnsupervised learning algorithms can identify natural groupings in the data, revealing insights that might not be obvious to humans.\nTechniques like dimensionality reduction simplify data by reducing its features while retaining important information, making it easier to visualize and analyze.\nLabeling data could be a costly process, unsupervised learning provides a scalable approach to uncover patterns without requiring human intervention.\n\nIn this project:\n\n\nDimensionality reduction is applied to pit stop data to simplify the dataset and enhance the interpretability of subsequent analysis.\nClustering is employed to:\n\nAnalyze pit stop strategies and uncover potential groupings.\nGroup circuits based on their features."
  },
  {
    "objectID": "technical-details/unsupervised-learning/UnsupervisedLearning.html#principal-component-analysis-pca",
    "href": "technical-details/unsupervised-learning/UnsupervisedLearning.html#principal-component-analysis-pca",
    "title": "Introduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPCA transforms the data into a set of orthogonal components that capture the maximum variance in the data.\nProcess:\n\n\nThe data is standardized to ensure a consistent scale for all variables.\nCovariance matrix is computed to explore the dependencies between relationships.\nThe eigenvectors determine the directions (Principal Components), and the eigen values indicate the amount of variance captured.\nBased on the dimension, we want to reduce the data to, we select the eigenvectors that capture the most information."
  },
  {
    "objectID": "technical-details/supervised-learning/SupervisedLearning.html",
    "href": "technical-details/supervised-learning/SupervisedLearning.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nSupervised Learning is a branch of Machine Learning where models are trained on labeled data. The primary goal of supervised learning is to learn a mapping function that can predict outputs for unseen data accurately. This approach relies on historical data to identify patterns and relationships, enabling it to generalize well on new, unseen data.\nProcess:\n\nThe model is provided with input-output pairs from historical data. Each data point consists of:\n\nFeatures (X): Independent variables or predictors.\nLabels (Y): Target outputs or dependent variables.\n\nThe model learns the relationship between the input features and corresponding outputs using a loss function that measures prediction error. The goal is to minimize this error.\nOnce trained, the model can predict outputs for new inputs.\nPerformance is evaluated using metrics like accuracy, precision, recall, F1-score for classification tasks, or RMSE, MAE for regression tasks.\n\nTypes of Supervised Learning\n\nClassification:\n\n\nThe goal is to categorize inputs into discrete classes or categories.\nAlgorithms: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forest, Naive Bayes, K-Nearest Neighbors (KNN).\n\n\nRegression:\n\n\nThe goal is to predict a continuous output based on input features.\nAlgorithms: Linear Regression, Polynomial Regression, Ridge and Lasso Regression, Support Vector Regression (SVR).\n\nChallenges:\n\nSupervised learning requires large volumes of labeled data, which can be expensive and time-consuming to obtain.\nModels may memorize training data instead of generalizing to new inputs. Regularization and cross-validation techniques help mitigate this.\nIn classification problems, imbalanced classes can affect performance.\n\nIn this project:\nSupervised Learning Algorithms are applied to classify race results and predict pit stop duration."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/EDA.html",
    "href": "technical-details/eda/EDA.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The primary goal of Exploratory Data Analysis (EDA) is to gain a comprehensive understanding of the data, its quality, identify any underlying trends or relationships that may influence the analysis and modeling process.\nImportance:\n\nEDA helps detect outliers, and inconsistencies in the data, addressing these issues ensures that the analysis is based on accurate and clean data.\nThrough visualizations and statistical analysis, EDA identifies which features are most relevant to the questions and should be included in the model.\nEDA reveals relationships between variables, such as correlations, and trends, which can help discard certain features or create new derived features.\nBy analyzing the distribution of variables, we can identify central tendencies and variations. This helps us understand how the data is spread out and it it meets assumptions required by certain algorithms.\n\nTechniques:\n\nUnivariate Analysis:\n\nTo understand the distribution, and central tendency of a variable.\nVisualizations: Histograms, Box Plots, kernel Density plots.\nStatistical measures: Mean, Median, Mode, Variance, Standard Deviation.\n\nBivariate Analysis:\n\nTo understand the relationship between two variables\nCategorical-Categorical: Heatmaps.\nNumerical-Categorical: Box plots, Bar Graphs.\nNumerical-Numerical: Scatter plots, correlation coefficients.\n\nMultivariate Analysis:\n\nTo explore relationships between three or more variables simultaneously.\nVisualizations: Heatmaps, Scatter plots.\nDimensionality Reduction: PCA, t-SNE.\n\nData Distribution:\n\nHistograms and density plots are useful to detect skewness or multimodal distributions.\n\nStatistical Analysis:\n\nHypothesis Testing: t-tests, chi-square tests, ANOVA.\nCorrelation: Pearson, Spearman correlation coefficients\nSummary Statistics: Mean, variance, Quartiles.\n\n\nIn this project:\nThere are various datasets, and mutiple independent features such as: Driver Performance, Pit stop durations, race track features, overall performance of the car, which play an important role in determining the outcome of the race. Therefore, EDA is crucial to understand ow these factors influence race results."
  },
  {
    "objectID": "technical-details/eda/EDA.html#introduction",
    "href": "technical-details/eda/EDA.html#introduction",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The primary goal of Exploratory Data Analysis (EDA) is to gain a comprehensive understanding of the data, its quality, identify any underlying trends or relationships that may influence the analysis and modeling process.\nImportance:\n\nEDA helps detect outliers, and inconsistencies in the data, addressing these issues ensures that the analysis is based on accurate and clean data.\nThrough visualizations and statistical analysis, EDA identifies which features are most relevant to the questions and should be included in the model.\nEDA reveals relationships between variables, such as correlations, and trends, which can help discard certain features or create new derived features.\nBy analyzing the distribution of variables, we can identify central tendencies and variations. This helps us understand how the data is spread out and it it meets assumptions required by certain algorithms.\n\nTechniques:\n\nUnivariate Analysis:\n\nTo understand the distribution, and central tendency of a variable.\nVisualizations: Histograms, Box Plots, kernel Density plots.\nStatistical measures: Mean, Median, Mode, Variance, Standard Deviation.\n\nBivariate Analysis:\n\nTo understand the relationship between two variables\nCategorical-Categorical: Heatmaps.\nNumerical-Categorical: Box plots, Bar Graphs.\nNumerical-Numerical: Scatter plots, correlation coefficients.\n\nMultivariate Analysis:\n\nTo explore relationships between three or more variables simultaneously.\nVisualizations: Heatmaps, Scatter plots.\nDimensionality Reduction: PCA, t-SNE.\n\nData Distribution:\n\nHistograms and density plots are useful to detect skewness or multimodal distributions.\n\nStatistical Analysis:\n\nHypothesis Testing: t-tests, chi-square tests, ANOVA.\nCorrelation: Pearson, Spearman correlation coefficients\nSummary Statistics: Mean, variance, Quartiles.\n\n\nIn this project:\nThere are various datasets, and mutiple independent features such as: Driver Performance, Pit stop durations, race track features, overall performance of the car, which play an important role in determining the outcome of the race. Therefore, EDA is crucial to understand ow these factors influence race results."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\n\n\n\n\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised Learning is a branch of Machine Learning that involves using algorithms to analyze unlabeled data. Unlike supervised learning, which relies on input-output pairs, unsupervised learning does not have a predefined outcomes to predict. Instead, it focuses on discovering hidden patterns, clusters, or structures within the data without explicit instructions.\nUnsupervised Learning Algorithms:\n\nClustering: Grouping data based on their similarity\nDimensionality Reduction: reducing the number of features in the dataset while preserving key information.\nAssociation Rule Learning: Identifying relationships between variables in a dataset. In this project, Dimensionality reduction and Clustering algorithms are used, which will be explored in detail in their respective section.\n\nImportance\n\nUnsupervised learning algorithms can identify natural groupings in the data, revealing insights that might not be obvious to humans.\nTechniques like dimensionality reduction simplify data by reducing its features while retaining important information, making it easier to visualize and analyze.\nLabeling data could be a costly process, unsupervised learning provides a scalable approach to uncover patterns without requiring human intervention.\n\nIn this project:\n\n\nDimensionality reduction is applied to pit stop data to simplify the dataset and enhance the interpretability of subsequent analysis.\nClustering is employed to:\n\nAnalyze pit stop strategies and uncover potential groupings.\nGroup circuits based on their features."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#principal-component-analysis-pca",
    "href": "technical-details/unsupervised-learning/main.html#principal-component-analysis-pca",
    "title": "Unsupervised Learning",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPCA transforms the data into a set of orthogonal components that capture the maximum variance in the data.\nProcess:\n\n\nThe data is standardized to ensure a consistent scale for all variables.\nCovariance matrix is computed to explore the dependencies between relationships.\nThe eigenvectors determine the directions (Principal Components), and the eigen values indicate the amount of variance captured.\nBased on the dimension, we want to reduce the data to, we select the eigenvectors that capture the most information.\n\n\n\nCode\n# import required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.cluster import Birch, KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster, set_link_color_palette\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n\n\n\n\nCode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n\n\nCode\ndf_transformed = pd.read_csv(\"../../data/processed-data/pitstop.csv\")\n\n\n\n\nCode\ndf_transformed.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nYear\nRound\nRaceName\nDriverID\nLap1\nLap2\nLap3\nLap4\nLap5\n...\nTime5\nTime6\nTime7\nDuration1\nDuration2\nDuration3\nDuration4\nDuration5\nDuration6\nDuration7\n\n\n\n\n0\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n0.000000\n0.229730\n0.479452\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.453661\n0.428042\n0.457423\n0.000000\n0.0\n0.0\n0.0\n\n\n1\n1\n2011\n1\nAustralian Grand Prix\nalonso\n0.174603\n0.364865\n0.575342\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.392151\n0.432766\n0.419802\n0.000000\n0.0\n0.0\n0.0\n\n\n2\n2\n2011\n1\nAustralian Grand Prix\nambrosio\n0.206349\n0.513514\n0.000000\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.426017\n0.462739\n0.000000\n0.000000\n0.0\n0.0\n0.0\n\n\n3\n3\n2011\n1\nAustralian Grand Prix\nbarrichello\n0.190476\n0.310811\n0.383562\n0.512821\n0.0\n...\n0.0\n0.0\n0.0\n0.398762\n0.662386\n0.293259\n0.469108\n0.0\n0.0\n0.0\n\n\n4\n4\n2011\n1\nAustralian Grand Prix\nbuemi\n0.222222\n0.391892\n0.000000\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.427417\n0.404192\n0.000000\n0.000000\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 33 columns\n\n\n\n\n\nCode\n# features to be used for PCA\nfeatures = [\n    \"Lap1\", \"Lap2\", \"Lap3\", \"Lap4\", \"Lap5\", \"Lap6\", \"Lap7\",\n    \"Stop1\", \"Stop2\", \"Stop3\", \"Stop4\", \"Stop5\", \"Stop6\", \"Stop7\",\n    \"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\",\n    \"Duration1\", \"Duration2\", \"Duration3\", \"Duration4\", \"Duration5\", \"Duration6\", \"Duration7\"\n]\n# extract the features from the dataset\nX = df_transformed[features]\n\n\n\n\n\nCode\n# apply PCA\npca = PCA(n_components=28)\n# transofrm the data to the PCA space\nX_pca = pca.fit_transform(X)\n\n# Explained Variance Ratio for each PC\nexplained_variance_ratio = pca.explained_variance_ratio_\n# cummulative explained variance\ncumulative_explained_variance = np.cumsum(explained_variance_ratio)\n\n\n\n\nCode\n# Plot Explained Variance\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', label='Explained Variance')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Number of Components', fontsize=14)\nplt.ylabel('Explained Variance Ratio', fontsize=14)\nplt.title('Explained Variance by Each Component', fontsize=14)\nplt.grid(True)\nplt.show()\n\n# Plot Cumulative Explained Variance\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', label='Cumulative Variance')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Number of Components', fontsize=14)\nplt.ylabel('Cumulative Explained Variance', fontsize=14)\nplt.title('Cumulative Explained Variance', fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplained variance Plot:\n\nDisplays the amount of information (variance) each component captures from the original dataset.\nHigher variance indicates that the component captures more informations - making it more valuable for dimensionality reduction.\n\nCummulative Explained Variance:\n\nDisplays the cummulative sum of explained variance as each component is added.\nHelps us understand how many components are required to reach a desired variance threshold.\n\nInterpretation:\n\nBoth the plots are used to decide the number of components needed to capture the desired percentage of the total variance.\nWe can decide on the number of components based on the elbow of the plot or by setting a specific variance threshold we want to achieve.\nBy reducing dimensions to this optimal number, we can simplify the dataset while preserving most the original information, improiving computational.\n\n\n\nCode\n# Plot PCA results in 2D space using the first 2 principal components\nplt.figure(figsize=(8, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"PCA Results\", fontsize=14)    \nplt.xlabel(\"PC1\", fontsize=14)\nplt.ylabel(\"PC2\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nThe PCA scatter plot shows the projection of the high-dimensional (28 features) onto th first 2 principal components.\nInterpretation:\n\nThe plot reveals clear clusters in the data, indicating that there are distinct patterns or groups among pit stop data.\nSince the features include lap numbers, pit stop counts, durations, and times, the clustering likely reflects drivers with similar pit stop strategies or lap time behavior. Difference in pit stop durations, frequency, and lap performance might have contributed to the visible groupings.\nThis PCA plot validates that the first two components effectively reduce the complexity of the data while preserving its structure.\n\nNext Steps:\nUse Clustering algorithms to formally identify groups in the lower dimensional space, these clusters can further be compared to the actual race outcomes – finish positions."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#t-sne",
    "href": "technical-details/unsupervised-learning/main.html#t-sne",
    "title": "Unsupervised Learning",
    "section": "t-SNE",
    "text": "t-SNE\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised machine learning technique designed for non-linear dimensionality reduction, often used for visualizing high-dimensional data in lower dimensions.\nProcess:\n\nt-SNE begins by computing the pairwise similarities between data points in the original high-dimensional space. These similarities are modeled using probability distributions.\nTo preserve these relationships in the lower-dimensional space (e.g., 2D or 3D), t-SNE represents pairwise similarities using the Student-t distribution. This distribution, known for its heavier tails compared to Gaussian distributions, ensures that dissimilar points are pushed farther apart while similar points remain clustered together.\nThe algorithm aims to minimize the Kullback-Leibler (KL) divergence, a measure of difference between two probability distributions, ensuring the lower-dimensional embedding closely reflects the structure of the original space.\n\nHyperparameter: Perplexity\nPerplexity controls the balance between local and global relationships in the data. Lower perplexity values emphasize local patterns (small clusters), whereas higher perplexity values account for broader structures.\n\n\nCode\n# Apply t-SNE\n\n# n_components=2 = reduces the data to 2D space\n# random_state=123 = for reproducibility\n# perplexity=10 = hyperparameter - controls the number of neighbours considered\n# it influences the balance between local and gloabl structure preservation\ntsne = TSNE(n_components=2, random_state=123, perplexity=10)\n\n# apply t-SNE transformation\nX_tsne = tsne.fit_transform(X)\n\n# Plot t-SNE results\nplt.figure(figsize=(10, 7))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"t-SNE Results (perplexity = 10)\", fontsize=14)\nplt.xlabel(\"t-SNE-1\", fontsize=14)\nplt.ylabel(\"t-SNE-2\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# higher perplexity\ntsne = TSNE(n_components=2, random_state=123, perplexity=30)\nX_tsne = tsne.fit_transform(X)\n\n# Plot t-SNE results\n# Plot t-SNE results\nplt.figure(figsize=(10, 7))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"t-SNE Results (perplexity = 30)\", fontsize=14)\nplt.xlabel(\"t-SNE-1\", fontsize=14)\nplt.ylabel(\"t-SNE-2\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# higher preplexity\ntsne = TSNE(n_components=2, random_state=123, perplexity=50)\nX_tsne = tsne.fit_transform(X)\n\n# Plot t-SNE results\nplt.figure(figsize=(10, 7))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"t-SNE Results (perplexity = 50)\", fontsize=14)\nplt.xlabel(\"t-SNE-1\", fontsize=14)\nplt.ylabel(\"t-SNE-2\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation:\n\nPerplexity = 10:\n\n\nAt a lower perplexity, the data is more dispersed with distinct local clusters.\nSmaller groups of points are clearly formed, but there are scattered points and fewer global patterns.\n\n\nPerplexity = 30:\n\n\nSeveral well-defined clusters with clear separation between them.\nThe structure of the data is more interpretable as it highlights both local and global trends.\n\n\nPerplexity = 50:\n\n\nAt a higher perplexity, the algorithm focuses more on global relationships in the data. There is a smoother, broader organization of the data, but fine-grained patters are less visible.\n\nInsights:\n\nThe variation in perplexity directly affects the way the algorithm balances local and global relationships.\nThe presence of clearly separated clusters across all three plots suggests that the features contain inherent groupings."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#pca-vs-t-sne",
    "href": "technical-details/unsupervised-learning/main.html#pca-vs-t-sne",
    "title": "Unsupervised Learning",
    "section": "PCA vs t-SNE",
    "text": "PCA vs t-SNE\nPCA is particularly effective at preserving the overall global structure of the data, highlighting large-scale patterns based on variance across features. However, because it relies on linear projections, PCA struggles to capture complex or non-linear relationships within the data.\nt-SNE is well-suited for capturing the local structure of data. Unlike PCA, it is a non-linear technique that identifies clusters and relationships that may not be apparent through linear methods.\nIn summary, PCA and t-SNE are complementary techniques. PCA is ideal for global variance-based analysis and dimensionality reduction, while t-SNE is better for exploring hidden local structures and clusters. Together, they provide a robust approach for understanding complex datasets."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#introduction-1",
    "href": "technical-details/unsupervised-learning/main.html#introduction-1",
    "title": "Unsupervised Learning",
    "section": "Introduction",
    "text": "Introduction\nClustering techniques identify hidden patterns or natural groupings within data, providing critical insights without requiring labeled information. It is widely applied in data analysis to explore relationships, reduce dimensionality, and uncover actionable insights.\nClusters are characterized by two main properties:\nCohesion:\nMeasures how closely related data points are within the same cluster. High cohesion indicates that the points within a cluster are similar to each other. Typically evaluated using intra-cluster distances, where a lower distance implies higher cohesion.\nSeparation:\nMeasures how distinct or well-separated a cluster is from other clusters. High separation indicates clear boundaries between clusters. Typically evaluated using inter-cluster distances, where larger distances signify better separation.\nTypes of Clustering Methods\n\nDistance-Based Clustering\n\nRelies on distance measures (e.g., Euclidean distance) to form clusters.\nK-Means Clustering: Partitions data into k clusters by minimizing the variance (intra-cluster distance) within each cluster.Efficient for large datasets but assumes convex-shaped clusters and requires the number of clusters (k) in advance.\nHierarchical Clustering: Builds a hierarchy of clusters represented as a tree or dendrogram.\n\nAgglomerative (bottom-up): Starts with individual points and merges clusters iteratively.\nDivisive (top-down): Starts with one cluster and splits it iteratively.\n\n\nDensity-Based Clustering\n\nGroups points based on areas of high point density and identifies sparse regions as noise (outliers).\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups closely packed points into clusters while marking points in low-density regions as noise. Does not require specifying the number of clusters and works well for arbitrarily shaped clusters.\nOPTICS (Ordering Points To Identify the Clustering Structure): Similar to DBSCAN but can handle clusters of varying densities.\n\nModel-Based Clustering\n\nAssumes that data is generated from an underlying probabilistic model, often involving distributions.\nGaussian Mixture Models (GMM): Assumes data is generated from a mixture of several Gaussian distributions with unknown parameters. Clusters are modeled as probabilistic regions based on the Gaussian components.\n\nGrid-Based Clustering\n\nDivides the data space into a grid structure and performs clustering on the grid cells.\nCLIQUE (Clustering In QUEst): Efficiently handles high-dimensional data by partitioning the data space into a grid and identifying dense regions.\n\nSpectral Clustering\n\nUses graph theory and the eigenvalues of a similarity matrix to transform data into a lower-dimensional space.\nClustering algorithms like K-Means are then applied in this reduced space.\nEffective for non-convex and complex-shaped clusters.\n\n\nEvaluation Metrics\n\nInertia (Within-Cluster Sum of Squares): Measures the compactness of clusters by summing the squared distances between points and their cluster centroids. Lower inertia indicates higher cohesion.\nSilhouette Score: Measures how well each data point fits within its assigned cluster versus how far it is from neighboring clusters. The silhouette score ranges from -1 to 1, where higher values indicate better-defined clusters.\nDavies-Bouldin Index: Evaluates both intra-cluster distances (cohesion) and inter-cluster distances (separation). A lower Davies-Bouldin Index indicates better clustering.\nCalinski-Harabasz Index: Measures the ratio of inter-cluster separation to intra-cluster cohesion. Higher values indicate better clustering results."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#kmeans",
    "href": "technical-details/unsupervised-learning/main.html#kmeans",
    "title": "Unsupervised Learning",
    "section": "KMeans",
    "text": "KMeans\nApplied to the data reduced to 2D space using the PCA algorithm.\nKMeans is a distance-based unsupervised clustering algorithm that partitions a dataset into kkk clusters by minimizing the intra-cluster variance, also known as inertia. The algorithm iteratively refines cluster centroids to find an optimal partition of the data. KMeans aims to ensure that data points within a cluster are as close as possible to the centroid, while centroids of different clusters are as far apart as possible.\nProcess:\n\nInitialize k centroids randomly.\nAssign each data point to the nearest centroid based on a distance metric, typically Euclidean distance.\nRecalculate the centroids as the mean position of all points assigned to each cluster.\nRepeat the assignment and centroid update steps until convergence, i.e., when centroids no longer change significantly or a predefined number of iterations is reached.\n\nEvaluation Metrics Used\n\nInertia (Within-Cluster Sum of Squares): Measures the compactness of clusters by summing the squared distances between points and their cluster centroids. Lower inertia indicates higher cohesion.\nSilhouette Score: Measures how well each data point fits within its assigned cluster versus how far it is from neighboring clusters. The silhouette score ranges from -1 to 1, where higher values indicate better-defined clusters.\n\n\n\nCode\n# evaluate using elbow method and silhouette score\ndef kmeans_elbow_silhouette(X, cluster_range):\n\n    # initialize lists to store results\n    inertia_scores = []\n    silhouette_scores = []\n\n    # iterate over the range of cluster values\n    for k in cluster_range:\n        # initialize and fit KMeans\n        kmeans = KMeans(n_clusters=k, n_init=10, random_state=5000)\n        labels = kmeans.fit_predict(X)\n\n        # calculate inertia\n        inertia_scores.append(kmeans.inertia_)\n\n        # calculate silhouette score if there are more than 1 cluster\n        if len(set(labels)) &gt; 1:\n            silhouette_scores.append(silhouette_score(X, labels))\n        else:\n            silhouette_scores.append(-1)  # Assign a low score for invalid clusters\n\n    # plot Elbow Curve and Silhouette Score\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n\n    # Elbow Curve - Inertia\n    ax1.plot(cluster_range, inertia_scores, 'blue', marker='o', label='Inertia')\n    ax1.set_xlabel(\"Number of Clusters (k)\", fontsize=14)\n    ax1.set_ylabel(\"Inertia\", color=\"blue\", fontsize=14)\n    ax1.tick_params(axis=\"y\", labelcolor=\"blue\", labelsize=14)\n\n    # Silhouette Score Curve - on the secondary y-axis\n    ax2 = ax1.twinx()\n    ax2.plot(cluster_range, silhouette_scores, 'darkslateblue', label='Silhouette Score', marker='o')\n    ax2.set_ylabel(\"Silhouette Score\", color='darkslateblue', fontsize=14)\n    ax2.tick_params(axis=\"y\", labelcolor='darkslateblue', labelsize=14)\n\n    # Title and grid\n    plt.title(\"Elbow Curve and Silhouette Score vs Number of Clusters\", fontsize=16)\n    plt.grid(True)\n    fig.tight_layout()\n    plt.show()\n\n# range of clusters to evaluate\ncluster_range = range(2, 11)  \nkmeans_elbow_silhouette(X, cluster_range)\n\n\n\n\n\n\n\n\n\nInterpretation\n\nElbow Method:\n\n\nThe Inertia curve (blue line) decreases sharply as the number of clusters increases, indicating that adding clusters reduces intra-cluster distances.\nHowever, the reduction in inertia slows down noticeably around k=4, forming an “elbow” in the curve. This point indicates that increasing the number of clusters beyond k=4 provides diminishing returns in terms of variance reduction.\n\n\nSilhouette Score:\n\n\nThe Silhouette Score curve (dark slate blue) evaluates cluster cohesion and separation. The score peaks at around k=6, reaching approximately 0.72, which is a strong indication of well-defined clusters.\nHowever, the silhouette score for k=4 is also high (~0.70), and beyond k=6, the score declines, suggesting a decrease in clustering quality.\n\nConclusion: - Based on the Elbow Method and Silhouette Score, k=4 is chosen as the optimal number of clusters. - This decision balances minimizing intra-cluster distances (inertia) and maintaining well-separated, cohesive clusters, while ensuring computational efficiency and interpretability.\n\n\nCode\n# kmeans with optimnal k (= 4)\nkmeans = KMeans(n_clusters=4, n_init=10, random_state=123)\nkmeans_labels = kmeans.fit_predict(X_pca)\n\n# Visualize clustering results on PCA-transformed data\ncustom_colors = [\"lightskyblue\", \"cornflowerblue\", \"blue\", \"steelblue\", \"deepskyblue\"]\ncustom_cmap = ListedColormap(custom_colors)\n\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, alpha=0.6, cmap=custom_cmap)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Principal Component 1', fontsize=14)\nplt.ylabel('Principal Component 2', fontsize=14)\nplt.title(f'KMeans Clustering Results Visualized with PCA (k=4)', fontsize=14)\nplt.colorbar(scatter, label=\"Cluster Labels\")\nplt.grid(True)\nplt.show()\n\n# Evaluate silhouette score\nsil_score = silhouette_score(X_pca, kmeans_labels)\nprint(f\"Silhouette Score for KMeans on PCA-transformed Data: {sil_score:.3f}\")\n\n\n\n\n\n\n\n\n\nSilhouette Score for KMeans on PCA-transformed Data: 0.700\n\n\nThe clusters are well-separated, suggesting that KMeans has effectively grouped the data points.\n\n\nCode\n# analysing clusters\ndf_transformed['Cluster'] = kmeans_labels\ndf_transformed.drop(columns=['Unnamed: 0'], inplace=True)\nprint(df_transformed.head())\n\n\n\n   Year  Round               RaceName     DriverID      Lap1      Lap2  \\\n0  2011      1  Australian Grand Prix  alguersuari  0.000000  0.229730   \n1  2011      1  Australian Grand Prix       alonso  0.174603  0.364865   \n2  2011      1  Australian Grand Prix     ambrosio  0.206349  0.513514   \n3  2011      1  Australian Grand Prix  barrichello  0.190476  0.310811   \n4  2011      1  Australian Grand Prix        buemi  0.222222  0.391892   \n\n       Lap3      Lap4  Lap5  Lap6  ...  Time6  Time7  Duration1  Duration2  \\\n0  0.479452  0.000000   0.0   0.0  ...    0.0    0.0   0.453661   0.428042   \n1  0.575342  0.000000   0.0   0.0  ...    0.0    0.0   0.392151   0.432766   \n2  0.000000  0.000000   0.0   0.0  ...    0.0    0.0   0.426017   0.462739   \n3  0.383562  0.512821   0.0   0.0  ...    0.0    0.0   0.398762   0.662386   \n4  0.000000  0.000000   0.0   0.0  ...    0.0    0.0   0.427417   0.404192   \n\n   Duration3  Duration4  Duration5  Duration6  Duration7  Cluster  \n0   0.457423   0.000000        0.0        0.0        0.0        3  \n1   0.419802   0.000000        0.0        0.0        0.0        3  \n2   0.000000   0.000000        0.0        0.0        0.0        0  \n3   0.293259   0.469108        0.0        0.0        0.0        1  \n4   0.000000   0.000000        0.0        0.0        0.0        0  \n\n[5 rows x 33 columns]\n\n\n\n\nCode\n# descriptive statistics - Laps\ncluster_summary = df_transformed.groupby('Cluster').describe()\n\nselected_features = ['Lap1', 'Lap2']\n\nfiltered_summary = cluster_summary[selected_features]\n\nfiltered_summary.T\n\n\n\n\n\n\n\n\n\nCluster\n0\n1\n2\n3\n\n\n\n\nLap1\ncount\n2023.000000\n409.000000\n1673.000000\n1013.000000\n\n\nmean\n0.218472\n0.101409\n0.361332\n0.147025\n\n\nstd\n0.123682\n0.103965\n0.166313\n0.101554\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.142857\n0.015873\n0.253968\n0.063492\n\n\n50%\n0.206349\n0.095238\n0.365079\n0.142857\n\n\n75%\n0.285714\n0.158730\n0.476190\n0.206349\n\n\nmax\n0.841270\n0.698413\n1.000000\n0.555556\n\n\nLap2\ncount\n2023.000000\n409.000000\n1673.000000\n1013.000000\n\n\nmean\n0.486132\n0.238188\n0.000000\n0.345655\n\n\nstd\n0.141812\n0.161187\n0.000000\n0.141362\n\n\nmin\n0.027027\n0.027027\n0.000000\n0.027027\n\n\n25%\n0.405405\n0.108108\n0.000000\n0.256757\n\n\n50%\n0.472973\n0.243243\n0.000000\n0.351351\n\n\n75%\n0.567568\n0.337838\n0.000000\n0.418919\n\n\nmax\n1.000000\n0.837838\n0.000000\n0.851351\n\n\n\n\n\n\n\n\n\nCode\n# descriptive statistics - Duration\ncluster_summary = df_transformed.groupby('Cluster').describe()\n\nselected_features = ['Duration2', 'Duration1']\n\nfiltered_summary = cluster_summary[selected_features]\n\nfiltered_summary.T\n\n\n\n\n\n\n\n\n\nCluster\n0\n1\n2\n3\n\n\n\n\nDuration2\ncount\n2023.000000\n409.000000\n1673.000000\n1013.000000\n\n\nmean\n0.412609\n0.359939\n0.000000\n0.404845\n\n\nstd\n0.107918\n0.152168\n0.000000\n0.116942\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.384980\n0.321482\n0.000000\n0.373274\n\n\n50%\n0.409424\n0.372697\n0.000000\n0.405452\n\n\n75%\n0.440264\n0.429564\n0.000000\n0.442862\n\n\nmax\n1.000000\n0.990551\n0.000000\n0.974559\n\n\nDuration1\ncount\n2023.000000\n409.000000\n1673.000000\n1013.000000\n\n\nmean\n0.405394\n0.381307\n0.420707\n0.387143\n\n\nstd\n0.098226\n0.137291\n0.079428\n0.126414\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.372940\n0.339512\n0.374256\n0.357289\n\n\n50%\n0.397548\n0.380361\n0.402523\n0.391071\n\n\n75%\n0.430698\n0.431769\n0.455364\n0.434923\n\n\nmax\n0.956469\n0.987722\n1.000000\n0.931372"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dbscan",
    "href": "technical-details/unsupervised-learning/main.html#dbscan",
    "title": "Unsupervised Learning",
    "section": "DBSCAN",
    "text": "DBSCAN\nApplied to the data reduced to 2D space using the PCA algorithm\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is a clustering algorithm that identifies clusters based on the density of data points. Unlike K-Means, which requires the number of clusters to be predefined, DBSCAN automatically detects clusters of arbitrary shapes and sizes and can identify outliers (noise) in the data.\nHyperparameters:\n\nEpsilon (ε): The maximum distance between two points for them to be considered as neighbors.\nMinPts: The minimum number of points required to form a dense region (a cluster).\n\nTypes of Points:\n\nCore Points: A point is a core point if it has at least MinPts neighbors within distance ε.\nBorder Points: A point that is within ε of a core point but does not meet the MinPts requirement itself.\nNoise Points: Any point that is neither a core point nor a border point is treated as noise or an outlier. DBSCAN groups points into clusters by iteratively expanding the dense regions (clusters) starting from core points.\n\nKey Characteristics:\n\nDensity-Based: Works well for identifying clusters of arbitrary shapes (e.g., circular, elongated).\nNoise Detection: Automatically identifies outliers as noise, which is useful for datasets with anomalies.\nNo Need to Specify k: Unlike K-Means, DBSCAN does not require the number of clusters as input.\nParameter Sensitivity: The choice of ε and MinPts can significantly impact results and requires tuning.\n\n\n\nCode\ndef optimize_dbscan_with_heatmap(X, eps_range, min_samples_range):\n    best_score = -1\n    best_eps = None\n    best_min_samples = None\n    best_labels = None\n    best_cluster_count = None\n\n    # initialize a dictionary to store silhouette scores for heatmap\n    score_dict = {}\n\n    # loop through each eps value in the range\n    for eps in eps_range:\n        score_dict[eps] = []\n        # loop through min_samples values\n        for min_samples in min_samples_range:\n            # Initialize DBSCAN\n            model = DBSCAN(eps=eps, min_samples=min_samples)\n            labels = model.fit_predict(X)\n\n            # calculate silhouette score if there are more than 1 cluster\n            if len(set(labels)) &gt; 1: \n                score = silhouette_score(X, labels)\n            else:\n                score = -1  \n            # append the score to score dictionary\n            score_dict[eps].append(score)\n\n            # update the score if a better score if found\n            if score &gt; best_score:\n                best_score = score\n                best_eps = eps\n                best_min_samples = min_samples\n                best_labels = labels\n                # count clusters excluding noise\n                best_cluster_count = len(set(labels)) - (1 if -1 in labels else 0)\n\n    # convert the score dictionary to a DataFrame for heatmap\n    score_df = pd.DataFrame(score_dict, index=min_samples_range).T\n\n    # plot the heatmap\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(score_df, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=min_samples_range, yticklabels=eps_range)\n    plt.title(\"Silhouette Score Heatmap for DBSCAN\")\n    plt.xlabel(\"Min Samples\")\n    plt.ylabel(\"Eps\")\n    plt.show()\n\n    # output the optimal parameters\n    print(f\"Best Silhouette Score: {best_score}\")\n    print(f\"Optimal eps: {best_eps}\")\n    print(f\"Optimal min_samples: {best_min_samples}\")\n\n    return best_eps, best_min_samples, best_cluster_count, best_labels\n\neps_range = np.arange(0.1, 1.0, 0.1) \nmin_samples_range = range(2, 10)  \n\nbest_eps, best_min_samples, best_cluster_count, best_labels = optimize_dbscan_with_heatmap(X, eps_range, min_samples_range)\n\n\n\n\n\n\n\n\n\nBest Silhouette Score: 0.7156373392601885\nOptimal eps: 0.6\nOptimal min_samples: 3\n\n\n\n\nCode\n# apply DBCSAN with optimal paramters \ndbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\ndbscan_labels = dbscan.fit_predict(X_pca)\n\ncustom_colors = [\"lightskyblue\", \"cornflowerblue\", \"blue\", \"steelblue\", \"deepskyblue\", \"cyan\"]\ncustom_cmap = ListedColormap(custom_colors)\n\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap=custom_cmap, alpha=0.7)\nplt.title(\"DBSCAN Clustering Results on PCA-transformed Data (2D)\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.colorbar(scatter, label=\"Cluster Labels\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nnum_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\nprint(f\"Number of clusters found: {num_clusters}\")\n\n\nNumber of clusters found: 6"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "href": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical Clustering is an unsupervised machine learning technique used to group data into a hierarchy of clusters. It builds a tree-like structure called a dendrogram, which visually represents the relationships between data points and how clusters are formed step by step.\nTypes:\nAgglomerative Clustering (Bottom-Up):\n\nstarts with each data point as an individual cluster.\nIteratively combines the closest clusters based on a distance metric.\nContinues until all points are merged into a single cluster.\nThe dendrogram can be cut at different levels to obtain the desired number of clusters.\n\nDivisive Clustering (Top-Down):\n\nStarts with all data points in a single cluster.\nIteratively splits clusters into smaller subclusters until each data point becomes its own cluster.\n\nDistance Metric Common metrics include:\n\nEuclidean Distance: Measures the straight-line distance between points.\nManhattan Distance: Measures distance as the sum of absolute differences.\nCosine Distance: Measures the angle between two vectors.\n\nIn this project: Hierarchical clustering is applied to understand groupings within the racetrack features dataset.\n\n\nCode\n# load the data\ndf = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\ndf.drop(columns=['Unnamed: 7'], inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\n\n\n\n\n\n\n\n\n\nCode\nset_link_color_palette([\"lightskyblue\", \"cornflowerblue\", \"blue\", \"steelblue\"])\n\ndf = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\n# only circuits from 2023\ndf = df[df[\"Year\"] == 2023]\ndf.dropna\n\n# features used for clustering\nfeatures = [\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\", \"Number of Corners\", \"Number of Straights\"]\nX = df[features]\n\n# standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ward's linkage method - minimizes variance within clusters\nlinkage_matrix = linkage(X_scaled, method='ward')  \n\nplt.figure(figsize=(10, 7))\ndendrogram(linkage_matrix, labels=df[\"Grand Prix\"].values, leaf_rotation=90, leaf_font_size=10,color_threshold=6)\nplt.title(\"Hierarchical Clustering Dendrogram - 2023\", fontsize = 14)\n\nplt.xlabel(\"Grand Prix\", fontsize = 14)\nplt.ylabel(\"Distance\", fontsize = 14)\nplt.show()\n\nmax_distance = 5\nclusters = fcluster(linkage_matrix, max_distance, criterion='distance')\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# group circuits based on clusters\nmax_distance = 5\nclusters = fcluster(linkage_matrix, max_distance, criterion='distance')\ndf[\"Cluster\"] = clusters\ngrouped_df = df.groupby(\"Cluster\")[features].mean()\ngrouped_df.T\n\n\n\n\n\n\n\n\nCluster\n1\n2\n3\n4\n\n\n\n\nTrack Length (m)\n1.116553\n-0.524224\n0.103169\n-2.269306\n\n\nMax Speed (km/h)\n0.884510\n0.015604\n0.793869\n-2.591115\n\n\nFull Throttle (%)\n0.877347\n0.155178\n-0.538241\n-2.058832\n\n\nNumber of Corners\n-0.360778\n-0.853982\n0.947287\n2.298239\n\n\nNumber of Straights\n-0.788297\n-0.488102\n0.750199\n0.862772"
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The primary goal of Exploratory Data Analysis (EDA) is to gain a comprehensive understanding of the data, its quality, identify any underlying trends or relationships that may influence the analysis and modeling process.\nImportance:\n\nEDA helps detect outliers, and inconsistencies in the data, addressing these issues ensures that the analysis is based on accurate and clean data.\nThrough visualizations and statistical analysis, EDA identifies which features are most relevant to the questions and should be included in the model.\nEDA reveals relationships between variables, such as correlations, and trends, which can help discard certain features or create new derived features.\nBy analyzing the distribution of variables, we can identify central tendencies and variations. This helps us understand how the data is spread out and it it meets assumptions required by certain algorithms.\n\nTechniques:\n\nUnivariate Analysis:\n\nTo understand the distribution, and central tendency of a variable.\nVisualizations: Histograms, Box Plots, kernel Density plots.\nStatistical measures: Mean, Median, Mode, Variance, Standard Deviation.\n\nBivariate Analysis:\n\nTo understand the relationship between two variables\nCategorical-Categorical: Heatmaps.\nNumerical-Categorical: Box plots, Bar Graphs.\nNumerical-Numerical: Scatter plots, correlation coefficients.\n\nMultivariate Analysis:\n\nTo explore relationships between three or more variables simultaneously.\nVisualizations: Heatmaps, Scatter plots.\nDimensionality Reduction: PCA, t-SNE.\n\nData Distribution:\n\nHistograms and density plots are useful to detect skewness or multimodal distributions.\n\nStatistical Analysis:\n\nHypothesis Testing: t-tests, chi-square tests, ANOVA.\nCorrelation: Pearson, Spearman correlation coefficients\nSummary Statistics: Mean, variance, Quartiles.\n\n\nIn this project:\nThere are various datasets, and mutiple independent features such as: Driver Performance, Pit stop durations, race track features, overall performance of the car, which play an important role in determining the outcome of the race. Therefore, EDA is crucial to understand ow these factors influence race results."
  },
  {
    "objectID": "technical-details/eda/main.html#introduction",
    "href": "technical-details/eda/main.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The primary goal of Exploratory Data Analysis (EDA) is to gain a comprehensive understanding of the data, its quality, identify any underlying trends or relationships that may influence the analysis and modeling process.\nImportance:\n\nEDA helps detect outliers, and inconsistencies in the data, addressing these issues ensures that the analysis is based on accurate and clean data.\nThrough visualizations and statistical analysis, EDA identifies which features are most relevant to the questions and should be included in the model.\nEDA reveals relationships between variables, such as correlations, and trends, which can help discard certain features or create new derived features.\nBy analyzing the distribution of variables, we can identify central tendencies and variations. This helps us understand how the data is spread out and it it meets assumptions required by certain algorithms.\n\nTechniques:\n\nUnivariate Analysis:\n\nTo understand the distribution, and central tendency of a variable.\nVisualizations: Histograms, Box Plots, kernel Density plots.\nStatistical measures: Mean, Median, Mode, Variance, Standard Deviation.\n\nBivariate Analysis:\n\nTo understand the relationship between two variables\nCategorical-Categorical: Heatmaps.\nNumerical-Categorical: Box plots, Bar Graphs.\nNumerical-Numerical: Scatter plots, correlation coefficients.\n\nMultivariate Analysis:\n\nTo explore relationships between three or more variables simultaneously.\nVisualizations: Heatmaps, Scatter plots.\nDimensionality Reduction: PCA, t-SNE.\n\nData Distribution:\n\nHistograms and density plots are useful to detect skewness or multimodal distributions.\n\nStatistical Analysis:\n\nHypothesis Testing: t-tests, chi-square tests, ANOVA.\nCorrelation: Pearson, Spearman correlation coefficients\nSummary Statistics: Mean, variance, Quartiles.\n\n\nIn this project:\nThere are various datasets, and mutiple independent features such as: Driver Performance, Pit stop durations, race track features, overall performance of the car, which play an important role in determining the outcome of the race. Therefore, EDA is crucial to understand ow these factors influence race results."
  },
  {
    "objectID": "technical-details/eda/main.html#required-libraries",
    "href": "technical-details/eda/main.html#required-libraries",
    "title": "Exploratory Data Analysis",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n\nCode\n# import required libraries\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport json\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nfrom matplotlib.cm import get_cmap\nimport seaborn as sns\nfrom scipy.stats import ttest_ind"
  },
  {
    "objectID": "technical-details/eda/main.html#news-datalecutre_content",
    "href": "technical-details/eda/main.html#news-datalecutre_content",
    "title": "Exploratory Data Analysis",
    "section": "News Data1",
    "text": "News Data1\nIn this section, we will analyze media coverage of select F1 drivers who created significant buzz during the 2024 season. The drivers include Max Verstappen, Carlos Sainz, Lando Norris, Daniel Riccardo, and Lewis Hamilton. These drivers have been at the center of media discussions due to their performance, contract negotiations, and potential moves for the 2025 season.\n\n\nCode\n# Function to generate a word cloud for a single JSON file\ndef generate_wordcloud_top_n(file_path, output_folder, top_n=10):\n    # helper function to plot and save the word cloud\n    def plot_cloud(wordcloud, output_file):\n        plt.figure(figsize=(10, 8))\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        plt.savefig(output_file) \n\n        plt.close()\n\n    # read the text from the JSON file\n    with open(file_path, 'r', encoding='utf-8') as file:\n        data = json.load(file)\n\n    # concatenate all text content if it's a JSON dictionary\n    if isinstance(data, dict):\n        my_text = ' '.join(data.values())\n\n    # generate word cloud to extract word frequencies\n    wordcloud = WordCloud(\n        width=3000,\n        height=2000,\n        random_state=1,\n        background_color=\"white\",  \n        colormap=\"Blues_r\",  \n        collocations=False,\n        stopwords=STOPWORDS\n    ).generate(my_text)\n\n    # extract word frequencies and get the top N words\n    word_frequencies = wordcloud.words_\n    top_words = dict(list(word_frequencies.items())[:top_n])\n\n    # generate a new word cloud with only the top N words\n    top_wordcloud = WordCloud(\n        width=3000,\n        height=2000,\n        random_state=1,\n        background_color=\"white\",  \n        colormap=\"Blues_r\",\n        collocations=False,\n        stopwords=STOPWORDS\n    ).generate_from_frequencies(top_words)\n\n    # Save and display the word cloud\n    file_name = os.path.basename(file_path).replace('.json', '_wordcloud.png')\n    output_file = os.path.join(output_folder, file_name)\n    plot_cloud(top_wordcloud, output_file)\n\n\n# function to process all JSON files in a folder\ndef generate_wordclouds_for_folder(input_folder, output_folder, top_n=10):\n    os.makedirs(output_folder, exist_ok=True)  \n    # loop through all files in the input folder\n    for file_name in os.listdir(input_folder):\n        # process only JSON files\n        if file_name.endswith('.json'):  \n            file_path = os.path.join(input_folder, file_name)\n            try:\n                generate_wordcloud_top_n(file_path, output_folder, top_n)\n            # for debugging purpose\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n\n\n\n\nCode\ninput_folder = \"../../data/processed-data/News_Drivers/\" \noutput_folder = \"../../data/eda/WordClouds/\"  \ngenerate_wordclouds_for_folder(input_folder, output_folder, top_n=20)\n\n\nAnalysis\nMax Verstappen\n\n\n\n\n\n\n\n\n\n\nStrong occurence of “Norris” suggests the media discussion about the rivalry and cmpetition between the top two drivers.\nThe presence of “penalty” indicates that articles may have covered controversies or penalties during races that involved Verstappen. This adds to the narrative of the challenges and incidents faced during his title run.\nWords such as “title” and “world champion” reinforce the focus on Max Verstappen’s achievements, particularly his dominance throughout the season and his consecutive championship wins."
  },
  {
    "objectID": "technical-details/eda/main.html#drivers-performance",
    "href": "technical-details/eda/main.html#drivers-performance",
    "title": "Exploratory Data Analysis",
    "section": "Drivers’ Performance",
    "text": "Drivers’ Performance\n\n\nCode\n# number of points by driver\ndata = pd.read_csv(\"../../data/processed-data/driver_standings_2000_2023.csv\")\n# Calculate total points for each driver\ntotal_points = data.groupby(\"driverName\")[\"Points\"].sum().reset_index()\n\n# plot only top 10 drivers \ntop_10_drivers = total_points.sort_values(by=\"Points\", ascending=False).head(10)\n\n# Plot the bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.barh(top_10_drivers[\"driverName\"], top_10_drivers[\"Points\"], color=\"deepskyblue\")\n\nfor bar in bars:\n    plt.text(\n        bar.get_width() + 1, \n        bar.get_y() + bar.get_height() / 2,  \n        f'{int(bar.get_width())}',  \n        va='center', fontsize=12, color='black'\n    )\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.xlabel(\"Total Points\", fontsize=14)\nplt.ylabel(\"Driver\", fontsize=14)\nplt.title(\"Total Points per Driver\", fontsize=16)\nplt.gca().invert_yaxis()  \nplt.tight_layout()\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe cumulative points reflect a driver’s career longevity, performance, and overall success in the psport\nLewis Hamilton stands out as the clear leader with 4,630 points, significantly ahead of other drivers. This highlights his dominance in the sport over the years, attributed to his consistent race wins, and career with top teams like McLaren and Mercedes in their eras.\nSebastian Vettel holds the second spot with 3,098 points, his performance in Red Bull and Ferrari might have significant contribution to these numbers, since his early years in Sauber, and Toro Rosso, and his performance in Aston Martin were not as successful.\nMax Verstappen has the third highest cumulative total despite entering the sport relatively later. His rapid accumulation of points in the recent years underscores his dominance in the current era of F1.\n\n\n\nCode\n# points scored by driver over time with different teams\ndef plot_driver_points(data, driver_name, color_list=None):\n\n    # filter the data to include only the specified driver\n    # sort by season\n    driver_data = data[data['driverName'] == driver_name].sort_values(by='Season')\n    # list of unique constructors the driver raced for \n    unique_constructors = driver_data[\"Constructor_Name\"].unique()\n    # assign colors to constructors \n    if color_list:\n        constructor_colors = {\n            constructor: color_list[i % len(color_list)]\n            for i, constructor in enumerate(unique_constructors)\n        }\n    else:\n        constructor_colors = {\n            constructor: plt.cm.tab10(i / len(unique_constructors))\n            for i, constructor in enumerate(unique_constructors)\n        }\n    plt.figure(figsize=(10, 6))\n\n    # line plot\n    for i in range(len(driver_data) - 1):\n        season_start = driver_data.iloc[i]['Season']\n        season_end = driver_data.iloc[i + 1]['Season']\n        points_start = driver_data.iloc[i]['Points']\n        points_end = driver_data.iloc[i + 1]['Points']\n        constructor = driver_data.iloc[i]['Constructor_Name']\n        \n\n        plt.plot(\n            [season_start, season_end],\n            [points_start, points_end],\n            color=constructor_colors[constructor],\n            linewidth=4\n        )\n    \n\n    for _, row in driver_data.iterrows():\n        plt.text(row['Season'], row['Points'], str(row['Points']), fontsize=12, ha='center')\n    \n\n    plt.title(f\"Points Over Seasons for {driver_name}\", fontsize=16)\n    plt.xticks(fontsize = 14, rotation = 90)\n    plt.yticks(fontsize = 14)\n    plt.xlabel(\"Season\", fontsize=14)\n    plt.ylabel(\"Points\", fontsize=14)\n    plt.tight_layout()\n    plt.xticks(range(int(driver_data[\"Season\"].min()), int(driver_data[\"Season\"].max()) + 1))\n\n\n    legend_elements = [\n        mlines.Line2D([], [], color=constructor_colors[constructor], label=constructor, linewidth=3)\n        for constructor in unique_constructors\n    ]\n    plt.legend(handles=legend_elements, loc=\"upper left\", fontsize=14)\n    \n\n    plt.show()\n\ncolor_list = [\"lightskyblue\", \"cornflowerblue\", \"deepskyblue\", \"powderblue\", \"steelblue\", \"lightblue\"]\n\n\n\n\nCode\nplot_driver_points(data, \"Lewis Hamilton\", color_list)\n\n\n\n\n\n\n\n\n\n    \nInsights:\n\nA significant portion of the total championship points for most drivers has been earned while driving for top-performing teams such as Ferrari and Red Bull.\nThere is a clear correlation between the points scored in a season and the team a driver represented, emphasizing the importance of team performance in a driver’s success.\nObserving the career trajectories of drivers like Daniel Riccardo, Kimi Räikkönen, and Sebastian Vettel, we notice a pattern where they initially drove for top teams, followed by a move to midfield teams before retiring.\nSome drivers switched teams despite scoring a substantial number of championship points in the previous season. This highlights the strategic decisions made by teams in planning their driver line-ups and the drivers’ foresights regarding the potential improvements and competitiveness of cars across the grid."
  },
  {
    "objectID": "technical-details/eda/main.html#constructors-performance",
    "href": "technical-details/eda/main.html#constructors-performance",
    "title": "Exploratory Data Analysis",
    "section": "Constructors’ Performance",
    "text": "Constructors’ Performance\n\n\nCode\ntotal_points = data.groupby(\"Constructor_Name\")[\"Points\"].sum().reset_index()\n\n\nconstructors = total_points.sort_values(by=\"Points\", ascending=False).head(10)\n\n# Plot the bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.barh(constructors[\"Constructor_Name\"], constructors[\"Points\"], color=\"deepskyblue\")\n\nfor bar in bars:\n    plt.text(\n        bar.get_width() + 1, \n        bar.get_y() + bar.get_height() / 2,  \n        f'{int(bar.get_width())}',  \n        va='center', fontsize=12, color='black'\n    )\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.xlabel(\"Total Points\", fontsize=14)\nplt.ylabel(\"Constructor\", fontsize=14)\nplt.title(\"Total Points per Constructor\", fontsize=16)\nplt.gca().invert_yaxis()  \nplt.tight_layout()\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe top 3 teams – Ferrari, Mercedes, and Red Bull – are closely positioned in the cumulative points standings.\nMercedes’ position is particularly notable because they re-entered the sports as a constructor in 2010, while Ferrari and Red Bull have accumulated points across the full 2000-2023 period. Despite the sorter timeframe, Mercedes managed to secure the second position, showcasing their unparalleled dominance 2014 onward.\nFrom domain knowledge, a key differentiating factor for Mercedes has been the consistent performance of their drivers, such as Lewis Hamilton, Nico Rosberg, and Valtteri Bottas, who regularly secured podium finishes during their tenure. This level of consistency has allowed Mercedes to close the gap with Ferrari and Red Bull in a relatively short period.\nFor Ferrari and Red Bull, the points reflect their long-standing success and ability to remain competitive across different eras. Ferrari’s consistent presence at the top, coupled with Red Bull’s dominance during Sebastian Vettel’s era (2010-2013) and Max Verstappen’s current reign, has solidified their positions in the top three\n\n\n\nCode\n# distribution of \"Lapped\", \"Mechanical\", \"Accident\" across Constructors\ndata = pd.read_csv(\"../../data/processed-data/race_info.csv\")\n\n\n\n\nCode\n# Filter the data for specific statuses\nfiltered_data = data[data[\"status\"].isin([\"Lapped\", \"Mechanical\", \"Accident\"])]\n\n# Group by constructor and status to get counts\nstatus_distribution = filtered_data.groupby([\"constructorName\", \"status\"]).size().unstack(fill_value=0)\n\n# Plot the distribution as a stacked horizontal bar chart\nstatus_distribution.plot(\n    kind=\"barh\",\n    figsize=(12, 6),\n    color=['steelblue', 'deepskyblue', 'lightskyblue'],  \n    edgecolor=\"black\",\n    stacked=True \n)\n\n# Customize the plot\nplt.title(\"Distribution of 'Lapped', 'Mechanical', and 'Accident' Across Constructors\", fontsize=16)\nplt.xlabel(\"Count\", fontsize=14)\nplt.ylabel(\"Constructor\", fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(title=\"Status\", fontsize=12, loc=\"upper right\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# distrbution of status for each constructor \ndef autopct_format(pct):\n    return f'{pct:.1f}%' if pct &gt; 0 else ''\n\nfiltered_data = data[data[\"status\"].isin([\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"])]\n# Group by constructor and status to get counts\nstatus_distribution = filtered_data.groupby([\"constructorName\", \"status\"]).size().unstack(fill_value=0)\n# total number of entries per constructor\nstatus_distribution[\"Total\"] = data.groupby(\"constructorName\").size()\n# total number of constructors\nnum_constructors = len(status_distribution)\n# for subplot\nnum_cols = 3\nnum_rows = (num_constructors + num_cols - 1) // num_cols \nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n\naxes = axes.flatten()\n\ncolors = ['steelblue', 'deepskyblue', 'lightskyblue', 'powderblue']\n# loop through each constructor and create a pie chart\nfor idx, constructor in enumerate(status_distribution.index):\n    # extract status counts for each constructor\n    status_counts = status_distribution.loc[constructor, [\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"]]\n    total_count = status_distribution.loc[constructor, \"Total\"]\n    # calcuate percentage for each status\n    percentages = (status_counts / total_count) * 100\n\n\n    wedges, texts, autotexts = axes[idx].pie(\n        percentages,\n        labels=None,  \n        autopct=autopct_format,\n        colors=colors,\n        startangle=140,\n        textprops={'fontsize': 14} \n    )\n    axes[idx].set_title(f\"{constructor}\", fontsize=14)\n\n    for autotext in autotexts:\n        autotext.set_fontsize(12)  \n\n\nfor i in range(idx + 1, len(axes)):\n    fig.delaxes(axes[i])\n\n# Add a shared legend\nfig.legend(\n    labels=[\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"],\n    loc=\"lower center\",\n    ncol=1,\n    fontsize=12\n)\n\nplt.tight_layout(rect=[0, 0.1, 1, 1])  \nplt.show()\n\n\n\n\n\n\n\n\n\nInsights:\n\nFerrari stands out with an impressive 77.9% race completion rate. The low percentage of mechanical failures and lapped races further emphasize their ability to build durable and reliable cars. Mercedes also displays as a high completion rate (66.2%) and a low percentage of mechanical failures and accidents. McLaren is a close third with 59.8% finished races, although it has low percentage of accidents, it has comparatively high percentage of lapped and incomplete races due to mechanical failures out of the top 3.\nWilliams, a team with a glorious past, shows 47.2% completion rate and 28.4% lapped races, highlighting the shift in their performance over the years. Having 11.4% accidents indicates that the drivers have not been performing well, 13.1% mechanical failures indicates struggles with car development, limited resources, and their current inability to compete with the top teams.\nHaas F1 Team a relatively new entrant, records 27.8% completion rate, with notable lapped races (45.6%) and mechanical failures. Their results reflect the challenges faced by newer teams with smaller budgets and sponsorships.\nMidfield and smaller teams face significant challenges in maintaining consistency due to resource limitations, budget caps, and the learning curve required to compete at the highest level. These teams often serve as development platforms for young drivers transitioning from F2 to F1 but struggle to compete with top constructors.\n\n\n\nCode\n# trends in teams scoring points across seasons - for \"Red Bull\", \"Ferrari\", \"McLaren\", \"Mercedes\"\ndata = pd.read_csv(\"../../data/processed-data/driver_standings_2000_2023.csv\")\n\nfiltered_data = data[data[\"Constructor_Name\"].isin([\"Red Bull\", \"Ferrari\", \"McLaren\", \"Mercedes\"])]\n\ngrouped_data = filtered_data.groupby([\"Season\", \"Constructor_Name\"])[\"Points\"].sum().unstack()\n\ncolor_list = [\"red\", \"orange\", \"black\", \"deepskyblue\"]\n\nunique_constructors = grouped_data.columns\nconstructor_colors = {constructor: color_list[i % len(color_list)] for i, constructor in enumerate(unique_constructors)}\n\n\nplt.figure(figsize=(12, 6))\nfor constructor in grouped_data.columns:\n    plt.plot(\n        grouped_data.index, \n        grouped_data[constructor], \n        marker='o', \n        label=constructor, \n        color=constructor_colors[constructor]\n    )\n\n\nplt.title(\"Trends in Points for Selected Constructors\", fontsize=16)\nplt.xlabel(\"Season\", fontsize=14)\nplt.ylabel(\"Points\", fontsize=14)\nplt.xticks(grouped_data.index, rotation=90, fontsize=12) \nplt.yticks(fontsize=12)\nplt.legend(title=\"Constructor\", fontsize=12)\nplt.tight_layout()\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\nInsights:\n\nThe plot highlights clear periods of highs and lows for each team, emphasizing the dynamic nature of Formula 1. Success in the sport is cyclical, with teams experiencing periods of dominance followed by rebuilding phases.\nRed Bull showed a rapid rise starting in 2009, coinciding with their dominance during the Sebastian Vettel era (2010–2013). After a slight dip between 2014–2020, they have experienced another significant upward trend, driven by Max Verstappen’s dominance in recent years.\nMercedes entered the sport as a constructor in 2010 and saw rapid progress starting in 2014 during the hybrid engine era, where they dominated for nearly a decade. Their decline starting in 2021 coincides with the resurgence of Red Bull and highlights challenges in adapting to regulation changes.\nFerrari has experienced the most noticeable highs and lows among the teams. A steep decline is observed in 2020, likely due to car performance struggles, followed by a sharp recovery in 2022, marking their return to competitiveness.\nMcLaren had strong results in the early 2000s but faced a significant decline after 2012, struggling during the hybrid era. Their gradual recovery post-2018, reflected in the upward trend, signals improvements in car performance and team structure.\nThis analysis underscores that success in Formula 1 is not just about driver skill but also hinges on the ability of teams to innovate, adapt, and build competitive cars under constantly evolving technical regulations.\n\n\n\nCode\nsns.pairplot(data[['position', 'points', 'laps', 'grid', 'FinishCategory']], hue='FinishCategory', palette={'Podium':'steelblue', 'Points Finish':'lightskyblue', 'No Points': 'dodgerblue'})"
  },
  {
    "objectID": "technical-details/eda/main.html#circuit-features",
    "href": "technical-details/eda/main.html#circuit-features",
    "title": "Exploratory Data Analysis",
    "section": "Circuit Features",
    "text": "Circuit Features\n\n\nCode\ndata = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\n\n\n\n\nCode\n# Full Throttle (%) vs Number of Straights\n\n#  grouping based on the median of 'Number of Straights'\nmedian_straights = data['Number of Straights'].median()\n\n# tracks with fewer straights than median\nlow_straights = data[data['Number of Straights'] &lt; median_straights]['Full Throttle (%)']\n# tracks with &gt;= median straights\nhigh_straights = data[data['Number of Straights'] &gt;= median_straights]['Full Throttle (%)']\n\n# perform t-test\nt_stat, p_value = ttest_ind(low_straights, high_straights, nan_policy='omit')\n\n# resutls\nprint(f\"T-Statistic: {t_stat:.4f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\n\nT-Statistic: 8.7778\nP-Value: 0.0000\n\n\nThere is a significant difference in Full Throttle (%) between tracks with low and high straights.\n\n\nCode\n# Full Throttle (%) vs Number of Corners\n\nmedian_corners = data['Number of Corners'].median()\n\n# tracks with lower number of corners than median\nlow_corners = data[data['Number of Corners'] &lt; median_corners]['Full Throttle (%)']\n# tracks with &gt;= median corners\nhigh_corners = data[data['Number of Corners'] &gt;= median_corners]['Full Throttle (%)']\n\n# perform t-test\nt_stat, p_value = ttest_ind(low_corners, high_corners, nan_policy='omit')\n\n# results\nprint(f\"T-Statistic: {t_stat:.4f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\n\nT-Statistic: 6.8204\nP-Value: 0.0000\n\n\nThere is a significant difference in Full Throttle (%) between tracks with low and high corners.\n\n\nCode\nsns.pairplot(data[[\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorr_matrix = data[[\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]].corr()\ncorr_matrix\n\n\n\n\n\n\n\n\n\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\nTrack Length (m)\n1.000000\n0.373616\n0.399017\n-0.058163\n-0.118008\n\n\nMax Speed (km/h)\n0.373616\n1.000000\n0.462422\n-0.246056\n-0.281559\n\n\nFull Throttle (%)\n0.399017\n0.462422\n1.000000\n-0.654084\n-0.667532\n\n\nNumber of Corners\n-0.058163\n-0.246056\n-0.654084\n1.000000\n0.631757\n\n\nNumber of Straights\n-0.118008\n-0.281559\n-0.667532\n0.631757\n1.000000\n\n\n\n\n\n\n\n\n\nCode\nsns.heatmap(corr_matrix, cmap=\"Blues_r\")\nplt.show()"
  },
  {
    "objectID": "technical-details/eda/main.html#pitstop-analysis",
    "href": "technical-details/eda/main.html#pitstop-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Pitstop Analysis",
    "text": "Pitstop Analysis\n\n\nCode\ndata = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nYear\nRound\nRaceName\nDriverID\nLap1\nLap2\nLap3\nLap4\nLap5\n...\nDuration1\nDuration2\nDuration3\nDuration4\nDuration5\nDuration6\nDuration7\nconstructorName\nposition\ngrid\n\n\n\n\n0\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n-1.492522\n-0.208201\n1.339923\n-0.281099\n-0.15023\n...\n0.471224\n0.733521\n1.882538\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nAlphaTauri\n11.0\n12.0\n\n\n1\n1\n2011\n1\nAustralian Grand Prix\nalonso\n-0.414215\n0.355923\n1.723412\n-0.281099\n-0.15023\n...\n-0.122753\n0.755727\n1.679973\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nFerrari\n4.0\n5.0\n\n\n2\n2\n2011\n1\nAustralian Grand Prix\nambrosio\n-0.218159\n0.976458\n-0.577525\n-0.281099\n-0.15023\n...\n0.204284\n0.896614\n-0.580401\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nMarussia\n14.0\n22.0\n\n\n3\n3\n2011\n1\nAustralian Grand Prix\nbarrichello\n-0.316187\n0.130273\n0.956433\n3.039918\n-0.15023\n...\n-0.058909\n1.835036\n0.998617\n3.860649\n-0.130754\n-0.060017\n-0.01976\nWilliams\n16.0\n17.0\n\n\n4\n4\n2011\n1\nAustralian Grand Prix\nbuemi\n-0.120131\n0.468747\n-0.577525\n-0.281099\n-0.15023\n...\n0.217802\n0.621420\n-0.580401\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nAlphaTauri\n8.0\n10.0\n\n\n\n\n5 rows × 36 columns\n\n\n\n\n\nCode\nnumerical_columns = [\"Lap1\", \"Lap2\", \"Lap3\", \"Lap4\", \"Stop2\", \"Stop3\", \"Stop4\",\n                     \"Duration1\", \"Duration2\", \"Duration3\", \"Duration4\", \"position\", \"grid\"]\ncorrelation_data = data[numerical_columns]\n\n# Calculate correlation matrix\ncorrelation_matrix = correlation_data.corr()\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"Blues_r\", fmt=\".2f\", linewidths=0.5)\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\n\n\n\n\n\nSome of the cell outputs were excluded from the main page. They can be found here"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Raw data collected from various sources is rarely in a format ready for analysis. It often contains inconsistencies, missing values, duplicates, and irrelevant information, which can hinder the analytical process and lead to inaccurate or biased results. Data Cleaning is the process of transforming this messy data into a structured, consistent, and reliable format, suitable for extracting meaningful insights and applying models effectively.\nImportance:\n\nData cleaning ensures that the data is complete, accurate, and consistent, significantly enhancing the reliability of insights derived from the analysis and the performance of models built using the data. High-quality data forms the foundation of robust models and predictions.\nMissing values in the dataset can introduce bias or distort the analysis. Addressing these gaps through techniques like imputation (filling in missing values using statistical or logical methods) or removal helps maintain the integrity of results.\nDuplicates in the dataset can overrepresent certain patterns, while outliers may distort metrics and affect model accuracy. Identifying and appropriately handling these anomalies ensures the analysis remains valid and unbiased.\nWhen data is collected from multiple sources, differences in format, naming conventions, and measurement units can cause inconsistencies. Standardizing these elements across all datasets ensures that the data can be seamlessly integrated and analyzed as a whole.\n\nIn this project:\nData was collected from a variety of sources, Data Cleaning was an essential step. Each raw dataset underwent a tailored cleaning process designed to fit its specific structure and use case. These processes will be discussed in detail within the respective sections dedicates to each dataset."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#required-libraries",
    "href": "technical-details/data-cleaning/main.html#required-libraries",
    "title": "Data Cleaning",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n\nCode\nimport os\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/nandinikodali/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#news-data",
    "href": "technical-details/data-cleaning/main.html#news-data",
    "title": "Data Cleaning",
    "section": "News Data",
    "text": "News Data\n\n\nCode\n# load English stop words\nstop_words = set(stopwords.words('english'))\n\ndef string_cleaner(input_string):\n    try:\n        # remove unwanted punctuation\n        out = re.sub(r\"[.,;@#?!&$-]+\", \" \", input_string) \n        \n        # remove escape characters \n        out = re.sub(r'\\\\u[0-9a-fA-F]{4}', '', out)\n        \n        # remove extra whitespace\n        out = re.sub(r'\\s+', ' ', out).strip()\n        \n        # convert to lowercase\n        out = out.lower()\n        \n        # tokenization\n        words = out.split()\n        # remove stop words and words of length &lt;= 3\n        words = [word for word in words if len(word) &gt; 3 and word not in stop_words]\n        \n        # join words back into a single string\n        out = ' '.join(words)\n        \n    except Exception as e:\n        print(f\"Error cleaning string: {e}\")\n        out = ''\n    \n    return out\n\n\n\n\nCode\n# Function to clean news data\ndef clean_news_data(raw_data_dir, clean_data_dir):\n    \n    # Iterate through raw data files\n    for file_name in os.listdir(raw_data_dir):\n        # Process only raw data files\n        if file_name.endswith(\"_raw_text.json\"):  \n            \n            # Load the raw data\n            raw_file_path = os.path.join(raw_data_dir, file_name)\n            with open(raw_file_path, 'r') as raw_file:\n                raw_data = json.load(raw_file)\n            \n            # inititlize dictionary to store clean the data\n            clean_data = {}\n            for article in raw_data:\n                # extract title\n                title = article.get('title', '')\n                # extract article's description\n                description = article.get('description', '')\n                \n                # clean the title and description\n                if title and description:\n                    # clean the title\n                    clean_title = string_cleaner(title)\n                    # clean the description\n                    clean_description = string_cleaner(description)\n                    # store the clean data in the dictionary\n                    clean_data[clean_title] = clean_description\n            \n            # Save the cleaned data to a new file\n            clean_file_name = file_name.replace(\"_raw_text.json\", \"_clean_news.json\")\n            clean_file_path = os.path.join(clean_data_dir, clean_file_name)\n            with open(clean_file_path, 'w') as clean_file:\n                json.dump(clean_data, clean_file, indent=4)\n\n\n\n\nCode\n# define directories\n\n# directory with raw data files\nraw_data_dir = \"../../data/raw-data/News_Drivers\"  \n\n# directory for cleaned data files\nclean_data_dir = \"../../data/processed-data/News_drivers\"  \n\n\n\n\nCode\n# call the function to process the clean data\nclean_news_data(raw_data_dir, clean_data_dir)\n\n\n\n\nCode\n# raw data\nprint(\"RAW DATA---\")\nraw = \"../../data/raw-data/News_drivers/Alonso_raw_text.json\"\nwith open(raw, 'r') as json_file:\n    data = json.load(json_file)\nfor entry in data[:2]:  \n    title = entry.get(\"title\", \"No Title\")\n    description = entry.get(\"description\", \"No Description\")\n    print(title,\":\",description)\n\nprint(\"\\n\")\n\n# clean data\nprint(\"CLEAN DATA---\")\nclean = \"../../data/processed-data/News_drivers/Alonso_clean_news.json\"\nwith open(clean, 'r') as json_file:\n    data = json.load(json_file)\n\nfor key, value in list(data.items())[:2]: \n    print(key,\":\", value)\n\n\nRAW DATA---\nNorris calls Verstappen 'dangerous' as Sainz wins in Mexico : Lando Norris cuts Max Verstappen’s lead to 47 points and labels his rival “dangerous” as the championship battle reaches boiling point at the Mexico City Grand Prix.\nHow well do you know Fernando Alonso? : As he prepares for his 400th F1 grand prix in Mexico City this weekend, find out how much you know about Fernando Alonso.\n\n\nCLEAN DATA---\nnorris calls verstappen 'dangerous' sainz wins mexico : lando norris cuts verstappens lead points labels rival dangerous championship battle reaches boiling point mexico city grand prix\nwell know fernando alonso : prepares grand prix mexico city weekend find much know fernando alonso"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#drivers-standings",
    "href": "technical-details/data-cleaning/main.html#drivers-standings",
    "title": "Data Cleaning",
    "section": "Drivers Standings",
    "text": "Drivers Standings\n\n\nCode\ninput_file = \"../../data/raw-data/Driver_standings/driver_standings_2000_2023.json\"\noutput_file = \"../../data/processed-data/driver_standings_2000_2023.csv\"\n\n\n\n\nCode\nwith open(input_file, 'r') as f:\n    data = json.load(f)\n\n# initialize a list to store extracted records\ncleaned_data = []\n\n# loop through each season in the JSON\nfor season, season_data in data.items():\n    # navigate to the StandingsLists section \n    standings_lists = season_data.get('MRData', {}).get('StandingsTable', {}).get('StandingsLists', [])\n    \n    # loop through each standings entry\n    for standings in standings_lists:\n        # extract driver standings\n        driver_standings = standings.get('DriverStandings', [])\n        \n        # loop through each entry\n        for entry in driver_standings:\n            # Extract required fields\n            position = entry.get('position', '')            # positions in the standings\n            points = entry.get('points', '')                # points earned by the driver\n            wins = entry.get('wins', '')                    # number of wins by the driver\n            driver = entry.get('Driver', {})                # nested dict. containing driver info\n            constructors = entry.get('Constructors', [])    # team info of the driver\n            \n            # extract driver and constructor details\n            given_name = driver.get('givenName', '')\n            family_name = driver.get('familyName', '')\n            constructor_id = constructors[0].get('constructorId', '') if constructors else ''\n            constructor_name = constructors[0].get('name', '') if constructors else ''\n            \n            # append the record to the cleaned data list\n            cleaned_data.append({\n                \"Season\": season,\n                \"Position\": position,\n                \"FirstName\": given_name,\n                \"LastName\": family_name,\n                \"Constructor_ID\": constructor_id,\n                \"Constructor_Name\": constructor_name,\n                \"Points\": points,\n                \"Wins\": wins\n            })\n\n\n\n\nCode\n# convert the cleaned data into a dataframe for easier manipulation\ndf = pd.DataFrame(cleaned_data)\ndf.head()\n\n\n\n\n\n\n\n\n\nSeason\nPosition\nFirstName\nLastName\nConstructor_ID\nConstructor_Name\nPoints\nWins\n\n\n\n\n0\n2000\n1\nMichael\nSchumacher\nferrari\nFerrari\n108\n9\n\n\n1\n2000\n2\nMika\nHäkkinen\nmclaren\nMcLaren\n89\n4\n\n\n2\n2000\n3\nDavid\nCoulthard\nmclaren\nMcLaren\n73\n3\n\n\n3\n2000\n4\nRubens\nBarrichello\nferrari\nFerrari\n62\n1\n\n\n4\n2000\n5\nRalf\nSchumacher\nwilliams\nWilliams\n24\n0\n\n\n\n\n\n\n\n\n\nCode\n# check for missing values\ndf.isnull().sum()\n\n\nSeason              0\nPosition            0\nFirstName           0\nLastName            0\nConstructor_ID      0\nConstructor_Name    0\nPoints              0\nWins                0\ndtype: int64\n\n\n\n\nCode\n# concat FirstName and LastName\ndf['driverName'] = df['FirstName'] + \" \" + df['LastName']\ndf = df.drop(['FirstName', 'LastName'], axis=1)\n\n# write the clean data to a new csv file\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#circuit-information",
    "href": "technical-details/data-cleaning/main.html#circuit-information",
    "title": "Data Cleaning",
    "section": "Circuit Information",
    "text": "Circuit Information\n\n\nCode\ninput_file = \"../../data/raw-data/circuit_data/circuit_data.json\"\noutput_file = \"../../data/processed-data/circuit_data_clean.csv\"\n\n\n\n\nCode\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# read the JSON file\nwith open(input_file, 'r') as f:\n    data = json.load(f)\n\n# extract circuit data\ncircuits = data.get('MRData', {}).get('CircuitTable', {}).get('Circuits', [])\n\n# prepare a list to store extracted records\ncleaned_data = []\n\nfor circuit in circuits:\n    circuit_id = circuit.get('circuitId', '')\n    circuit_name = circuit.get('circuitName', '')\n    country = circuit.get('Location', {}).get('country', '')\n    latitude = circuit.get('Location', {}).get('lat', '')\n    longitude = circuit.get('Location', {}).get('long', '')\n    \n    # append to the list\n    cleaned_data.append({\n        \"Circuit_ID\": circuit_id,\n        \"Circuit_Name\": circuit_name,\n        \"Country\": country,\n        \"Latitude\": latitude,\n        \"Longitude\": longitude\n    })\n\n# convert the list to a Pandas DataFrame\ndf = pd.DataFrame(cleaned_data)\n\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\nCircuit_ID\nCircuit_Name\nCountry\nLatitude\nLongitude\n\n\n\n\n0\nadelaide\nAdelaide Street Circuit\nAustralia\n-34.9272\n138.617\n\n\n1\nain-diab\nAin Diab\nMorocco\n33.5786\n-7.6875\n\n\n2\naintree\nAintree\nUK\n53.4769\n-2.94056\n\n\n3\nalbert_park\nAlbert Park Grand Prix Circuit\nAustralia\n-37.8497\n144.968\n\n\n4\namericas\nCircuit of the Americas\nUSA\n30.1328\n-97.6411\n\n\n\n\n\n\n\n\n\nCode\n# check for null values\ndf.isnull().sum()\n\n\nCircuit_ID      0\nCircuit_Name    0\nCountry         0\nLatitude        0\nLongitude       0\ndtype: int64\n\n\n\n\nCode\n# Save the DataFrame to a CSV file\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#race-data",
    "href": "technical-details/data-cleaning/main.html#race-data",
    "title": "Data Cleaning",
    "section": "Race data",
    "text": "Race data\n\n\nCode\n## Cleaning all the race_data and appending them in to a single csv file \n\n# input output directory\ninput_dir = \"../../data/raw-data/\"\n# output directory\noutput_file = \"../../data/processed-data/all_race_results_cleaned.csv\"\n\n# creating an output file\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# initialize a list to hold all results\nall_combined_results = []\n\n# process each JSON file in the input directory\n# they are the only .json files in the directory\nfor file_name in os.listdir(input_dir):\n    # process only JSON files\n    if file_name.endswith(\".json\"): \n        file_path = os.path.join(input_dir, file_name)\n        #print(f\"Processing file: {file_path}\")\n        \n        # read the JSON file\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        \n        # extract races from the JSON\n        races = data.get('MRData', {}).get('RaceTable', {}).get('Races', [])\n        \n        # prepare a list to hold flattened race results for this file\n        file_results = []\n\n        # loop through each race and flatten its data\n        for race in races:\n            # extract required information \n            race_info = { \n                \"season\": race.get(\"season\", \"\"),\n                \"round\": race.get(\"round\", \"\"),\n                \"raceName\": race.get(\"raceName\", \"\"),\n                \"url\": race.get(\"url\",\"\"),\n                \"circuitName\": race.get(\"Circuit\", {}).get(\"circuitName\", \"\"),\n                \"locality\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"locality\", \"\"),\n                \"country\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"country\", \"\"),\n                \"lat\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"lat\", \"\"),\n                \"long\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"long\", \"\"),\n                \"date\": race.get(\"date\", \"\"),\n            }\n            \n            # extract results and combine with useful details\n            results = race.get(\"Results\", [])\n            for result in results:\n                # combine race-level and result-level data\n                combined_data = {**race_info, **result}\n                # add flattened driver and constructor details\n                combined_data.update({\n                    \"driverId\": result.get(\"Driver\", {}).get(\"driverId\", \"\"),\n                    \"driverGivenName\": result.get(\"Driver\", {}).get(\"givenName\", \"\"),\n                    \"driverFamilyName\": result.get(\"Driver\", {}).get(\"familyName\", \"\"),\n                    \"constructorId\": result.get(\"Constructor\", {}).get(\"constructorId\", \"\"),\n                    \"constructorName\": result.get(\"Constructor\", {}).get(\"name\", \"\"),\n                    \"status\": result.get(\"status\", \"\"),\n                    \"timeMillis\": result.get(\"Time\", {}).get(\"millis\", \"\"),\n                    \"time\": result.get(\"Time\", {}).get(\"time\", \"\")\n                })\n                file_results.append(combined_data)\n\n        # append the results for this file to the combined list\n        all_combined_results.extend(file_results)\n\n# convert the combined results to a Pandas DataFrame\ndf = pd.DataFrame(all_combined_results)\n\n\n\n\nCode\ndf.head()\n\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\nlat\nlong\ndate\n...\nstatus\nTime\nFastestLap\ndriverId\ndriverGivenName\ndriverFamilyName\nconstructorId\nconstructorName\ntimeMillis\ntime\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '5960396', 'time': '1:39:20.396'}\n{'rank': '1', 'lap': '45', 'Time': {'time': '1...\nalonso\nFernando\nAlonso\nferrari\nFerrari\n5960396\n1:39:20.396\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '5976495', 'time': '+16.099'}\n{'rank': '5', 'lap': '38', 'Time': {'time': '1...\nmassa\nFelipe\nMassa\nferrari\nFerrari\n5976495\n+16.099\n\n\n2\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '5983578', 'time': '+23.182'}\n{'rank': '4', 'lap': '42', 'Time': {'time': '1...\nhamilton\nLewis\nHamilton\nmclaren\nMcLaren\n5983578\n+23.182\n\n\n3\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '5999195', 'time': '+38.799'}\n{'rank': '12', 'lap': '32', 'Time': {'time': '...\nvettel\nSebastian\nVettel\nred_bull\nRed Bull\n5999195\n+38.799\n\n\n4\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '6000609', 'time': '+40.213'}\n{'rank': '13', 'lap': '45', 'Time': {'time': '...\nrosberg\nNico\nRosberg\nmercedes\nMercedes\n6000609\n+40.213\n\n\n\n\n5 rows × 28 columns\n\n\n\n\n\nCode\n# save the combined DataFrame to a CSV file\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#weather-data",
    "href": "technical-details/data-cleaning/main.html#weather-data",
    "title": "Data Cleaning",
    "section": "Weather Data",
    "text": "Weather Data\n\n\nCode\nweather_df = pd.read_csv(\"../../data/raw-data/weather/race_data_with_weather.csv\")\n\n\n\n\nCode\nweather_df.isnull().sum()\n\n\nseason      0\nraceName    0\nurl         0\nweather     0\ndtype: int64\n\n\n\n\nCode\nweather_df['weather']\n\n\n0                                                  Sunny\n1                      Overcast with light rain at start\n2                                     Mainly cloudy, dry\n3                                           Cloudy, rain\n4                                     Mainly cloudy, dry\n                             ...                        \n117    Sunny with temperatures reaching up to 27 °C (...\n118    Dry start, with heavy rain and thunderstorm/mo...\n119                                                 Rain\n120                                                Sunny\n121                                          Warm, Sunny\nName: weather, Length: 122, dtype: object\n\n\nwe will try to categorise the weather description into one of the following categories:\n\nSunny\nCloudy\nRainy\nWindy\n\n\n\nCode\ndef classify_weather(weather_description):\n\n    weather_description = weather_description.lower()\n    \n    if \"sunny\" in weather_description or \"fine\" in weather_description or \"clear\" in weather_description or \"dry\" in weather_description:\n        return \"Sunny\"\n    elif \"cloudy\" in weather_description or \"overcast\" in weather_description or \"cloud\" in weather_description:\n        return \"Cloudy\"\n    elif \"rain\" in weather_description or \"thunderstorms\" in weather_description or \"drizzle\" in weather_description:\n        return \"Rainy\"\n    elif \"windy\" in weather_description:\n        return \"Windy\"\n    # If no match, classify as \"Not Available\"\n    else:\n        return \"Not Available\" \n\n\n\n\nCode\n# call classify_weather()\nweather_df['weather_class'] = weather_df['weather'].apply(classify_weather)\n\n\n\n\nCode\nweather_df['weather_class'].value_counts()\n\n\nweather_class\nSunny            95\nCloudy           24\nRainy             2\nNot Available     1\nName: count, dtype: int64\n\n\nDue to severe class imbalance, weather_class feature is not a suitable candidate for effective analysis or model training.\n\n\nCode\nprint(weather_df[weather_df['weather_class'] == \"Not Available\"])\n\n\n   season             raceName  \\\n9    2006  European Grand Prix   \n\n                                                 url        weather  \\\n9  http://en.wikipedia.org/wiki/2006_European_Gra...  Not Available   \n\n   weather_class  \n9  Not Available  \n\n\nThe weather data for 2006 European Grand Prix is not available on wikipedia.\n\nUsing longitude, latitude and the date: The weather was Sunny\n\n\n\nCode\nweather_df['weather_class'] = weather_df['weather_class'].replace('Not Available', 'Sunny')\n\n\n\n\nCode\n# Save the updated DataFrame to a CSV file\noutput_csv = \"../../data/processed-data/classified_weather_data.csv\"\nweather_df.to_csv(output_csv, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#race-info-merged",
    "href": "technical-details/data-cleaning/main.html#race-info-merged",
    "title": "Data Cleaning",
    "section": "Race Info Merged",
    "text": "Race Info Merged\n\n\nCode\n# merge race results and weather information\nrace_df = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")\nweather_df = pd.read_csv(\"../../data/processed-data/classified_weather_data.csv\")\n\n# the url contains the season and the race name \n# url can be used to merge the data\nmerged_df = race_df.merge(weather_df[['url','weather_class']], on='url', how='left')\n\nmerged_df.to_csv(\"../../data/processed-data/race_weather_merged.csv\", index=False)\n\n\n\n\nCode\nmain_df = pd.read_csv(\"../../data/processed-data/race_weather_merged.csv\")\nmain_df.head()\n\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\nlat\nlong\ndate\n...\nTime\nFastestLap\ndriverId\ndriverGivenName\ndriverFamilyName\nconstructorId\nconstructorName\ntimeMillis\ntime\nweather_class\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5960396', 'time': '1:39:20.396'}\n{'rank': '1', 'lap': '45', 'Time': {'time': '1...\nalonso\nFernando\nAlonso\nferrari\nFerrari\n5960396.0\n1:39:20.396\nSunny\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5976495', 'time': '+16.099'}\n{'rank': '5', 'lap': '38', 'Time': {'time': '1...\nmassa\nFelipe\nMassa\nferrari\nFerrari\n5976495.0\n+16.099\nSunny\n\n\n2\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5983578', 'time': '+23.182'}\n{'rank': '4', 'lap': '42', 'Time': {'time': '1...\nhamilton\nLewis\nHamilton\nmclaren\nMcLaren\n5983578.0\n+23.182\nSunny\n\n\n3\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5999195', 'time': '+38.799'}\n{'rank': '12', 'lap': '32', 'Time': {'time': '...\nvettel\nSebastian\nVettel\nred_bull\nRed Bull\n5999195.0\n+38.799\nSunny\n\n\n4\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '6000609', 'time': '+40.213'}\n{'rank': '13', 'lap': '45', 'Time': {'time': '...\nrosberg\nNico\nRosberg\nmercedes\nMercedes\n6000609.0\n+40.213\nSunny\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\nCode\nmain_df.columns\n\n\nIndex(['season', 'round', 'raceName', 'url', 'circuitName', 'locality',\n       'country', 'lat', 'long', 'date', 'number', 'position', 'positionText',\n       'points', 'Driver', 'Constructor', 'grid', 'laps', 'status', 'Time',\n       'FastestLap', 'driverId', 'driverGivenName', 'driverFamilyName',\n       'constructorId', 'constructorName', 'timeMillis', 'time',\n       'weather_class'],\n      dtype='object')\n\n\n\n\nCode\n# drop un-needed columns\nmain_df = main_df.drop(['lat', 'long', 'number', 'positionText', 'Driver', 'Constructor', 'Time', 'FastestLap'], axis=1)\n\n\n\n\nCode\nmain_df.columns\n\n\nIndex(['season', 'round', 'raceName', 'url', 'circuitName', 'locality',\n       'country', 'date', 'position', 'points', 'grid', 'laps', 'status',\n       'driverId', 'driverGivenName', 'driverFamilyName', 'constructorId',\n       'constructorName', 'timeMillis', 'time', 'weather_class'],\n      dtype='object')\n\n\n\n\nCode\nmain_df.isnull().sum()\n\n\nseason                 0\nround                  0\nraceName               0\nurl                    0\ncircuitName            0\nlocality               0\ncountry                0\ndate                   0\nposition               0\npoints                 0\ngrid                   0\nlaps                   0\nstatus                 0\ndriverId               0\ndriverGivenName        0\ndriverFamilyName       0\nconstructorId          0\nconstructorName        0\ntimeMillis          1291\ntime                1291\nweather_class          0\ndtype: int64\n\n\nThe missing values in ‘timeMillis' and’time' columns are of those drivers who did not finish the race. Therefore, we will drop these columns and try to analyse the performance based on other metrics.\n\n\nCode\n# drop un-needed columns\nmain_df = main_df.drop(['timeMillis', 'time'], axis=1)\n\n\n\n\nCode\nmain_df['constructorName'].value_counts()\n\n\nconstructorName\nFerrari           235\nMcLaren           234\nWilliams          229\nRed Bull          184\nRenault           142\nSauber            139\nMercedes          134\nToro Rosso        127\nForce India       100\nHaas F1 Team       79\nToyota             76\nJordan             57\nBAR                56\nMinardi            54\nAlfa Romeo         50\nJaguar             45\nBMW Sauber         40\nAlphaTauri         40\nLotus F1           38\nAlpine F1 Team     30\nAston Martin       30\nHonda              28\nArrows             26\nHRT                24\nMarussia           24\nSuper Aguri        24\nCaterham           24\nRacing Point       20\nProst              18\nBenetton           18\nVirgin             16\nManor Marussia     16\nLotus              16\nBrawn              10\nMF1                 9\nSpyker              8\nName: count, dtype: int64\n\n\n\n\nCode\nmain_df = main_df.drop(['constructorId'], axis=1)\n\n\nSome of the team names were changed in the process of rebranding or due to a change in ownership. For accurate analysis, we will replace the older versions of the constructors’ names with the current ones.\n\nFerrari\nMcLaren\nJaguar\nWilliams\nSauber \\(\\rightarrow\\) BMW Sauber \\(\\rightarrow\\) Sauber \\(\\rightarrow\\) Alfa Romeo \\(\\rightarrow\\) Sauber\nBAR \\(\\rightarrow\\) Honda \\(\\rightarrow\\) Brawn \\(\\rightarrow\\) Mercedes\nBenetton \\(\\rightarrow\\) Renault \\(\\rightarrow\\) Lotus F1 \\(\\rightarrow\\) Renault \\(\\rightarrow\\) Alpine\nJordan \\(\\rightarrow\\) Midland \\(\\rightarrow\\) Spyker \\(\\rightarrow\\) Force India \\(\\rightarrow\\) Aston Martin\nMinardi \\(\\rightarrow\\) Toro Rosso \\(\\rightarrow\\) Scuderia AlphaTauri\nHaas\nToyota\nVirgin Racing \\(\\rightarrow\\) Marussia F1 \\(\\rightarrow\\) Manor Marussia\nLotus \\(\\rightarrow\\) Caterham\nArrows\nSuper Aguri\nHRT\nProst\n\n\n\nCode\nconstructor_mapping= {\n    \"Jaguar\" : \"Red Bull\",\n    \"BMW Sauber\" : \"Sauber\",\n    \"Alfa Romeo\" : \"Sauber\",\n    \"BAR\" : \"Mercedes\",\n    \"Honda\" : \"Mercedes\",\n    \"Brawn\" : \"Mercedes\",\n    \"Minardi\" : \"AlphaTauri\",\n    \"Toro Rosso\" : \"AlphaTauri\",\n    \"Force India\" : \"Aston Martin\",\n    \"Jordan\" : \"Aston Martin\",\n    \"Racing Point\" : \"Aston Martin\",\n    \"MF1\" : \"Aston Martin\",\n    \"Spyker\" : \"Aston Martin\",\n    \"Lotus F1\" : \"Alpine F1 Team\",\n    \"Renault\" : \"Alpine F1 Team\",\n    \"Benetton\" : \"Alpine F1 Team\",\n    \"Manor Marussia\" : \"Marussia\",\n    \"Virgin\" : \"Marussia\",\n    \"Lotus\" : \"Caterham\",\n\n}\n\n\n\n\nCode\nmain_df['constructorName'] = main_df['constructorName'].replace(constructor_mapping)\n\n\n\n\nCode\nmain_df['constructorName'].value_counts()\n\n\nconstructorName\nFerrari           235\nMcLaren           234\nRed Bull          229\nWilliams          229\nSauber            229\nMercedes          228\nAlpine F1 Team    228\nAston Martin      224\nAlphaTauri        221\nHaas F1 Team       79\nToyota             76\nMarussia           56\nCaterham           40\nArrows             26\nSuper Aguri        24\nHRT                24\nProst              18\nName: count, dtype: int64\n\n\n\n\nCode\nmain_df['driverName'] = main_df['driverGivenName'] + \" \" + main_df['driverFamilyName']\n\n\n\n\nCode\nmain_df = main_df.drop(['driverGivenName','driverFamilyName'], axis = 1)\n\n\n\n\nCode\nmain_df.head(2)\n\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\ndate\nposition\npoints\ngrid\nlaps\nstatus\ndriverId\nconstructorName\nweather_class\ndriverName\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n1\n25.0\n3\n49\nFinished\nalonso\nFerrari\nSunny\nFernando Alonso\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n2\n18.0\n2\n49\nFinished\nmassa\nFerrari\nSunny\nFelipe Massa\n\n\n\n\n\n\n\nClassify the satus column in to broader categories. This column provides information on whether the driver has finished the race or not, if not, was it because of a mechanical failure, an accident, or was he lapped. The categories are:\n\nFinished\nLap\nAccident\nMechanical\n\nGrouping the data helps understand the major reasons for the race results without getting overwhelmed by the granular details.\n\n\nCode\nmain_df['status'].unique()\n\n\narray(['Finished', '+1 Lap', '+2 Laps', 'Electrical', 'Hydraulics',\n       'Overheating', 'Gearbox', 'Suspension', 'Accident', '+5 Laps',\n       'Wheel', 'Engine', 'Spun off', 'Collision', '+3 Laps', '+4 Laps',\n       '+10 Laps', 'Throttle', 'Clutch', 'Technical', 'Mechanical',\n       'Driveshaft', 'Transmission', 'Steering', 'Puncture', 'Brakes',\n       'Retired', 'Tyre', 'Fuel pressure', '+9 Laps', 'Water leak',\n       'Disqualified', 'Did not qualify', '+42 Laps', 'Engine misfire',\n       'Power Unit', 'Oil pressure', 'Safety concerns', 'Fuel system',\n       '+6 Laps', 'Electronics', 'Collision damage', 'Wheel nut',\n       'Battery', 'Oil leak', '+7 Laps', 'Stalled', 'Exhaust',\n       'Vibrations', 'Broken wing', 'Fuel', 'Wheel rim', 'Power loss',\n       '107% Rule', '+8 Laps', 'ERS', 'Withdrew', 'Cooling system',\n       'Water pump', 'Fuel leak', 'Front wing', 'Turbo', 'Damage',\n       'Out of fuel', 'Radiator', 'Oil line', 'Fuel rig',\n       'Launch control', 'Not classified', 'Pneumatics', 'Differential'],\n      dtype=object)\n\n\n\n\nCode\n# classifying different categories under status\ndef classify_status(status):\n    if status == 'Finished':\n        return 'Finished'\n    # if the status contains the word 'Lap' \n    elif 'Lap' in status:  \n        return 'Lapped'\n    elif status in ['Accident', 'Collision', 'Spun off', 'Withdrew']:\n        return 'Accident'\n    # all other statuses are classified as 'Mechanical'\n    else:\n        return 'Mechanical'\n\nmain_df['status'] = main_df['status'].apply(classify_status)\n\n\n\n\nCode\nmain_df['status'].value_counts()\n\n\nstatus\nFinished      1105\nLapped         693\nMechanical     412\nAccident       190\nName: count, dtype: int64\n\n\n\n\nCode\nmain_df.to_csv(\"../../data/processed-data/race_info.csv\", index=False)\n\n\nFinish category - new categorical variable\nIn F1, race outcomes are categorized based on finishing positions:\n\nThe top 3 finishers are celebrated on the podium and referred to as achieving a Podium Finish\nAll drivers who finish in top 10 earn championship points, ranging from 25 points for the winner, 18 for second place, and decreasing to 1 point for the 10th position.\nDrivers finishing outside the top 10 do not earn any championship points.\n\n\n\nCode\ndata = pd.read_csv(\"../../data/processed-data/race_info.csv\")\n# create a new categorical variable \ndata['FinishCategory'] = ''\n# iterate through each row of the dataset\nfor i in range(len(data)):\n    # if the position is 1,2 or 3\n    if data['position'][i] in [1,2,3]:\n        # assign the category as podium\n        data['FinishCategory'][i] = \"Podium\"\n    # if the position is in between 4 and 10\n    elif data['position'][i] in [4,5,6,7,8,9,10]:\n        # assign the category as points finish\n        data['FinishCategory'][i] = \"Points Finish\"\n    # assign no points for poistions outside top 10\n    else:\n        data['FinishCategory'][i] = \"No Points\"\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\ndate\nposition\npoints\ngrid\nlaps\nstatus\ndriverId\nconstructorName\nweather_class\ndriverName\nFinishCategory\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n1\n25.0\n3\n49\nFinished\nalonso\nFerrari\nSunny\nFernando Alonso\nPodium\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n2\n18.0\n2\n49\nFinished\nmassa\nFerrari\nSunny\nFelipe Massa\nPodium\n\n\n2\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n3\n15.0\n4\n49\nFinished\nhamilton\nMcLaren\nSunny\nLewis Hamilton\nPodium\n\n\n3\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n4\n12.0\n1\n49\nFinished\nvettel\nRed Bull\nSunny\nSebastian Vettel\nPoints Finish\n\n\n4\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n5\n10.0\n5\n49\nFinished\nrosberg\nMercedes\nSunny\nNico Rosberg\nPoints Finish\n\n\n\n\n\n\n\n\n\nCode\ndata.to_csv(\"../../data/processed-data/race_info.csv\", index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#race-track-features",
    "href": "technical-details/data-cleaning/main.html#race-track-features",
    "title": "Data Cleaning",
    "section": "Race Track Features",
    "text": "Race Track Features\n\n\nCode\n# race track features\ndata = pd.read_csv(\"../../data/raw-data/circuit_data/merged_circuit_features.csv\")\ndata.head()\n\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\n0\n2020\nPre-Season Test 1\n4312.438437\n323\n70.673953\n1\n4\n\n\n1\n2020\nPre-Season Test 2\n4312.438437\n323\n70.673953\n1\n4\n\n\n2\n2020\nAustrian Grand Prix\n4312.438437\n323\n70.673953\n1\n4\n\n\n3\n2020\nStyrian Grand Prix\n4292.610384\n300\n46.556886\n2\n6\n\n\n4\n2020\nHungarian Grand Prix\n4348.049386\n318\n58.114374\n0\n6\n\n\n\n\n\n\n\n\n\nCode\ndata.isnull().sum()\n\n\nYear                   0\nGrand Prix             0\nTrack Length (m)       0\nMax Speed (km/h)       0\nFull Throttle (%)      0\nNumber of Corners      0\nNumber of Straights    0\ndtype: int64\n\n\nStandardization\nStandardization is a preprocessing step used to scale the features is a consistent scale for more accurate and stable modelling. Features with different scales can lead to one feature dominating others during model training, Standardization eliminates disparity.\nFor the Race Track Features, StandardScaler is used, this standardizes the data by “removing the mean and scaling to unit variance”1.\nFormula for Standardization: \\[ Z = \\frac{X - \\mu}{\\sigma} \\] where:\n\n\\(X\\): the original value\n\\(\\mu\\): mean\n\\(\\sigma\\): standard deviation of the feature\n\n\n\nCode\nreq_cols = [\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]\nscaler = StandardScaler()\n\n# Apply the scaler to the dataframe\ndata[req_cols] = scaler.fit_transform(data[req_cols])\n\n\n\n\nCode\ndata.head()\n\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\n\n\n\n\n\n\n\n\n\nCode\ndata.to_csv(\"../../data/processed-data/race_track_features.csv\")"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#pitstop-data",
    "href": "technical-details/data-cleaning/main.html#pitstop-data",
    "title": "Data Cleaning",
    "section": "Pitstop data",
    "text": "Pitstop data\n\n\nCode\ndf = pd.read_csv(\"../../data/raw-data/pitstop_data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nYear\nRound\nRaceName\nDriverID\nLap\nStop\nTime\nDuration\n\n\n\n\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n1\n1\n17:05:23\n26.898\n\n\n1\n2011\n1\nAustralian Grand Prix\nmichael_schumacher\n1\n1\n17:05:52\n25.021\n\n\n2\n2011\n1\nAustralian Grand Prix\nwebber\n11\n1\n17:20:48\n23.426\n\n\n3\n2011\n1\nAustralian Grand Prix\nalonso\n12\n1\n17:22:34\n23.251\n\n\n4\n2011\n1\nAustralian Grand Prix\nmassa\n13\n1\n17:24:10\n23.842\n\n\n\n\n\n\n\nPivoting involves reshaping data by rearranging rows into columns. It is used to transform long-format data (many rows for each entity) to wide-format (one row per entity with multiple columns).\nHere, each Stop Number becomes a seperate set of columns, Lap1, Lap2, Duration1, Duration2 and so on.\n\n\nCode\ntransformed = (\n    # group the data by year, round, race name and driver\n    # create Pit stop number for each pit stop of a driver in the data\n    # pit stop number will be used for pivoting\n    df.assign(PitStopNumber=lambda x: x.groupby(['Year', 'Round', 'RaceName', 'DriverID']).cumcount() + 1)\n    # index[] - unique identifiers\n    # columns - used to create new columns for each pit stop\n    # values - values to be assigned to the new columns in the pivot table\n    .pivot(index=['Year', 'Round', 'RaceName', 'DriverID'], columns='PitStopNumber', values=['Lap', 'Stop', 'Time', 'Duration'])\n)\n\n# after pivoting, cols have multi-index structure\n# combine col name with pit stop number for easier handling\ntransformed.columns = [f\"{col[0]}{col[1]}\" for col in transformed.columns]\n\n# reset index to turn it into a DataFrame\ntransformed.reset_index(inplace=True)\n\n# handling missing values - replace with 0\ntransformed = transformed.fillna(0)\n\ntransformed.head(2)\n\n\n\n\n\n\n\n\n\nYear\nRound\nRaceName\nDriverID\nLap1\nLap2\nLap3\nLap4\nLap5\nLap6\n...\nTime5\nTime6\nTime7\nDuration1\nDuration2\nDuration3\nDuration4\nDuration5\nDuration6\nDuration7\n\n\n\n\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n1\n17\n35\n0\n0\n0\n...\n0\n0\n0\n26.898\n24.463\n26.348\n0\n0\n0\n0\n\n\n1\n2011\n1\nAustralian Grand Prix\nalonso\n12\n27\n42\n0\n0\n0\n...\n0\n0\n0\n23.251\n24.733\n24.181\n0\n0\n0\n0\n\n\n\n\n2 rows × 32 columns\n\n\n\n\n\nCode\ntransformed.shape\n\n\n(5118, 32)\n\n\nMinMaxx Scaling: Transforms the fature range by scaling its min and max values.\nFormula: \\[X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}\\]\n\n\nCode\n# convert relevant columns to minutes\nfrom sklearn.preprocessing import MinMaxScaler\ndef convert_to_minutes(value):\n    try:\n        # convert seconds to minutes\n        return float(value) / 60  \n    except:\n        # default to 0 for invalid values\n        return 0  \n# List of time and duration columns\ntime_and_duration_cols = [\n    \"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\",\n    \"Duration1\", \"Duration2\", \"Duration3\", \"Duration4\", \"Duration5\", \"Duration6\", \"Duration7\"\n]\n\n# convert each column to minutes\nfor col in time_and_duration_cols:\n    if col in transformed.columns:\n        transformed[col] = transformed[col].apply(convert_to_minutes)\n\n# handling missing values - replace with 0\ntransformed.fillna(0, inplace=True)\n\n# select all columns except `Year`, `Round`, `RaceName`, and `DriverID` for scaling\nscalable_columns = transformed.drop(columns=[\"Year\", \"Round\", \"RaceName\", \"DriverID\"]).columns\n\n# apply MinMax scaling\nscaler = MinMaxScaler()\ntransformed[scalable_columns] = scaler.fit_transform(transformed[scalable_columns])\n\nprint(transformed.head())\n\n\n   Year  Round               RaceName     DriverID      Lap1      Lap2  \\\n0  2011      1  Australian Grand Prix  alguersuari  0.000000  0.229730   \n1  2011      1  Australian Grand Prix       alonso  0.174603  0.364865   \n2  2011      1  Australian Grand Prix     ambrosio  0.206349  0.513514   \n3  2011      1  Australian Grand Prix  barrichello  0.190476  0.310811   \n4  2011      1  Australian Grand Prix        buemi  0.222222  0.391892   \n\n       Lap3      Lap4  Lap5  Lap6  ...  Time5  Time6  Time7  Duration1  \\\n0  0.479452  0.000000   0.0   0.0  ...    0.0    0.0    0.0   0.453661   \n1  0.575342  0.000000   0.0   0.0  ...    0.0    0.0    0.0   0.392151   \n2  0.000000  0.000000   0.0   0.0  ...    0.0    0.0    0.0   0.426017   \n3  0.383562  0.512821   0.0   0.0  ...    0.0    0.0    0.0   0.398762   \n4  0.000000  0.000000   0.0   0.0  ...    0.0    0.0    0.0   0.427417   \n\n   Duration2  Duration3  Duration4  Duration5  Duration6  Duration7  \n0   0.428042   0.457423   0.000000        0.0        0.0        0.0  \n1   0.432766   0.419802   0.000000        0.0        0.0        0.0  \n2   0.462739   0.000000   0.000000        0.0        0.0        0.0  \n3   0.662386   0.293259   0.469108        0.0        0.0        0.0  \n4   0.404192   0.000000   0.000000        0.0        0.0        0.0  \n\n[5 rows x 32 columns]\n\n\n\n\nCode\ntransformed.to_csv(\"../../data/processed-data/pitstop.csv\")\n\n\nAll the clean datasets can be found here"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#footnotes",
    "href": "technical-details/data-cleaning/main.html#footnotes",
    "title": "Data Cleaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStandardScaler↩︎"
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "In this section, we focus on the methods and sources used to collect data that are relevant to the research questions of this project. The quality of the data collected determines the accuracy of insights and predictions. Poor data collection practices can lead to biased analysis, inaccurate results, and ineffective models. Therefore, careful planning and execution of data collection is essential for the success of any data-driven project.\nChallenges in Data Collection Some important points to be considered when collecting data are:\n\nData quality: Ensuring the data is accurate, complete, and relevant to the research questions is crucial.\nInconsistencies: Data form different sources may have different formats, structures, naming conventions, and units of measurement, requiring a thorough understanding of the data for pre-processing.\nEthics and Privacy: Data collection methods must adhere to ethical guidelines, ensuring that no sensitive or private information is collected or misused.\nData Bias: Collecting data from sources that introduces bias can lead to inaccurate results and models.\nTechnical Constraints: Issues such as API rate limits, website restrictions, or incomplete data can hinder data collection.\n\nBy addressing these challenges, it is ensured that the data collected was relevant, accurate, and reliable."
  },
  {
    "objectID": "technical-details/data-collection/main.html#web-scraping",
    "href": "technical-details/data-collection/main.html#web-scraping",
    "title": "Data Collection",
    "section": "Web Scraping",
    "text": "Web Scraping\nWeb Scraping is an automatic way to collect data from websites. It involves the use of automated scripts or tools to interact with the website’s structure to retrieve information. The data is extracted using selectors like tags, classes, or IDs. While web scraping can be tailored to collect a variety od data types from multiple web pages, one should be aware of the website’s terms of service and ethical scraping practices, and manage rate limits to avoid being blocked.\n\nIn this Project: Using Python libraries like requests and BeautifulSoup, weather information was extracted from infoboxes of each race’s Wikipedia pages.\n\nProcess:\n- The URLs for each race were obtained from the race data collected, these links were used to locate the Wikipedia pages of each race.\n- HTTP requests were sent to the Wikipedia pages to retrieve the HTML content.\n- Parsing the HTML using BeautifulSoup locate the infobox containing the race metadata.\n- Locating and extracting the Weather field from the table.\n1"
  },
  {
    "objectID": "technical-details/data-collection/main.html#apis",
    "href": "technical-details/data-collection/main.html#apis",
    "title": "Data Collection",
    "section": "APIs",
    "text": "APIs\nAPIs (Application Programming Interfaces) are mechanisms that enable two software components to communicate with each other using a set of definitions and protocols. APIs enable developers to access data or functionality from a system without having to know the underlying implementation details, making it easier to integrate data from multiple sources. They act as intermediaries, providing a structured way for programs to request and retrieve information or services.2\nHow do APIs Work?3  APIs facilitate communication between applications, systems, or devices through a structured request-response cycle\n\nAPI Client: The process begins with an API client, which sends a request to the API server, which can be triggered by user interaction or external events.\nAPI Request: An API request typically contains the following components: \n\nEndpoint: URL that provides access to a specific resource.\nMethod: Indicates that action to be performed on the resource.\nParameters: Variables that are passed along with the request to customize the response.\nHeaders: Key-value pairs that provide additional details about the request, such as authentication tokens or the content format.\nRequest Body: Includes actual data required for operations like creating, updating, or deleting resources.\n\nAPI Server: Receives the request and performs actions such as authenticating the client, validating the input, and processing the request by retrieving or updating the requested data.\nAPI Response: The API server returns a response to the client, this typically includes:\n\n\nStatus Code: A numerical code indicating the result of the request (e.g., 200 for success, 201 for resource creation, or 404 for resource not found).\nHeaders: Additional metadata about the response.\nResponse Body: The data requested by the client, or an error message.\n\n\nAPI Architectural Styles:1 \n\nREST (Representational State of Resource): A widely used style for data exchange over the internet. In RESTful APIs, resources are accessed through endpoints, and standard HTTP methods such as GET, POST, PUT, and DELETE are used to perform operations on these resources.\nSOAP (Simple Object Access Protocol): Protocol that uses XML to facilitate the transfer of highly structured messages between client and sever. While it provides features for security and reliability, it can be slower compared to other architectural styles.\nGraphQL: An open-source query language designed to allow clients fetch only the data they need via a single API endpoint. This eliminates the need for multiple requests, making it valuable for applications that operate over slower or less reliable network connections.\n\nIn this Project: \nThe Ergast Developer API was used to collect race, driver standings, circuit information, and pitstop data. “The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes”4."
  },
  {
    "objectID": "technical-details/data-collection/main.html#required-libraries",
    "href": "technical-details/data-collection/main.html#required-libraries",
    "title": "Data Collection",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n\nCode\n# for sending HTTP requests\nimport requests\n# for data manipulations \nimport pandas as pd\n# for numerical computations and array manipulations\nimport numpy as np\n# for parsing and manipulating JSON data\nimport json\n# for interacting with the operating system\nimport os\n# for pattern matching, cleaning and extracting text\nimport re\n# for working with dates and times\nfrom datetime import datetime\nimport time\n# for parsing and extracting data from HTML and XML documents\nfrom bs4 import BeautifulSoup\n# for reading and writing to csv files\nimport csv\n# for car telemetry and results\nimport fastf1"
  },
  {
    "objectID": "technical-details/data-collection/main.html#race-information",
    "href": "technical-details/data-collection/main.html#race-information",
    "title": "Data Collection",
    "section": "Race Information",
    "text": "Race Information\nImportance: Contains information about the season, round, date, grand prix name, location, results,wikipedia url of the race, driver and constructor details.\n\nSeason: The year of the race.\nRound: The number of the race in the season.\nGrand Prix Name: The official name of the race.\nResults: Includes the finishing poistion and points earned.\nDriver Details: Provides information about the driver’s name, ID, nationality, and date of birth.\nConstructor Details: Constructor refers to the team that builds and maintains the cars. It contains information about the constructor’s name, ID, and nationality.\n\nSource: ERGAST API\n\n\nCode\n# funtion to get race results for a specifi year\ndef get_race_results(url, year, offset, limit=1000):\n    full_url = f\"{url}/{year}/results.json?limit={limit}&offset={offset}\"\n    # GET request\n    result = requests.get(full_url)\n    # output = JSON object\n    return result.json()\n\n\n\n\nCode\n# Testing for 2023 season\nseason_2023_json = get_race_results(url='http://ergast.com/api/f1', year=2023, offset=0)\n\n# Save the data to a JSON file\nwith open('../../data/raw-data/race_data_2023.json', 'w') as outfile:\n    json.dump(season_2023_json, outfile)\n\n\n\n\nCode\n# collecting data from 2000 to 2022\n# function to loop through years and fetch the results\ndef race_data(start_year, end_year, output_dr, url):\n\n    for year in range(start_year, end_year + 1):\n        # results for the current year\n        race_data = get_race_results(url, year, offset=0)\n        # save the output \n        output_file = os.path.join(output_dr, f\"race_data_{year}.json\")\n        # save the data to a JSON file\n        with open(output_file, 'w') as f:\n            json.dump(race_data, f)\n\n\n\n\nCode\n# call race_data()\nrace_data(\n    start_year = 2000,\n    end_year = 2009,\n    output_dr = \"../../data/raw-data\",\n    url = 'http://ergast.com/api/f1'\n)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#driver-standings",
    "href": "technical-details/data-collection/main.html#driver-standings",
    "title": "Data Collection",
    "section": "Driver Standings",
    "text": "Driver Standings\nAfter each round, drivers earn points based on their final position. These points are added to their overall tally, and the driver with the most points at the end of the season wins the World Driver’s Championship (WDC).\nImportance: Contains total points earned by each driver in every season from 2000 to 2023. It is crucial for identifying trends in driver performance over the years.\nSource: ERGAST API\n\n\nCode\n# function to fetch driver standings for a specific year\ndef driverstanding_info(url, season):\n    # construct the URL \n    full_url = f\"{url}/{season}/driverStandings.json\"\n    # GET request\n    response = requests.get(full_url)\n    return response.json()\n\n# Function to fetch and save all driver standings for the given seasons\ndef driverstandings_info(start_year, end_year, output_file, url=\"http://ergast.com/api/f1\"):\n    # for storing driver standings data\n    driver_standings_data = {}\n    \n    # loop through each year \n    for year in range(start_year, end_year + 1):\n        # results for the current year\n        data = driverstanding_info(url, year)\n        # add the data with year as key\n        driver_standings_data[year] = data\n    \n    # Save to output file\n    with open(output_file, 'w') as outfile:\n        json.dump(driver_standings_data, outfile)\n\n# Call the function for seasons 2000–2023\ndriverstandings_info(\n    start_year=2000,\n    end_year=2023,\n    output_file=\"../../data/raw-data/driver_standings/driver_standings_2000_2023.json\"\n)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#circuit-information",
    "href": "technical-details/data-collection/main.html#circuit-information",
    "title": "Data Collection",
    "section": "Circuit Information",
    "text": "Circuit Information\nThe race tracks are referred to as circuits.\nImportance: This data includes the circuit name, locality, country as well as its longitude and latitude. These values can be used for collecting weather information on the race day.\nSource: ERGAST API\n\n\nCode\ndef circuit_info(output_file, url):\n    results = requests.get(url)\n    \n    # save to output file\n    with open(output_file, 'w') as f:\n        json.dump(results.json(), f)\n        \n\n\n\n\nCode\ncircuit_info(output_file='../../data/raw-data/circuit_data.json', url = \"http://ergast.com/api/f1/circuits.json\")"
  },
  {
    "objectID": "technical-details/data-collection/main.html#news-of-top-10-drivers",
    "href": "technical-details/data-collection/main.html#news-of-top-10-drivers",
    "title": "Data Collection",
    "section": "News of Top 10 Drivers",
    "text": "News of Top 10 Drivers\nStandings in 2024 season as on 11-24-2024\nSilly Season in F1 refers to the period of speculation, rumors, and announcements surrounding driver lineups for the next season. This period typically begins during the latter half of the season, as drivers, teams, and sponsors negotiate deals for the future. Headlines during the silly season often speculate on whether drivers will extend their contract, switch teams, or retire from the sport, creating a buzz that fuels media interest.\nImportance: Analyzing news coverage about drivers can provide insights into their career trajectories, and potential moves in the upcoming season.\nSource: NEWS API\nResources: NEWS-API DSAN 5000 Lecture Content\n\n\nCode\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\nAPI_KEY='86d4dac5a4864ece92da90bc31277e53'\n\n\n\n\nCode\n# function to fetch news articles for a given topic\ndef news_data(topic, API_KEY, total_requests=1, verbose=True):\n    baseURL = \"https://newsapi.org/v2/everything?\"\n\n    # API parameters\n    URLpost = {\n        'apiKey': API_KEY,      # API key\n        'q': '+'+topic,         # topic\n        'sotBy': 'relevancy',   # sort by relevance\n        'pageSize': 100,        # maximum articles per pages = 100\n        'page': 1               # start with page 1\n    }\n    # last name of the drives to avoid spaces in the file names\n    file_name = topic.split()[-1]\n    # initialize a list to store all articles\n    all_articles = []\n\n    # loop through the number of API requests\n    for request_num in range(total_requests):\n        # make the API request\n        response = requests.get(baseURL, params=URLpost)\n        response_data = response.json()\n        # extract artivles from the response\n        articles = response_data.get('articles', [])\n        all_articles.extend(articles)\n\n        # increment the page number for next request\n        URLpost['page'] += 1\n\n\n    # output file path\n    output_dr = \"../../data/raw-data/News_Drivers\"\n    output_file = os.path.join(output_dr, f\"{file_name}_raw_text.json\")\n\n    # save to output file\n    with open(output_file, 'w') as f:\n        json.dump(all_articles, f, indent=4)\n    \n    return all_articles\n\n\n\nTop 10 Drivers as of Round 22 (Las Vegas Grand Prix)\n\nMax Verstappen\nLando Norris\nCharles Leclerc\nOscar Piastri\nCarlos Sainz\nGeorge Russell\nLewis Hamilton\nSergio Perez\nFernando Alonso\nNico Hulkenberg\n\n\n\nCode\n# testing \ntext_data = news_data('Max Verstappen', API_KEY, total_requests=1, verbose=True)\n\n\n\n\nCode\ntext_data = news_data('Lando Norris', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Charles Leclerc', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Oscar Piastri', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Carlos Sainz', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('George Russell', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Lewis Hamilton', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Sergio Perez', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Fernando Alonso', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Nico Hulkenberg', API_KEY, total_requests=1, verbose=True)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#weather-data",
    "href": "technical-details/data-collection/main.html#weather-data",
    "title": "Data Collection",
    "section": "Weather Data",
    "text": "Weather Data\nImportance: Weather conditions play a crucial role in race strategy, tire choices, and driver performance.\nSource: Wikipedia\n\n\nCode\n# the url for each race is in the race data collected using ergast API\nrace_df = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")\n\n\n\n\nCode\nrace_data = race_df[['season', 'raceName', 'url']]\n\n\n\n\nCode\nrace_data = race_data.drop_duplicates()\n\n\n\n\nCode\nrace_data.head()\n\n\n\n\n\n\n\n\n\nseason\nraceName\nurl\n\n\n\n\n0\n2010\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\n\n\n24\n2010\nAustralian Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Australian_G...\n\n\n48\n2010\nMalaysian Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Malaysian_Gr...\n\n\n72\n2010\nChinese Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Chinese_Gran...\n\n\n96\n2010\nSpanish Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Spanish_Gran...\n\n\n\n\n\n\n\n\n\nCode\n\ndef get_weather_from_wikipedia(url):\n    # GET request\n    response = requests.get(url)\n    # parse the HTML content\n    bs = BeautifulSoup(response.text, 'html.parser')  \n    \n    # locate the infobox table\n    table = bs.find('table', {'class': 'infobox infobox-table vevent'})\n    if not table:\n        print(f\"No infobox found on the page: {url}\")\n        return \"Not Available\"\n    \n    # iterate through each row for the \"Weather\" field \n    for row in table.find_all('tr'):\n        # find the header cell with class 'infobox-label'\n        header = row.find('th', {'class': 'infobox-label'})  \n        # check if it contains \"Weather\"\n        if header and 'Weather' in header.text: \n            # find the corresponding data cell with class 'infobox-data'\n            data = row.find('td', {'class': 'infobox-data'})  \n            # remove any unnecessary spaces\n            if data:\n                return data.text.strip()  \n    \n# column in the dataframe to store weather information   \nrace_data['weather'] = None\n\n# fetch weather information for each URL\nfor index, row in race_data.iterrows():\n    url = row['url']\n    # for debugging purpose\n    #print(f\"Fetching weather for: {url}\")\n    \n    # get the weather information\n    weather = get_weather_from_wikipedia(url)\n    \n    # update the weather column\n    race_data.at[index, 'weather'] = weather\n\n# save to output file\noutput_csv = \"../../data/raw-data/weather/race_data_with_weather.csv\"\nrace_data.to_csv(output_csv, index=False)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#circuit-features",
    "href": "technical-details/data-collection/main.html#circuit-features",
    "title": "Data Collection",
    "section": "Circuit Features",
    "text": "Circuit Features\nAround the F1 season, circuits vary widely in their features—some are known for tight, technical corners, others for long, high-speed straights, and a few for their narrow and challenging layouts. These unique characteristics influence car and driver performance significantly, with certain drivers or car designs excelling on specific track types.\nImportance: Analyzing racetrack features is crucial for understanding how different teams and drivers perform under varying conditions. This information can be used to classify tracks, which can further be studied to identify patterns and trends in race results.\nSource: fastf1\n\n\nCode\n#os.makedirs('cache', exist_ok=True)\n#fastf1.Cache.enable_cache('cache')\n\n\n\n\nCode\n# initialize an empty list\ntrack_data = []\n\n# function to fetch data for specific year and race\ndef extract_track_features(year, race_name):\n    # load session data for the qualifying session\n    session = fastf1.get_session(year, race_name, 'Q') \n    session.load()\n\n    # fastest lap\n    fastest_lap = session.laps.pick_fastest()\n    telemetry = fastest_lap.get_telemetry()\n\n    # track Length\n    track_length = telemetry['Distance'].iloc[-1]  # Distance of the fastest lap\n\n    # max Speed\n    max_speed = telemetry['Speed'].max()\n\n    # average Speed\n    avg_speed = track_length / fastest_lap['LapTime'].total_seconds()\n\n    # percentage of Full Throttle (throttle &gt;= 95%)\n    full_throttle = telemetry[telemetry['Throttle'] &gt;= 95]\n    perc_full_throttle = (len(full_throttle) / len(telemetry)) * 100\n\n    # calcute the number of corners based on telemetry speed (&lt; 100 km/h)\n    telemetry['is_corner'] = telemetry['Speed'] &lt; 100\n    num_corners = (telemetry['is_corner'] & ~telemetry['is_corner'].shift(1, fill_value=False)).sum()\n\n    # calculate the number of straights based on telemetry speed (&gt; 150 km/h)\n    telemetry['is_straight'] = telemetry['Speed'] &gt; 150\n    num_straights = (telemetry['is_straight'] & ~telemetry['is_straight'].shift(1, fill_value=False)).sum()\n\n    return {\n        \"Year\": year,\n        \"Grand Prix\": race_name,\n        \"Track Length (m)\": track_length,\n        \"Max Speed (km/h)\": max_speed,\n        \"Full Throttle (%)\": perc_full_throttle,\n        \"Number of Corners\": num_corners,\n        \"Number of Straights\": num_straights\n    }\n\n# for test\n# data was extracted for seasons: 2018 - 2023\n# data is available from 2018 season onwards in fastf1\nyear = 2023\nschedule = fastf1.get_event_schedule(year)\n\n# iterate through all races in the schedule\nfor _, event in schedule.iterrows():  \n    if not pd.isna(event['Session1']):  \n        try:\n            track_features = extract_track_features(year, event['EventName'])\n            track_data.append(track_features)\n        except Exception as e:\n            print(f\"Failed for {event['EventName']} in {year}: {e}\")\n\ndf_tracks = pd.DataFrame(track_data)\n\n\n\n\n\nCode\n# merging all racetrack features into a single csv\nfolder_path = \"../../data/raw-data/circuit_data/\"\n\ndataframes = []\n\nfor file_name in os.listdir(folder_path):\n    if file_name.endswith('.csv'): \n        file_path = os.path.join(folder_path, file_name)\n        df = pd.read_csv(file_path)\n        dataframes.append(df)\n\nmerged_df = pd.concat(dataframes, ignore_index=True)\n\noutput_file = \"../../data/raw-data/circuit_data/merged_circuit_features.csv\"\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\nmerged_df.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#pit-stop-data",
    "href": "technical-details/data-collection/main.html#pit-stop-data",
    "title": "Data Collection",
    "section": "Pit stop data",
    "text": "Pit stop data\nPit stop is when the car pulls in the pit lane, a designated area, for a quick maintenance, change of tires, mechanical repairs or any other actions necessary during the race. The teams have to strategically decide when to make a pit stop in order to gain a competitive advantage.\nImportance: The speed and precision of pit crews play a crucial role in minimizing the time drivers lose during a pit stop.\n\nLap: The specific lap during which the pit stop was made.\nStop: Whether it is the first, second, or subsequent stop for the driver.\nDuration: Time spent in the pit lane.\n\nSource: ERGAST API\n\n\nCode\n# data available from 2011\n\n# function to fetch pitstop data for a specific race\ndef get_pitstop_data(year, round_number):\n    # construct the url \n    url = f\"http://ergast.com/api/f1/{year}/{round_number}/pitstops.json?limit=1000\"\n    response = requests.get(url)\n    \n    # check if the request was successful\n    if response.status_code == 200 and response.text.strip():\n        try:\n            # parse and return JSON data\n            return response.json()\n        except Exception as e:\n            print(f\"Error parsing JSON for {year} Round {round_number}: {e}\")\n            return None\n    # in case of error - for tracking failures\n    else:\n        print(f\"Failed to fetch data for {year} Round {round_number}: {response.status_code}\")\n        return None\n\n# function to fetch race schedule for a specific season\ndef get_race_schedule(year):\n    url = f\"http://ergast.com/api/f1/{year}.json\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json().get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n    else:\n        print(f\"Failed to fetch schedule for {year}: {response.status_code}\")\n        return []\n\n# function to extract and save pitstop data to CSV\ndef fetch_and_save_pitstop_data(start_year, end_year, output_csv):\n    # create the CSV file and write the header\n    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n        fieldnames = [\"Year\", \"Round\", \"RaceName\", \"DriverID\", \"Lap\", \"Stop\", \"Time\", \"Duration\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        # loop through years and races\n        for year in range(start_year, end_year + 1):\n            print(f\"Fetching data for year: {year}\")\n            races = get_race_schedule(year)\n            \n            # loop through each race\n            for race in races:\n                round_number = race.get(\"round\")\n                race_name = race.get(\"raceName\")\n\n                # fetch pitstop data for the current race\n                pitstop_data = get_pitstop_data(year, round_number)\n                if pitstop_data:\n                    # extract the race list from the response\n                    races_list = pitstop_data.get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n                    \n                    # check if the response contains the data\n                    if races_list:\n                        # extract pitstops for the race\n                        pitstops = races_list[0].get(\"PitStops\", [])\n                        \n                        # write each pitstop to the CSV\n                        for pitstop in pitstops:\n                            writer.writerow({\n                                \"Year\": year,\n                                \"Round\": round_number,\n                                \"RaceName\": race_name,\n                                \"DriverID\": pitstop.get(\"driverId\"),\n                                \"Lap\": pitstop.get(\"lap\"),\n                                \"Stop\": pitstop.get(\"stop\"),\n                                \"Time\": pitstop.get(\"time\"),\n                                \"Duration\": pitstop.get(\"duration\")\n                            })\n                    else:\n                        print(f\"No race data available for {race_name} in {year}\")\n\n\n\n\nCode\n# write to output file\noutput_csv = \"../../data/raw-data/pitstop_data.csv\"\n# running the function for all seasons: 2000 - 2023\n# but the data is available only from 2011 season \nfetch_and_save_pitstop_data(\n    start_year=2000, \n    end_year=2023,\n    output_csv=output_csv\n)\n\n\nAll the datasets can be found here"
  },
  {
    "objectID": "technical-details/data-collection/main.html#footnotes",
    "href": "technical-details/data-collection/main.html#footnotes",
    "title": "Data Collection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2010 Bahrain Grand Prix↩︎\nWhat are APIs↩︎\nHow do APIs work↩︎\nErgast Developer API↩︎"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised Learning is a branch of Machine Learning where models are trained on labeled data. The primary goal of supervised learning is to learn a mapping function that can predict outputs for unseen data accurately. This approach relies on historical data to identify patterns and relationships, enabling it to generalize well on new, unseen data.\nProcess:\n\nThe model is provided with input-output pairs from historical data. Each data point consists of:\n\nFeatures (X): Independent variables or predictors.\nLabels (Y): Target outputs or dependent variables.\n\nThe model learns the relationship between the input features and corresponding outputs using a loss function that measures prediction error. The goal is to minimize this error.\nOnce trained, the model can predict outputs for new inputs.\nPerformance is evaluated using metrics like accuracy, precision, recall, F1-score for classification tasks, or RMSE, MAE for regression tasks.\n\nTypes of Supervised Learning\n\nClassification:\n\n\nThe goal is to categorize inputs into discrete classes or categories.\nAlgorithms: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forest, Naive Bayes, K-Nearest Neighbors (KNN).\n\n\nRegression:\n\n\nThe goal is to predict a continuous output based on input features.\nAlgorithms: Linear Regression, Polynomial Regression, Ridge and Lasso Regression, Support Vector Regression (SVR).\n\nChallenges:\n\nSupervised learning requires large volumes of labeled data, which can be expensive and time-consuming to obtain.\nModels may memorize training data instead of generalizing to new inputs. Regularization and cross-validation techniques help mitigate this.\nIn classification problems, imbalanced classes can affect performance.\n\nIn this project:\nSupervised Learning Algorithms are applied to classify race results and predict pit stop duration."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#correlation-analysis",
    "href": "technical-details/supervised-learning/main.html#correlation-analysis",
    "title": "Supervised Learning",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nCorrelation analysis helps identify features that are strongly correlated with the target variable.\n\n\nCode\nplt.figure(figsize=(10, 6))\ncorrelations = X.corrwith(y)\ncorrelations.sort_values().plot(kind='barh', color='deepskyblue')\nplt.title(\"Correlation Between Features and Target\")\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Correlation Coefficient\", fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.grid(axis='x')\nplt.show()"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#recursive-feature-elimination",
    "href": "technical-details/supervised-learning/main.html#recursive-feature-elimination",
    "title": "Supervised Learning",
    "section": "Recursive Feature Elimination",
    "text": "Recursive Feature Elimination\nRFE is a wrapper method that iteratively removes the least important features based on a model’s performance.\nProcess:\n\nTrain a machine learning model.\nRank features based on their importance or weights.\nRecursively eliminate the least significant feature(s) and retrain the model.\nStop when the desired number of features is selected.\n\n\n\nCode\n# inititlize logistic regression model\nmodel = LogisticRegression(max_iter=1000, random_state=123)\n# initialize RFE with the logistic regression model \nrfe = RFE(model, n_features_to_select=5)\n# fit RFE on data\nrfe.fit(X, y)\n# generate rankings for the features\n# 1 = most important \n# higher ranks indicate lower importance\nrfe_ranking = pd.Series(rfe.ranking_, index=features).sort_values()\n\nplt.figure(figsize=(10, 6))\nrfe_ranking.plot(kind='barh', color='deepskyblue')\nplt.title(\"Feature Importance by Recursive Feature Elimination (RFE)\", fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"RFE Ranking (Lower is Better)\", fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.grid(axis='x')\nplt.show()"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#mutual-information",
    "href": "technical-details/supervised-learning/main.html#mutual-information",
    "title": "Supervised Learning",
    "section": "Mutual Information",
    "text": "Mutual Information\nMutual information measures the dependency between two variables. It identifies the most informative features by quantifying how much knowing one variable reduces uncertainty about the other\n\n\nCode\n# compute MI between each features and y\nmutual_info = mutual_info_classif(X, y, random_state=123)\nmutual_info_series = pd.Series(mutual_info, index=features).sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nmutual_info_series.plot(kind='barh', color='deepskyblue')\nplt.title(\"Feature Importance by Mutual Information\", fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.xlabel(\"Mutual Information Score\", fontsize=14)\nplt.grid(axis='x')\nplt.show()\n\n\n\n\n\n\n\n\n\nSelected Features: grid,Duration1, Duration2, Lap1, Lap2, Lap3,Stop2, Stop3\nThese features have been consistenly selected as important by the Feature Selection Algorithms."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#decision-trees",
    "href": "technical-details/supervised-learning/main.html#decision-trees",
    "title": "Supervised Learning",
    "section": "Decision Trees",
    "text": "Decision Trees\nA Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It operates by recursively splitting the dataset based on the most informative features, represented as a tree-like structure of decisions. Each node in the tree represents a feature, branches represent decisions or splits, and the leaves represent the final outcome.\nProcess:\n\nDecision Trees identify the best feature for splitting the data at each step. Splits are made to maximize homogeneity within the resulting subsets.\n\nFor classification, splitting is often based on measures like Gini Impurity or Entropy (Information Gain)\nFor regression, the split minimizes the Mean Squared Error (MSE).\n\nRecursive Partitioning: The algorithm continues to split the data until a stopping condition is met, such as reaching a maximum depth, minimum samples in a node, or no further gain from splitting.\nPrediction:\n\nIn classification, predictions are based on the majority class in the leaf node.\nIn regression, predictions are based on the average value in the leaf node.\n\n\nHyperparameters:\n\nmax_depth: Limits the depth of the tree. A smaller depth reduces overfitting but may underfit the data.\nmin_samples_split: The minimum number of samples required to split a node. Increasing this value reduces overfitting by preventing small, irrelevant splits.\nmin_samples_leaf: Specifies the minimum number of samples a leaf node must contain. Larger values result in smoother decision boundaries.\ncriterion: Determines the measure of impurity.\nmax_features: Limits the number of features considered at each split, reducing overfitting and computational cost.\nmax_leaf_nodes: Limits the number of leaf nodes in the tree\n\nEvaluation Metrics:\n\nAccuracy: Measures the percentage of correctly predicted instances.\nPrecision: Fraction of true positive predictions out of all positive predictions. \\[ Precision = \\frac{TP}{TP + FP}\\]\nRecall (Sensitivity): Fraction of actual positives correctly identified. \\[ Recall = \\frac{TP}{TP + FN}\\]\nF1-Score: Harmonic mean of precision and recall, useful when dealing with imbalanced classes. \\[ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\]\nConfusion Matrix: Provides a matrix of predicted vs. actual class labels, offering insights into false positives, false negatives, etc.\nROC-AUC Score: Evaluates the trade-off between true positive rate and false positive rate.\n\nChallenges\n\nTrees tend to overfit when they grow too deep, capturing noise in the data.\nBias to Greedy Splits: Decision Trees use a greedy approach, which may lead to suboptimal splits.\nInstability: Small changes in the data can result in significant changes in the tree structure\n\n\n\nCode\ndf_transformed = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\n# create a new categortical variable - 1 if its a points finish\n#                                    - 0, otherwise\ndf_transformed['points_category'] = df_transformed['position'].apply(lambda x: 1 if x &lt;= 10 else 0)\n\n\n\n\nCode\n# Selected features\nX = df_transformed[['grid', 'Duration1', 'Duration2', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3']]\ny = df_transformed['points_category']  \n\n# Train-test split\n# splitting the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n\n\nCode\n# Initialize the Decision Tree Classifier\n# training with default values\ndt = DecisionTreeClassifier(random_state=42)\n\n# Fit the model\ndt.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = dt.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report - Decision Tree:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(dt, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Decision Tree\")\nplt.show()\n\n\nClassification Report - Decision Tree:\n              precision    recall  f1-score   support\n\n           0       0.64      0.69      0.67       114\n           1       0.71      0.66      0.68       128\n\n    accuracy                           0.67       242\n   macro avg       0.67      0.67      0.67       242\nweighted avg       0.68      0.67      0.67       242\n\n\n\n\n\n\n\n\n\n\n\nThe model performs similarly for both classes, indicated by similar recall, and f-1 scores.\n67% accuracy indcates that the model can be imporoved further with hyperparameter tuning.\nThere is no significant class imbalance.\n\n\n\nCode\n# Hyperparameter Tuning\n# Define max_depth range\nmax_depth_range = range(1, 25)\n\n# Store metrics for each max_depth\ntrain_accuracies_y1 = []  # Accuracy for y=1\ntest_accuracies_y1 = []\ntrain_recalls_y0 = []     # Recall for y=0\ntest_recalls_y0 = []\n\ntrain_accuracies = []     # Overall accuracy\ntest_accuracies = []\ntrain_recalls = []        # Overall recall for y=1\ntest_recalls = []\n\nfor max_depth in max_depth_range:\n    # Initialize Decision Tree Classifier\n    dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n    dt.fit(X_train, y_train)\n    \n    # Predictions on training and test data\n    y_train_pred = dt.predict(X_train)\n    y_test_pred = dt.predict(X_test)\n    \n    # Calculate overall accuracy\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n    \n    # Calculate recall for y=1\n    train_recalls.append(recall_score(y_train, y_train_pred, pos_label=1))\n    test_recalls.append(recall_score(y_test, y_test_pred, pos_label=1))\n    \n    # Accuracy for y=1\n    train_accuracies_y1.append(accuracy_score(y_train[y_train == 1], y_train_pred[y_train == 1]))\n    test_accuracies_y1.append(accuracy_score(y_test[y_test == 1], y_test_pred[y_test == 1]))\n    \n    # Recall for y=0\n    train_recalls_y0.append(recall_score(y_train, y_train_pred, pos_label=0))\n    test_recalls_y0.append(recall_score(y_test, y_test_pred, pos_label=0))\n\n# Plotting all metrics in a (2, 2) grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Subplot 1: Overall Accuracy\naxes[0, 0].plot(max_depth_range, train_accuracies, 'o-', label='Training', color='blue')\naxes[0, 0].plot(max_depth_range, test_accuracies, 'o-', label='Test', color='deepskyblue')\naxes[0, 0].set_title(\"Accuracy\")\naxes[0, 0].set_xlabel(\"Max Depth\")\naxes[0, 0].set_ylabel(\"Accuracy\")\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# Subplot 2: Recall for y=1\naxes[1, 0].plot(max_depth_range, train_recalls, 'o-', label='Training', color='blue')\naxes[1, 0].plot(max_depth_range, test_recalls, 'o-', label='Test', color='deepskyblue')\naxes[1, 0].set_title(\"Recall for y=1\")\naxes[1, 0].set_xlabel(\"Max Depth\")\naxes[1, 0].set_ylabel(\"Recall\")\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# Subplot 3: Recall for y=0\naxes[1, 1].plot(max_depth_range, train_recalls_y0, 'o-', label='Training', color='blue')\naxes[1, 1].plot(max_depth_range, test_recalls_y0, 'o-', label='Test', color='deepskyblue')\naxes[1, 1].set_title(\"Recall for y=0\")\naxes[1, 1].set_xlabel(\"Max Depth\")\naxes[1, 1].set_ylabel(\"Recall\")\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nfig.delaxes(axes[0, 1])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nAs the depth of the tree increases, the training accuracy of the model increases and the test accuracy of the model decreases. This is because the model starts to overfit the training data. To avoid overfitting of the model, we select the optimal number of layers when the test and training accuracy are high. In this case, we select the max depth to be 5. Beyond 5, the test accruacy significantly drops.\n\n\nCode\n# Hyper parameter tuning\n# fix max_depth = 5\n# define the parameter grid \nparam_grid = {\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 5]\n}\n\n# Perform Grid Search with max_depth fixed as 5 \ngrid_search = GridSearchCV(DecisionTreeClassifier(max_depth=5, random_state=123), \n                           param_grid, cv=5, scoring='accuracy')\n# fit the model to the training data\ngrid_search.fit(X_train, y_train)\n\n# results\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# evaluate the best model\nbest_dt = grid_search.best_estimator_\ny_pred_best = best_dt.predict(X_test)\n\n# evaluation metrics\nprint(\"Classification Report - Optimized Decision Tree with max_depth=5:\")\nprint(classification_report(y_test, y_pred_best))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(best_dt, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Decision Tree with max_depth=5\")\nplt.show()\n\n\nBest Parameters: {'min_samples_leaf': 2, 'min_samples_split': 5}\nClassification Report - Optimized Decision Tree with max_depth=5:\n              precision    recall  f1-score   support\n\n           0       0.81      0.75      0.78       114\n           1       0.79      0.84      0.82       128\n\n    accuracy                           0.80       242\n   macro avg       0.80      0.79      0.80       242\nweighted avg       0.80      0.80      0.80       242\n\n\n\n\n\n\n\n\n\n\n\nAfter hyperparameter tuning, we observe a significant increase in accuracy.\nThe precision for the turned model is also more balanced than the original model.\n\n\n\nCode\n# ROC Curve for the Initial Decision Tree\ny_proba_initial = dt.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_initial, tpr_initial, _ = roc_curve(y_test, y_proba_initial)\nroc_auc_initial = auc(fpr_initial, tpr_initial)\n\n# ROC Curve for the Optimized Decision Tree\ny_proba_optimized = best_dt.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_optimized, tpr_optimized, _ = roc_curve(y_test, y_proba_optimized)\nroc_auc_optimized = auc(fpr_optimized, tpr_optimized)\n\n# Plot both ROC curves\nplt.figure(figsize=(10, 6))\nplt.plot(fpr_initial, tpr_initial, label=f'Initial Decision Tree (AUC = {roc_auc_initial:.2f})', color='deepskyblue', lw=2)\nplt.plot(fpr_optimized, tpr_optimized, label=f'Optimized Decision Tree (AUC = {roc_auc_optimized:.2f})', color='blue', lw=2)\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  \nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves - Decision Tree')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Receiver Operating Characteristic (ROC) curve is used to evaluate the trade-off between True Positive Rate (TPR) (sensitivity/recall) and False Positive Rate (FPR) across different threshold values.\nArea Under The Curve (AUC) represents the measure of separability. Higher the AUC, the better is the model at predicting the classes.\nThe optimized Decision Tree outperforms the initial model, as evidenced by the higher AUC (0.84 vs. 0.79). Hyperparameter tuning has led to improvements in the model’s performance."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest",
    "href": "technical-details/supervised-learning/main.html#random-forest",
    "title": "Supervised Learning",
    "section": "Random Forest",
    "text": "Random Forest\nRandom Forests combine the strengths of multiple decision trees to deliver a robust, versatile, and high-performing model for classification and regression tasks.\nProcess:\n\nRandom Forest builds multiple decision trees using bootstrap aggregation, where each tree is trained on a bootstrapped subset of the data.\nAt each node, a random subset of features is considered for splitting, introducing diversity among the trees and reducing overfitting.\nEach tree is grown independently to its maximum depth or based on stopping criteria like max_depth or min_samples_split.\nFor classification tasks, the final prediction is determined through a majority vote from all trees. For regression tasks, the final output is obtained by averaging the predictions of all trees.\nBy combining outputs from multiple decision trees, Random Forest reduces variance, improves generalization, and enhances model robustness.\n\n\n\nCode\ndf_transformed = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\ndf_transformed['points_category'] = df_transformed['position'].apply(lambda x: 1 if x &lt;= 10 else 0)\nX = df_transformed[['grid', 'Duration1', 'Duration2', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3']]\ny = df_transformed['points_category']  \n\n# Train-test split\n# splitting the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n\n\nCode\n# Initialize Random Forest Classifier\nrf = RandomForestClassifier(random_state=123)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict\ny_pred = rf.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Random Forest\")\nplt.show()\n\n# Accuracy Score\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.79      0.76      0.78       114\n           1       0.80      0.82      0.81       128\n\n    accuracy                           0.79       242\n   macro avg       0.79      0.79      0.79       242\nweighted avg       0.79      0.79      0.79       242\n\n\n\n\n\n\n\n\n\n\nAccuracy: 0.7933884297520661\n\n\nThe Random Forest Classifier, using its default hyperparameters, achieved an accuracy of approximately 0.80. Comparable to a Decision Tree after hyperparameter tuning, this highlights the superior performance and robustness of Random Forests, especially when handling large datasets, as they effectively reduce overfitting compared to a single decision tree.\n\n\nCode\n# hyperparameter tuning\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],    \n    'min_samples_split': [2, 5, 10]\n}\n\n# grid search\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=123),\n                           param_grid=param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n# results\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# evaluate with best parameters\nbest_rf = grid_search.best_estimator_\ny_pred_best = best_rf.predict(X_test)\n\n# Classification Report\nprint(\"Classification Report - Optimized Random Forest:\")\nprint(classification_report(y_test, y_pred_best))\n\n\nBest Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 200}\nClassification Report - Optimized Random Forest:\n              precision    recall  f1-score   support\n\n           0       0.80      0.78      0.79       114\n           1       0.81      0.83      0.82       128\n\n    accuracy                           0.81       242\n   macro avg       0.81      0.80      0.80       242\nweighted avg       0.81      0.81      0.81       242\n\n\n\n\n\nCode\n# Confusion Matrix for Optimized Random Forest\nConfusionMatrixDisplay.from_estimator(best_rf, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Random Forest\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# ROC Curve for the Initial Random Forest\ny_proba_initial = rf.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_initial, tpr_initial, _ = roc_curve(y_test, y_proba_initial)\nroc_auc_initial = auc(fpr_initial, tpr_initial)\n\n# ROC Curve for the Optimized Random Forest\ny_proba_optimized = best_rf.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_optimized, tpr_optimized, _ = roc_curve(y_test, y_proba_optimized)\nroc_auc_optimized = auc(fpr_optimized, tpr_optimized)\n\n# Plot ROC Curves\nplt.figure(figsize=(10, 6))\nplt.plot(fpr_initial, tpr_initial, label=f'Initial Random Forest (AUC = {roc_auc_initial:.2f})', color='deepskyblue', lw=2)\nplt.plot(fpr_optimized, tpr_optimized, label=f'Optimized Random Forest (AUC = {roc_auc_optimized:.2f})', color='blue', lw=2)\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  # Diagonal line for random guessing\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves - Random Forest')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThe Random Forest Classifier has outperformed the Decision Tree in terms of accuracy, demonstrating a higher capability to classify race outcomes more effectively using the pit stop data.\nThis also suggests that race outcomes are significantly influenced by pit stop strategies, highlighting an important insight that can help predict potential race winners."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest-classifier",
    "href": "technical-details/supervised-learning/main.html#random-forest-classifier",
    "title": "Supervised Learning",
    "section": "Random Forest Classifier",
    "text": "Random Forest Classifier\n\n\nCode\n# Create a new column with binned classes based on 'position'\n# multiclass categorical variable\ndef position_to_class(position):\n    if 1 &lt;= position &lt;= 5:\n        return 1\n    elif 6 &lt;= position &lt;= 10:\n        return 2\n    elif 11 &lt;= position &lt;= 15:\n        return 3\n    elif 16 &lt;= position &lt;= 20:\n        return 4\n    else:\n        return None  \n\ndf_transformed['position_class'] = df_transformed['position'].apply(position_to_class)\n\ndf_transformed = df_transformed.dropna(subset=['position_class'])\n\n# Define features and target\nX = df_transformed[['grid', 'Duration1', 'Duration2', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3']]\ny = df_transformed['position_class']\n\n\n\n\nCode\n# Train-test split\n# splitting the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n\n\nCode\n# Initialize Random Forest Classifier\nrf_multi = RandomForestClassifier(random_state=123)\n\n# Train the model\nrf_multi.fit(X_train, y_train)\n\n# Predict\ny_pred_multi = rf_multi.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report - Multi-class Random Forest:\")\nprint(classification_report(y_test, y_pred_multi))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(rf_multi, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Multi-class Random Forest\")\nplt.show()\n\n\nClassification Report - Multi-class Random Forest:\n              precision    recall  f1-score   support\n\n           1       0.65      0.80      0.72        65\n           2       0.39      0.41      0.40        63\n           3       0.43      0.46      0.44        61\n           4       0.57      0.28      0.38        46\n\n    accuracy                           0.51       235\n   macro avg       0.51      0.49      0.48       235\nweighted avg       0.51      0.51      0.49       235\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Hyperparameter tuning\nparam_grid_multi = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# perform Grid Search\ngrid_search_multi = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n                                 param_grid=param_grid_multi, cv=5, scoring='accuracy')\ngrid_search_multi.fit(X_train, y_train)\n\n# results\nprint(\"Best Parameters:\", grid_search_multi.best_params_)\n\n# evaluate with best parameters\nbest_rf_multi = grid_search_multi.best_estimator_\ny_pred_best_multi = best_rf_multi.predict(X_test)\n\n# classification report\nprint(\"Classification Report - Optimized Multi-class Random Forest:\")\nprint(classification_report(y_test, y_pred_best_multi))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(best_rf_multi, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Multi-class Random Forest\")\nplt.show()\n\n\nBest Parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\nClassification Report - Optimized Multi-class Random Forest:\n              precision    recall  f1-score   support\n\n           1       0.66      0.75      0.71        65\n           2       0.38      0.40      0.39        63\n           3       0.44      0.51      0.47        61\n           4       0.58      0.30      0.40        46\n\n    accuracy                           0.51       235\n   macro avg       0.52      0.49      0.49       235\nweighted avg       0.51      0.51      0.50       235"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn",
    "href": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn",
    "title": "Supervised Learning",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\nKNN is a non-parametric and instance-based learning algorithm, meaning it makes no assumptions about the underlying data distribution and makes predictions based on the similarity of the input data to its neighbors.\nProcess:\n\nThe algorithm calculates the distance between the input (test) data point and all training data points.\nBased on the distance metric, KNN identifies the k-nearest neighbors to the test data point.\nPrediction:\n\nClassification: The algorithm assigns the class label that is the majority among the k-nearest neighbors (majority voting).\nRegression: The algorithm averages the values of the k-nearest neighbors to predict the outcome.\nHyperparameters: K – number of neighbors.\n\n\n\n\nCode\n# Range of k values to test\nk_values = range(1, 25)\n\n# Lists to store training and test accuracy\ntrain_accuracies = []\ntest_accuracies = []\n\nfor k in k_values:\n    # Initialize KNN with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    # Train the model\n    knn.fit(X_train, y_train)\n    \n    # Predict on training and test data\n    y_train_pred = knn.predict(X_train)\n    y_test_pred = knn.predict(X_test)\n    \n    # Calculate accuracy for training and test data\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n\n# Plot accuracy vs. k value\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, train_accuracies, label=\"Training Accuracy\", marker='o', color='blue')\nplt.plot(k_values, test_accuracies, label=\"Test Accuracy\", marker='o', color='deepskyblue')\nplt.xticks(k_values)\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs. Number of Neighbors (k)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nOptimal number for k is 19. Beyond 19, the test accuracy starts to drop.\n\n\nCode\n# Initialize KNN with k=19\nknn = KNeighborsClassifier(n_neighbors=19)\n\n# Train the model\nknn.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = knn.predict(X_test)\n\n# Evaluate Performance\nprint(\"Classification Report - KNN (k=19):\")\nprint(classification_report(y_test, y_pred))\n\n# Plot Confusion Matrix\nConfusionMatrixDisplay.from_estimator(knn, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - KNN (k=19)\")\nplt.show()\n\n\nClassification Report - KNN (k=19):\n              precision    recall  f1-score   support\n\n           1       0.68      0.80      0.74        65\n           2       0.41      0.52      0.46        63\n           3       0.44      0.39      0.42        61\n           4       0.40      0.22      0.28        46\n\n    accuracy                           0.51       235\n   macro avg       0.49      0.48      0.47       235\nweighted avg       0.49      0.51      0.49       235\n\n\n\n\n\n\n\n\n\n\nAcross models, the first class is consistently classified with the highest accuracy. This indicates that as the granularity of race results increases, classifying exact outcomes based solely on pit stop data becomes more challenging. While it is relatively easier to determine whether a driver will score points or not, accurately predicting the precise finishing position proves to be much more difficult."
  },
  {
    "objectID": "technical-details/data-cleaning/DataCleaning.html",
    "href": "technical-details/data-cleaning/DataCleaning.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nRaw data collected from various sources is rarely in a format ready for analysis. It often contains inconsistencies, missing values, duplicates, and irrelevant information, which can hinder the analytical process and lead to inaccurate or biased results. Data Cleaning is the process of transforming this messy data into a structured, consistent, and reliable format, suitable for extracting meaningful insights and applying models effectively.\nImportance:\n\nData cleaning ensures that the data is complete, accurate, and consistent, significantly enhancing the reliability of insights derived from the analysis and the performance of models built using the data. High-quality data forms the foundation of robust models and predictions.\nMissing values in the dataset can introduce bias or distort the analysis. Addressing these gaps through techniques like imputation (filling in missing values using statistical or logical methods) or removal helps maintain the integrity of results.\nDuplicates in the dataset can overrepresent certain patterns, while outliers may distort metrics and affect model accuracy. Identifying and appropriately handling these anomalies ensures the analysis remains valid and unbiased.\nWhen data is collected from multiple sources, differences in format, naming conventions, and measurement units can cause inconsistencies. Standardizing these elements across all datasets ensures that the data can be seamlessly integrated and analyzed as a whole.\n\nIn this project:\nData was collected from a variety of sources, Data Cleaning was an essential step. Each raw dataset underwent a tailored cleaning process designed to fit its specific structure and use case. These processes will be discussed in detail within the respective sections dedicates to each dataset."
  },
  {
    "objectID": "technical-details/data-collection/DataCollection.html",
    "href": "technical-details/data-collection/DataCollection.html",
    "title": "Introduction",
    "section": "",
    "text": "In this section, we focus on the methods and sources used to collect data that are relevant to the research questions of this project. The quality of the data collected determines the accuracy of insights and predictions. Poor data collection practices can lead to biased analysis, inaccurate results, and ineffective models. Therefore, careful planning and execution of data collection is essential for the success of any data-driven project.\nChallenges in Data Collection Some important points to be considered when collecting data are:\n\nData quality: Ensuring the data is accurate, complete, and relevant to the research questions is crucial.\nInconsistencies: Data form different sources may have different formats, structures, naming conventions, and units of measurement, requiring a thorough understanding of the data for pre-processing.\nEthics and Privacy: Data collection methods must adhere to ethical guidelines, ensuring that no sensitive or private information is collected or misused.\nData Bias: Collecting data from sources that introduces bias can lead to inaccurate results and models.\nTechnical Constraints: Issues such as API rate limits, website restrictions, or incomplete data can hinder data collection.\n\nBy addressing these challenges, it is ensured that the data collected was relevant, accurate, and reliable."
  },
  {
    "objectID": "technical-details/data-collection/DataCollection.html#web-scraping",
    "href": "technical-details/data-collection/DataCollection.html#web-scraping",
    "title": "Introduction",
    "section": "Web Scraping",
    "text": "Web Scraping\nWeb Scraping is an automatic way to collect data from websites. It involves the use of automated scripts or tools to interact with the website’s structure to retrieve information. The data is extracted using selectors like tags, classes, or IDs. While web scraping can be tailored to collect a variety od data types from multiple web pages, one should be aware of the website’s terms of service and ethical scraping practices, and manage rate limits to avoid being blocked.\n\nIn this Project: Using Python libraries like requests and BeautifulSoup, weather information was extracted from infoboxes of each race’s Wikipedia pages.\n\nProcess:\n- The URLs for each race were obtained from the race data collected, these links were used to locate the Wikipedia pages of each race.\n- HTTP requests were sent to the Wikipedia pages to retrieve the HTML content.\n- Parsing the HTML using BeautifulSoup locate the infobox containing the race metadata.\n- Locating and extracting the Weather field from the table.\n1"
  },
  {
    "objectID": "technical-details/data-collection/DataCollection.html#apis",
    "href": "technical-details/data-collection/DataCollection.html#apis",
    "title": "Introduction",
    "section": "APIs",
    "text": "APIs\nAPIs (Application Programming Interfaces) are mechanisms that enable two software components to communicate with each other using a set of definitions and protocols. APIs enable developers to access data or functionality from a system without having to know the underlying implementation details, making it easier to integrate data from multiple sources. They act as intermediaries, providing a structured way for programs to request and retrieve information or services.2\nHow do APIs Work?3  APIs facilitate communication between applications, systems, or devices through a structured request-response cycle\n\nAPI Client: The process begins with an API client, which sends a request to the API server, which can be triggered by user interaction or external events.\nAPI Request: An API request typically contains the following components: \n\nEndpoint: URL that provides access to a specific resource.\nMethod: Indicates that action to be performed on the resource.\nParameters: Variables that are passed along with the request to customize the response.\nHeaders: Key-value pairs that provide additional details about the request, such as authentication tokens or the content format.\nRequest Body: Includes actual data required for operations like creating, updating, or deleting resources.\n\nAPI Server: Receives the request and performs actions such as authenticating the client, validating the input, and processing the request by retrieving or updating the requested data.\nAPI Response: The API server returns a response to the client, this typically includes:\n\n\nStatus Code: A numerical code indicating the result of the request (e.g., 200 for success, 201 for resource creation, or 404 for resource not found).\nHeaders: Additional metadata about the response.\nResponse Body: The data requested by the client, or an error message.\n\n\nAPI Architectural Styles:1 \n\nREST (Representational State of Resource): A widely used style for data exchange over the internet. In RESTful APIs, resources are accessed through endpoints, and standard HTTP methods such as GET, POST, PUT, and DELETE are used to perform operations on these resources.\nSOAP (Simple Object Access Protocol): Protocol that uses XML to facilitate the transfer of highly structured messages between client and sever. While it provides features for security and reliability, it can be slower compared to other architectural styles.\nGraphQL: An open-source query language designed to allow clients fetch only the data they need via a single API endpoint. This eliminates the need for multiple requests, making it valuable for applications that operate over slower or less reliable network connections.\n\nIn this Project: \nThe Ergast Developer API was used to collect race, driver standings, circuit information, and pitstop data. “The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes”4."
  },
  {
    "objectID": "technical-details/data-collection/DataCollection.html#footnotes",
    "href": "technical-details/data-collection/DataCollection.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2010 Bahrain Grand Prix↩︎\nWhat are APIs↩︎\nHow do APIs work↩︎\nErgast Developer API↩︎"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nMotivation: Explain the reasoning behind the work, such as solving a specific problem, improving a system, or optimizing performance.\nObjectives: Outline the specific outcomes you aim to achieve, whether it’s implementing a solution, analyzing data, or building a model.\n\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/eda/WordClouds.html",
    "href": "technical-details/eda/WordClouds.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Analysis\nMax Verstappen\n\n\n\n\n\n\n\n\n\n\nStrong occurence of “Norris” suggests the media discussion about the rivalry and cmpetition between the top two drivers.\nThe presence of “penalty” indicates that articles may have covered controversies or penalties during races that involved Verstappen. This adds to the narrative of the challenges and incidents faced during his title run.\nWords such as “title” and “world champion” reinforce the focus on Max Verstappen’s achievements, particularly his dominance throughout the season and his consecutive championship wins."
  },
  {
    "objectID": "technical-details/eda/points_driver.html",
    "href": "technical-details/eda/points_driver.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Insights:\n\nA significant portion of the total championship points for most drivers has been earned while driving for top-performing teams such as Ferrari and Red Bull.\nThere is a clear correlation between the points scored in a season and the team a driver represented, emphasizing the importance of team performance in a driver’s success.\nObserving the career trajectories of drivers like Daniel Riccardo, Kimi Räikkönen, and Sebastian Vettel, we notice a pattern where they initially drove for top teams, followed by a move to midfield teams before retiring.\nSome drivers switched teams despite scoring a substantial number of championship points in the previous season. This highlights the strategic decisions made by teams in planning their driver line-ups and the drivers’ foresights regarding the potential improvements and competitiveness of cars across the grid."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Formula 1 Through Data: Uncovering What Drives Success on the Track",
    "section": "",
    "text": "Introduction\nFormula 1 (F1) is widely regarded as the pinnacle of motorsport, where cutting-edge technology, driver expertise, and strategic decision-making converge to create one of the most intricate and competitive sports globally. Spanning diverse circuits across the world, each Grand Prix epitomizes engineering excellence, precise strategy execution, and split-second decisions that often determine success or failure. The ultimate goal for teams and drivers each season is to secure the World Drivers’ Championship (WDC) and World Constructors’ Championship (WCC), where fractions of a second can be the difference between winning and losing. A team or driver’s success in F1 is influenced by several interdependent factors:\n\nDriver Skill and Adaptability: A driver’s ability to adjust to changing race conditions, car dynamics, and strategic demands is vital.\nConstructor Performance: The technical prowess of the car, including aerodynamics, power unit optimization, and tire management, plays a significant role in determining outcomes.\nCircuit Features: Each track is unique, defined by characteristics like track length, number of corners, straights, and the percentage of time drivers spend at full throttle, all of which affect race strategy.\nPit Stop Strategy: Fast and efficient pit stops are critical for success, helping drivers maintain competitive positions or gain an edge over rivals through well-timed stops.\n\nIn F1, data analysis is just as vital as the skill of the driver or the technological advancements of the car. During a race, teams collect vast amounts of data, monitoring key parameters such as tire wear, fuel consumption, aerodynamic efficiency, lap times, and pit stop durations. These data points offer opportunities to optimize car setups, refine race strategies, and predict future outcomes. However, integrating and analyzing these factors collectively remains a complex challenge, providing a rich foundation for data-driven research.\nThis project harnesses Exploratory Data Analysis (EDA) and machine learning techniques to identify relationships hidden within F1 racing data. By exploring driver performance, constructor success, pit stop efficiency, and circuit characteristics, this research seeks to uncover insights into how strategy, engineering decisions, and team execution collectively influence race results.\n\n\nResearch Questions\n\nHow does a driver’s performance evolve when they switch teams, and what role do constructors play in shaping their success?\nCan patterns or trends in pit stop data reveal insights into team strategies and race outcomes?\nCan data-driven models predict race outcomes, such as point finishes, podium placements, or race wins, based on race and pit stop data?\nWhat influence do pit stop duration, frequency, and timing have on overall race success for drivers and teams?\nAre certain circuit types, like high-speed tracks or technical layouts, better suited to specific constructors?\n\n\n\nLiterature Review\nThere is a limited body of research focused on Formula 1, with most studies concentrating on technological advancements in motorsport racing. However, a few notable works provide unique insights into the sport. For instance, research has explored topics such as determining the greatest driver of all time1, analyzing the impact of teams selling their technology to competitors on overall performance2, and examining how environmental changes influence team performance3.\n\n\n\n\n\nReferences\n\n1. Eichenberger, R., Stadelmann, D., et al. Who is the best formula 1 driver? An economic approach to evaluating talent. Economic Analysis and Policy 39, 389 (2009).\n\n\n2. Aversa, P., Furnari, S. & Haefliger, S. Business model configurations and performance: A qualitative comparative analysis in formula one racing, 2005–2013. Industrial and Corporate Change 24, 655–676 (2015).\n\n\n3. Marino, A., Aversa, P., Mesquita, L. & Anand, J. Driving performance via exploration in changing environments: Evidence from formula one racing. Organization Science 26, 1079–1100 (2015)."
  }
]