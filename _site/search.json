[
  {
    "objectID": "technical-details/unsupervised-learning/main.html",
    "href": "technical-details/unsupervised-learning/main.html",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Unsupervised Learning is a branch of Machine Learning that involves using algorithms to analyze unlabeled data. Unlike supervised learning, which relies on input-output pairs, unsupervised learning does not have a predefined outcomes to predict. Instead, it focuses on discovering hidden patterns, clusters, or structures within the data without explicit instructions.\nUnsupervised Learning Algorithms:\n\nClustering: Grouping data based on their similarity\nDimensionality Reduction: reducing the number of features in the dataset while preserving key information.\nAssociation Rule Learning: Identifying relationships between variables in a dataset. In this project, Dimensionality reduction and Clustering algorithms are used, which will be explored in detail in their respective section.\n\nImportance\n\nUnsupervised learning algorithms can identify natural groupings in the data, revealing insights that might not be obvious to humans.\nTechniques like dimensionality reduction simplify data by reducing its features while retaining important information, making it easier to visualize and analyze.\nLabeling data could be a costly process, unsupervised learning provides a scalable approach to uncover patterns without requiring human intervention.\n\nIn this project:\n\n\nDimensionality reduction is applied to pit stop data to simplify the dataset and enhance the interpretability of subsequent analysis.\nClustering is employed to:\n\nAnalyze pit stop strategies and uncover potential groupings.\nGroup circuits based on their features."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/main.html#suggested-page-structure",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#what-to-address",
    "href": "technical-details/unsupervised-learning/main.html#what-to-address",
    "title": "Unsupervised Learning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/eda/main.html",
    "href": "technical-details/eda/main.html",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The primary goal of Exploratory Data Analysis (EDA) is to gain a comprehensive understanding of the data, its quality, identify any underlying trends or relationships that may influence the analysis and modeling process.\nImportance:\n\nEDA helps detect outliers, and inconsistencies in the data, addressing these issues ensures that the analysis is based on accurate and clean data.\nThrough visualizations and statistical analysis, EDA identifies which features are most relevant to the questions and should be included in the model.\nEDA reveals relationships between variables, such as correlations, and trends, which can help discard certain features or create new derived features.\nBy analyzing the distribution of variables, we can identify central tendencies and variations. This helps us understand how the data is spread out and it it meets assumptions required by certain algorithms.\n\nTechniques:\n\nUnivariate Analysis:\n\nTo understand the distribution, and central tendency of a variable.\nVisualizations: Histograms, Box Plots, kernel Density plots.\nStatistical measures: Mean, Median, Mode, Variance, Standard Deviation.\n\nBivariate Analysis:\n\nTo understand the relationship between two variables\nCategorical-Categorical: Heatmaps.\nNumerical-Categorical: Box plots, Bar Graphs.\nNumerical-Numerical: Scatter plots, correlation coefficients.\n\nMultivariate Analysis:\n\nTo explore relationships between three or more variables simultaneously.\nVisualizations: Heatmaps, Scatter plots.\nDimensionality Reduction: PCA, t-SNE.\n\nData Distribution:\n\nHistograms and density plots are useful to detect skewness or multimodal distributions.\n\nStatistical Analysis:\n\nHypothesis Testing: t-tests, chi-square tests, ANOVA.\nCorrelation: Pearson, Spearman correlation coefficients\nSummary Statistics: Mean, variance, Quartiles.\n\n\nIn this project:\nThere are various datasets, and mutiple independent features such as: Driver Performance, Pit stop durations, race track features, overall performance of the car, which play an important role in determining the outcome of the race. Therefore, EDA is crucial to understand ow these factors influence race results."
  },
  {
    "objectID": "technical-details/eda/main.html#suggested-page-structure",
    "href": "technical-details/eda/main.html#suggested-page-structure",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/main.html#what-to-address",
    "href": "technical-details/eda/main.html#what-to-address",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html",
    "href": "technical-details/data-cleaning/main.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Raw data collected from various sources is rarely in a format ready for analysis. It often contains inconsistencies, missing values, duplicates, and irrelevant information, which can hinder the analytical process and lead to inaccurate or biased results. Data Cleaning is the process of transforming this messy data into a structured, consistent, and reliable format, suitable for extracting meaningful insights and applying models effectively.\nImportance:\n\nData cleaning ensures that the data is complete, accurate, and consistent, significantly enhancing the reliability of insights derived from the analysis and the performance of models built using the data. High-quality data forms the foundation of robust models and predictions.\nMissing values in the dataset can introduce bias or distort the analysis. Addressing these gaps through techniques like imputation (filling in missing values using statistical or logical methods) or removal helps maintain the integrity of results.\nDuplicates in the dataset can overrepresent certain patterns, while outliers may distort metrics and affect model accuracy. Identifying and appropriately handling these anomalies ensures the analysis remains valid and unbiased.\nWhen data is collected from multiple sources, differences in format, naming conventions, and measurement units can cause inconsistencies. Standardizing these elements across all datasets ensures that the data can be seamlessly integrated and analyzed as a whole.\n\nIn this project:\nData was collected from a variety of sources, Data Cleaning was an essential step. Each raw dataset underwent a tailored cleaning process designed to fit its specific structure and use case. These processes will be discussed in detail within the respective sections dedicates to each dataset."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/main.html#suggested-page-structure",
    "title": "Data Cleaning",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#general-comments",
    "href": "technical-details/data-cleaning/main.html#general-comments",
    "title": "Data Cleaning",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#what-to-address",
    "href": "technical-details/data-cleaning/main.html#what-to-address",
    "title": "Data Cleaning",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html",
    "href": "technical-details/supervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results.\n\n\n\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling.\n\n\n\n\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used\n\n\n\n\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset.\n\n\n\n\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc.\n\n\n\n\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots).\n\n\n\n\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#what-to-address",
    "href": "technical-details/supervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "href": "technical-details/supervised-learning/instructions.html#data-preprocessing",
    "title": "Instructions",
    "section": "",
    "text": "Normalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-selection",
    "href": "technical-details/supervised-learning/instructions.html#model-selection",
    "title": "Instructions",
    "section": "",
    "text": "Model Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/instructions.html#training-and-testing-strategy",
    "title": "Instructions",
    "section": "",
    "text": "Split Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/instructions.html#model-evaluation-metrics",
    "title": "Instructions",
    "section": "",
    "text": "Binary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#results",
    "href": "technical-details/supervised-learning/instructions.html#results",
    "title": "Instructions",
    "section": "",
    "text": "Model Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/instructions.html#discussion",
    "href": "technical-details/supervised-learning/instructions.html#discussion",
    "title": "Instructions",
    "section": "",
    "text": "Result Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "technical-details/llm-usage-log.html",
    "href": "technical-details/llm-usage-log.html",
    "title": "LLM usage log",
    "section": "",
    "text": "Proofreading\nStreamline lengthy algorithm explanations"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#brainstorming",
    "href": "technical-details/llm-usage-log.html#brainstorming",
    "title": "LLM usage log",
    "section": "Brainstorming",
    "text": "Brainstorming\n\nTo create the initial idea, LLM tools were used to brainstorm ideas and provide feedback and refine the project plan."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing",
    "href": "technical-details/llm-usage-log.html#writing",
    "title": "LLM usage log",
    "section": "",
    "text": "Proofreading\nStreamline lengthy algorithm explanations"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#code",
    "href": "technical-details/llm-usage-log.html#code",
    "title": "LLM usage log",
    "section": "Code:",
    "text": "Code:\n\nCode commenting and explanatory documentation1"
  },
  {
    "objectID": "technical-details/data-collection/overview.html",
    "href": "technical-details/data-collection/overview.html",
    "title": "Overview",
    "section": "",
    "text": "Overview\nIn this section, provide a high-level overview for technical staff, summarizing the key tasks and processes carried out here. Include the following elements:\n\nGoals: Clearly define the purpose of the tasks or analysis being conducted.\nMotivation: Explain the reasoning behind the work, such as solving a specific problem, improving a system, or optimizing performance.\nObjectives: Outline the specific outcomes you aim to achieve, whether it’s implementing a solution, analyzing data, or building a model.\n\nThis overview should give technical staff a clear understanding of the context and importance of the work, while guiding them on what to focus on in the details that follow."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html",
    "href": "technical-details/data-collection/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instruction once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling.\n\n\n\nBegin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work.\n\n\n\n\nDuring the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder\n\n\n\n\n\nYour data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "href": "technical-details/data-collection/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#what-to-address",
    "href": "technical-details/data-collection/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#start-collecting-data",
    "href": "technical-details/data-collection/instructions.html#start-collecting-data",
    "title": "Instructions",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "href": "technical-details/data-collection/instructions.html#saving-the-raw-data",
    "title": "Instructions",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/instructions.html#requirements",
    "href": "technical-details/data-collection/instructions.html#requirements",
    "title": "Instructions",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html",
    "href": "technical-details/data-cleaning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you on this page will be specific you your project and data. Some things might “make more sense” on other pages, depending on your workflow, for example, you might feel that normalization and scaling should be included in a later section, dealing with machine learning, rather than here, that is totally fine. Organize your project in the way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\n\nIterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning.\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "href": "technical-details/data-cleaning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#general-comments",
    "href": "technical-details/data-cleaning/instructions.html#general-comments",
    "title": "Instructions",
    "section": "",
    "text": "Iterative Process: Data cleaning is often not a one-time process. As your analysis progresses, you may need to revisit the cleaning phase, and re-run the code, to adjust to new insights or requirements.\nClarity and Reproducibility: Ensure your documentation is clear and thorough. Others should be able to follow your steps and achieve the same results.\nVisualizations: Use before-and-after visualizations to illustrate the impact of your cleaning steps, making the process more intuitive and transparent.\n\nBy the end of this phase, your cleaned data should be well-documented and ready for further stages, such as Exploratory Data Analysis (EDA) and Machine Learning."
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#what-to-address",
    "href": "technical-details/data-cleaning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe Data Cleaning page of your portfolio is where you document the process of transforming your raw data into a usable format. Data cleaning is essential for ensuring the quality of your analysis, and this page should serve as a clear and reproducible guide for anyone reviewing your work. It also provides transparency, allowing others to trace the steps you took to prepare your data.\nThe following is a guide to help you get started with possible thing to address on this page .\n\nDescription of the Data Cleaning Process: Explain the steps you took to clean and preprocess the data.\nCode Documentation: Provide the code used in the data cleaning process (link to GitHub or embed the code directly).\nProvide examples of data before and after cleaning: e.g. with df.head() or df.describe()\nRaw and Cleaned Data Links: Ensure your page links to both the original (raw) dataset and the cleaned dataset. (please keep organized and store the cleaned data in data/processed-data, or similar location which doesn’t get synced to GitHub)\n\nPossible things to include:\nIntroduction to Data Cleaning:\n\nProvide a brief explanation of the data cleaning phase, its importance in preparing the data for further analysis (EDA, modeling), and its iterative nature.\nMention that data cleaning may need to be revisited as the project evolves and analysis goals change.\n\nManaging Missing Data:\n\nIdentify Missing Values: Explain how you identified missing data and where it occurred.\nHandling Missing Data: Describe how missing values were addressed (e.g., imputation, removal of rows/columns).\nVisualize Missing Data: Include visualizations (e.g., heatmaps) showing missing values before and after handling them.\n\nOutlier Detection and Treatment:\n\nIdentify Outliers: Describe the methods you used to detect outliers in the dataset.\nAddressing Outliers: Explain how outliers were treated (e.g., removal, transformation, or retaining them for analysis).\nVisualize Outliers: Use visualizations (e.g., box plots) to show how outliers were managed.\n\nData Type Correction and Formatting:\n\nReview Data Types: Summarize the types of variables (numerical, categorical, date-time, etc.) and ensure they are correctly formatted.\nTransformation: Document any transformations performed, such as converting date formats, handling categorical variables, or encoding labels.\nImpact of Changes: Briefly explain why these changes were necessary for accurate analysis.\n\nNormalization and Scaling:\n\nData Distribution Analysis: Check and discuss the distribution of numerical variables (e.g., skewness).\nNormalization Techniques: Describe any normalization or scaling techniques used (e.g., min-max scaling, z-score normalization).\nBefore-and-After Visualizations: Provide visualizations comparing the data before and after scaling or normalization.\n\nSubsetting the Data:\n\nData Filtering: Explain any subsetting or filtering of the data (e.g., selecting quantitative or qualitative columns).\nRationale: Justify why you chose to work with a particular subset of the data."
  },
  {
    "objectID": "instructions/website-structure.html",
    "href": "instructions/website-structure.html",
    "title": "Website project structure",
    "section": "",
    "text": "Please at miniumum include the following pages in your website:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\nSupervised-learning\nLLM-usage\nProgress-log\n\n\nPlease adhere closely to this structure, for consistency accross projects.\nSub-sections can be handles as markdown headers in the respective pages.\nYou can add more pages,and if you want, you can merge EDA and unsupervised learning into one page, since the are similar. Or make these section headers in the dropdown menu, for further sub-sections creation.\nFor example:\n\nLanding page\nReport\nTechnical details\n\nData collection\nData cleaning\nEDA\nUnsupervised-learning\n\nClustering\nDimensionality Reduction\n\nSupervised-learning\n\nFeature selection\n\nregression\nclassification\n\nClassification\n\nBinary classification\nMulti-class classification\n\nRegression\n\nLLM-usage\nProgress-log\n\n\nImportant: Exactly what you put on these pages will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\nA skeleton of the recommended version of the website is provided in the github classroom repository.\n./\n├── README.md\n├── _quarto.yml\n├── assets\n│   ├── gu-logo.png\n│   ├── nature.csl\n│   └── references.bib\n├── build.sh\n├── data\n│   ├── processed-data\n│   │   └── countries_population.csv\n│   └── raw-data\n│       └── countries_population.csv\n├── index.qmd\n├── instructions\n│   ├── expectations.qmd\n│   ├── github-usage.qmd\n│   ├── llm-usage.qmd\n│   ├── overview.qmd\n│   ├── quarto-tips.qmd\n│   ├── topic-selection.qmd\n│   └── website-structure.qmd\n├── report\n│   └── report.qmd\n└── technical-details\n    ├── data-cleaning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── data-collection\n    │   ├── closing.qmd\n    │   ├── instructions.qmd\n    │   ├── main.ipynb\n    │   ├── methods.qmd\n    │   └── overview.qmd\n    ├── eda\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    ├── llm-usage-log.qmd\n    ├── progress-log.qmd\n    ├── supervised-learning\n    │   ├── instructions.qmd\n    │   └── main.ipynb\n    └── unsupervised-learning\n        ├── instructions.qmd\n        └── main.ipynb\nAlways strive to incorporate the following:\n\nStructure: Use clear headings and subheadings to break down each section of your EDA.\nClarity: Provide concise explanations for all tables and visualizations, ensuring they are easy to interpret.\nCode Links: Link to relevant code (e.g., GitHub) or embed code snippets for transparency and reproducibility.\nReproducibility: Make your EDA reproducible by providing access to the dataset, scripts, and tools you used.\nVisualization: Use visualizations to convey key insights\n\n\n\nIt is required that you build your website with Quarto.\n\n\n\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO.\n\n\n\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/website-structure.html#website-development",
    "href": "instructions/website-structure.html#website-development",
    "title": "Website project structure",
    "section": "",
    "text": "It is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/website-structure.html#website-hosting",
    "href": "instructions/website-structure.html#website-hosting",
    "title": "Website project structure",
    "section": "",
    "text": "You MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/website-structure.html#the-two-audiences",
    "href": "instructions/website-structure.html#the-two-audiences",
    "title": "Website project structure",
    "section": "",
    "text": "Knowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/quarto-tips.html",
    "href": "instructions/quarto-tips.html",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton.\n\n\n\n\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/quarto-tips.html#file-types",
    "href": "instructions/quarto-tips.html#file-types",
    "title": "Quarto Tips",
    "section": "",
    "text": "You can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/quarto-tips.html#quarto-includes",
    "href": "instructions/quarto-tips.html#quarto-includes",
    "title": "Quarto Tips",
    "section": "",
    "text": "Quarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\n\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/llm-usage.html",
    "href": "instructions/llm-usage.html",
    "title": "LLM usage",
    "section": "",
    "text": "We believe that the adoption of LLM tools is inevitable and will be an important skill for success in your future career. Therefore, appropriate and acceptable use of LLM tools is encouraged for this project. Use them to accelerate your workflow and learning, but not as a replacement for critical thinking and understanding. Carefully review and process their output, use them judiciously, and avoid bloating your text with LLM-generated content. Overusing these tools often degrades the quality of your work rather than enhancing it.\nRemember the following guidelines:\n\nUse common sense: If you feel like you’re doing something questionable, you probably are. A good test is to ask yourself, “Would I openly tell the professor or classmates what I’m doing right now?” If the answer is no, you’re probably doing something you shouldn’t.\nCite your LLM use cases: Always cite when and how you’ve used LLM tools. This is a requirement for the project.\nIs your use helping you grow professionally?: If your use of LLM tools is making you a more competent, efficient, and knowledgeable professional, you’re probably using them in an appropriate manner. If you’re using them as a shortcut to avoid work and gain free time, you’re using them incorrectly.\n\n\n\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1\n\n\n\n\n\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code.\n\n\n\n\n\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/llm-usage.html#citation",
    "href": "instructions/llm-usage.html#citation",
    "title": "LLM usage",
    "section": "",
    "text": "ALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/llm-usage.html#acceptable-use-cases",
    "href": "instructions/llm-usage.html#acceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "Note: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/llm-usage.html#unacceptable-use-cases",
    "href": "instructions/llm-usage.html#unacceptable-use-cases",
    "title": "LLM usage",
    "section": "",
    "text": "DO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\n\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\n\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/expectations.html",
    "href": "instructions/expectations.html",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare.\n\n\n\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable.\n\n\n\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields.\n\n\n\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field.\n\n\n\n\n\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important.\n\n\n\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point.\n\n\n\n\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual.\n\n\n\n\n\n\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project .\n\n\n\n\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/expectations.html#get-started-early",
    "href": "instructions/expectations.html#get-started-early",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Remember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/expectations.html#graduate-level-work",
    "href": "instructions/expectations.html#graduate-level-work",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "This is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/expectations.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/expectations.html#what-is-impact",
    "href": "instructions/expectations.html#what-is-impact",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Impact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/expectations.html#what-makes-a-good-research-project",
    "href": "instructions/expectations.html#what-makes-a-good-research-project",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Professional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/expectations.html#time-management",
    "href": "instructions/expectations.html#time-management",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/expectations.html#visualization-guidelines",
    "href": "instructions/expectations.html#visualization-guidelines",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Visualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/expectations.html#coding",
    "href": "instructions/expectations.html#coding",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "While not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\n\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/expectations.html#debugging",
    "href": "instructions/expectations.html#debugging",
    "title": "General Tips and Expectations",
    "section": "",
    "text": "Always remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nFormula 1 (F1) is widely regarded as the pinnacle of motorsport, where cutting-edge technology, driver expertise, and strategic decision-making converge to create one of the most intricate and competitive sports globally. Spanning diverse circuits across the world, each Grand Prix epitomizes engineering excellence, precise strategy execution, and split-second decisions that often determine success or failure. The ultimate goal for teams and drivers each season is to secure the World Drivers’ Championship (WDC) and World Constructors’ Championship (WCC), where fractions of a second can be the difference between winning and losing. A team or driver’s success in F1 is influenced by several interdependent factors:\n\nDriver Skill and Adaptability: A driver’s ability to adjust to changing race conditions, car dynamics, and strategic demands is vital.\nConstructor Performance: The technical prowess of the car, including aerodynamics, power unit optimization, and tire management, plays a significant role in determining outcomes.\nCircuit Features: Each track is unique, defined by characteristics like track length, number of corners, straights, and the percentage of time drivers spend at full throttle, all of which affect race strategy.\nPit Stop Strategy: Fast and efficient pit stops are critical for success, helping drivers maintain competitive positions or gain an edge over rivals through well-timed stops.\n\nIn F1, data analysis is just as vital as the skill of the driver or the technological advancements of the car. During a race, teams collect vast amounts of data, monitoring key parameters such as tire wear, fuel consumption, aerodynamic efficiency, lap times, and pit stop durations. These data points offer opportunities to optimize car setups, refine race strategies, and predict future outcomes. However, integrating and analyzing these factors collectively remains a complex challenge, providing a rich foundation for data-driven research.\nThis project harnesses Exploratory Data Analysis (EDA) and machine learning techniques to identify relationships hidden within F1 racing data. By exploring driver performance, constructor success, pit stop efficiency, and circuit characteristics, this research seeks to uncover insights into how strategy, engineering decisions, and team execution collectively influence race results.\n\n\nResearch Questions\n\nHow does a driver’s performance evolve when they switch teams, and what role do constructors play in shaping their success?\nCan patterns or trends in pit stop data reveal insights into team strategies and race outcomes?\nCan data-driven models predict race outcomes, such as point finishes, podium placements, or race wins, based on race and pit stop data?\nWhat influence do pit stop duration, frequency, and timing have on overall race success for drivers and teams?\nAre certain circuit types, like high-speed tracks or technical layouts, better suited to specific constructors?\n\n\n\nLiterature Review\nThere is a limited body of research focused on Formula 1, with most studies concentrating on technological advancements in motorsport racing. However, a few notable works provide unique insights into the sport. For instance, research has explored topics such as determining the greatest driver of all time1, analyzing the impact of teams selling their technology to competitors on overall performance2, and examining how environmental factors influence team performance3.\n\n\n\n\n\nReferences\n\n1. Eichenberger, R., Stadelmann, D., et al. Who is the best formula 1 driver? An economic approach to evaluating talent. Economic Analysis and Policy 39, 389 (2009).\n\n\n2. Aversa, P., Furnari, S. & Haefliger, S. Business model configurations and performance: A qualitative comparative analysis in formula one racing, 2005–2013. Industrial and Corporate Change 24, 655–676 (2015).\n\n\n3. Marino, A., Aversa, P., Mesquita, L. & Anand, J. Driving performance via exploration in changing environments: Evidence from formula one racing. Organization Science 26, 1079–1100 (2015)."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Introduction",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin the project, first read the instruction document (click here). This document is also accessible from the navigation bar.\nOnce you’ve completed that, you can proceed with the instructions found throughout the website."
  },
  {
    "objectID": "index.html#what-to-include-on-this-page",
    "href": "index.html#what-to-include-on-this-page",
    "title": "Introduction",
    "section": "What to Include on This Page",
    "text": "What to Include on This Page\nThis is the landing page for your project. Content from this page can be reused in sections of your final report.\n\nCreate an “About You” Page\n\nDevelop your “About You” page. You can reuse content from previous assignments.\nYou can include the content here or on a separate page.\n\nIt’s recommended to create one “About You” page for all DSAN projects, with links to your various class projects.\n\n\n\n\nCreate a Landing Page for Your Project\n\nSummarize your topic, its significance, related work, and the questions you plan to explore.\nDraft an introduction with at least 5 research questions. These may evolve as your project progresses, since data science is an iterative process.\nInclude your data science questions on this page.\n\n\n\nLiterature review\nOnce you decide on a topic, you should ALWAYS START WITH A LITERATURE REVIEW, this is particularly important for academic projects.\nThe literature review is the most important part of most projects.\nIt allows you to;\n\n\nDetermine what is already known and what has already been tried, so that you don't re-invent the wheel.\n\n\nIt makes you more of a subject matter expert, allowing you to ask the right questions, target impactful projects, and communicate with other professionals.\n\n\n\nDoing a project that you think will change the world, only to find at the end that a very similar version of your project was already done in the 1980’s, isn’t a great use of time.\n\nIn this section, please do a literature review and cite at least 3 academic publications per group member, and include internal academic citations.\nOptional: Consider using LLM tools to 10X your literature review, e.g. instead of focusing on 3 papers, aim for 30 or more\nBy following these steps, you can 10X the efficiency of your literature review process, gaining more insights while minimizing the time spent on manual reading.\n\nExpand Your Literature Search\nAim to gather a larger pool of papers. Use academic databases (Google Scholar, arXiv, etc.) to find relevant studies and ensure a broad scope.\nSkim the Abstracts\nRead the abstracts of each paper to quickly understand their focus and identify those most relevant to your topic. Prioritize these papers for deeper analysis.\nUse LLM Tools for Summarization\nUpload the selected papers to an LLM tool capable of text summarization. Have it condense the main points of each paper into a concise, manageable summary (e.g., condense hundreds of pages into a 10-page summary). Carefully review this summary to absorb the key insights.\nLeverage Interactive LLM Tools\nUse tools like NotebookLM or other AI-based text digesters to ask specific, targeted questions about the papers:\n\nExample questions:\n\n“In the papers uploaded, did any of them explore XYZ?”\n“Which paper is most closely related to the following project idea: [explain idea]?” These tools will help you quickly extract relevant information without re-reading entire papers."
  },
  {
    "objectID": "index.html#additional-ideas-for-things-to-include",
    "href": "index.html#additional-ideas-for-things-to-include",
    "title": "Introduction",
    "section": "Additional Ideas for things to include",
    "text": "Additional Ideas for things to include\n\nAudience: Who is this for? Data professionals, businesses, researchers, or curious readers.\nHeadline: A captivating title introducing the data science theme (e.g., “Unlocking Insights Through Data Stories”).\nIntroduction: A brief, engaging overview of what the website offers (e.g., data-driven stories, insights, or case studies).\nQuestions You Are Addressing: What do you hope to learn?\nMotivation: Explain why this topic matters, highlighting the importance of data in solving real-world problems.\nKey Topics: List the main focus areas (e.g., machine learning, data visualization, predictive modeling).\nUse Cases/Examples: A brief teaser of compelling stories or case studies you’ve worked on.\nCall to Action: Invite visitors to explore the content, follow along, or contact you for more information.\nVisual/Infographic: Add a simple graphic or visual element to make the page more dynamic."
  },
  {
    "objectID": "instructions/github-usage.html",
    "href": "instructions/github-usage.html",
    "title": "GitHub",
    "section": "",
    "text": "Your project will be fully transparent, with all source code hosted on GitHub. This platform will serve as the main repository for your project code, documentation, and website. Proper organization and regular updates are key for effective collaboration and project management.\n\nIMPORTANT: Proficiency in GitHub for collaboration is a valuable addition to your resume. Being able to join a team and immediately contribute by solving problems and adding value is a highly sought-after skill. Now is the time to develop this expertise—embrace Git fully, become proficient, and graduate with a critical skill for your future career.\n\n\n\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure.\n\n\n\n\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies.\n\n\n\n\n\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/github-usage.html#repository-setup",
    "href": "instructions/github-usage.html#repository-setup",
    "title": "GitHub",
    "section": "",
    "text": "You MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/github-usage.html#expectations-for-github-usage",
    "href": "instructions/github-usage.html#expectations-for-github-usage",
    "title": "GitHub",
    "section": "",
    "text": "Your grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "href": "instructions/github-usage.html#collaboration-in-groups-if-applicable",
    "title": "GitHub",
    "section": "",
    "text": "If you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html",
    "href": "instructions/overview.html",
    "title": "Project instruction:",
    "section": "",
    "text": "Author(s): Dr. H and Gerard Pendleton Thurston the 4th\nNote: You can delete this folder and remove it from the project website once you have read and understood the instructions. It shouldn’t be part of your final submission.\nAudio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "instructions/overview.html#python-package-optional",
    "href": "instructions/overview.html#python-package-optional",
    "title": "Project instruction:",
    "section": "Python package (optional)",
    "text": "Python package (optional)\nThis is an optional component of the project, if you’d like, you can create a dedicated Python package for your project. The source folder for this package should be included in the root of your repository, and it can be imported into your processing scripts used in the various technical-details sections. If you create a package, it should be well-documented, with an additional tab on the navigation bar for the package documentation.\nWhile a package would typically have its own GitHub repository, for this project, please include it within the same repository.\nThe skeleton for the package is not provided in the repo, but you can recycle what you created in past assignments."
  },
  {
    "objectID": "instructions/overview.html#select-a-broad-topic-area",
    "href": "instructions/overview.html#select-a-broad-topic-area",
    "title": "Project instruction:",
    "section": "Select a broad topic area",
    "text": "Select a broad topic area\nStart by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/overview.html#narrow-your-focus",
    "href": "instructions/overview.html#narrow-your-focus",
    "title": "Project instruction:",
    "section": "Narrow your focus",
    "text": "Narrow your focus\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/overview.html#data-science-questions",
    "href": "instructions/overview.html#data-science-questions",
    "title": "Project instruction:",
    "section": "Data Science Questions",
    "text": "Data Science Questions\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/overview.html#repository-setup",
    "href": "instructions/overview.html#repository-setup",
    "title": "Project instruction:",
    "section": "Repository Setup",
    "text": "Repository Setup\n\nYou MUST use GitHub Classroom to create your project repository. This ensures TAs can access your code and track your progress.\nClone the repository to your local machine, which will provide a basic directory structure."
  },
  {
    "objectID": "instructions/overview.html#expectations-for-github-usage",
    "href": "instructions/overview.html#expectations-for-github-usage",
    "title": "Project instruction:",
    "section": "Expectations for GitHub Usage",
    "text": "Expectations for GitHub Usage\nYour grade will reflect how effectively you use Git, including:\n\nIncremental progress on the project\nThe frequency and quality of commits\nRepository structure and organization\nAdherence to GitHub guidelines outlined below\n\nEnsure regular commits to GitHub (e.g., git add, git commit, git push) to sync your work and maintain a smooth development process.\n\n1. Use a Logical Repository Structure\n\nInclude a comprehensive README file that explains the purpose of the project.\nOrganize files logically to make navigation easier for collaborators and TAs.\nEnsure all files are well-documented and the code is easy to follow.\n\n\n\n2. Commit Regularly\n\nCommit frequently with clear, meaningful commit messages that reflect the changes made.\n\nGood commit message example: Added data cleaning script for tabular data\nPoor commit message example: Fix\n\n\n\n\n3. Data Storage\n\nDo not store large data files in your repository.\n\nStore raw data in the raw-data folder and processed data in the processed-data folder; these folders should be added to .gitignore.\nTip: Use external storage like Google Drive or GU Domains for large datasets and provide access links within the repository.\n\n\n\n\n4. Syncing with GU Domains\n\nSync your GitHub repository with your GU Domains website before submission deadlines to keep everything up to date. (this should be fully automated)\nEnsure your code repository and website are always in sync, particularly before the final submission, to avoid losing points.\n\n\n\n5. Code Documentation\n\nProvide clear and thorough documentation for each file and function in your project.\nInclude a README.md that outlines the project purpose, how to run the code, and any necessary dependencies."
  },
  {
    "objectID": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "href": "instructions/overview.html#collaboration-in-groups-if-applicable",
    "title": "Project instruction:",
    "section": "Collaboration in Groups (If Applicable)",
    "text": "Collaboration in Groups (If Applicable)\nIf you are working in a group, make full use of GitHub’s collaboration features:\n\nTask Assignment:\n\nAssign tasks using GitHub Issues or Project Boards to keep track of progress.\n\nBranching and Pull Requests:\n\nUse branches for feature development and pull requests for code reviews before merging into the main branch.\n\nCommunication:\n\nMaintain regular communication and conduct code reviews with your teammates to prevent conflicts.\n\nEqual Contribution:\n\nEnsure equal contribution from all team members. Unequal contributions will negatively affect individual grades.\nNote: Team members not contributing equally may be flagged by the group and penalized after review.\n\nContribution Documentation:\n\nDocument each member’s contributions clearly in the collaborators.qmd file, detailing who worked on specific aspects of the project.\n\nCode Reviews:\n\nConduct peer code reviews before merging changes into the main branch to maintain quality and consistency."
  },
  {
    "objectID": "instructions/overview.html#website-development",
    "href": "instructions/overview.html#website-development",
    "title": "Project instruction:",
    "section": "Website Development",
    "text": "Website Development\nIt is required that you build your website with Quarto."
  },
  {
    "objectID": "instructions/overview.html#website-hosting",
    "href": "instructions/overview.html#website-hosting",
    "title": "Project instruction:",
    "section": "Website Hosting",
    "text": "Website Hosting\nYou MUST host your website on the Georgetown Domains web space.\nNo exceptions. You may NOT use anything other than Georgetown Domains to host your website. For example, no RPubs, WordPress, Squarespace, or any other website development toolset. Failure to comply with this rule will result in a ZERO."
  },
  {
    "objectID": "instructions/overview.html#the-two-audiences",
    "href": "instructions/overview.html#the-two-audiences",
    "title": "Project instruction:",
    "section": "The two audiences",
    "text": "The two audiences\nKnowing your audience in data science writing is crucial because it shapes how you present information. Technical stakeholders may require detailed explanations of methodologies, while non-technical audiences need clear, simplified insights and data-driven conclusions. Tailoring your message ensures your analysis is both understandable and impactful, driving informed decision-making.\n\nExamples of technical audiences include data scientists, software engineers, and IT professionals. These individuals expect detailed explanations of models, algorithms, methodologies, or system architectures, and they’re comfortable with technical jargon, such as discussing hyperparameters, programming frameworks, or machine learning techniques.\nNon-technical audiences include executives, marketing teams, and clients. They prioritize high-level insights, actionable results, and visualizations that convey the impact of data without requiring an understanding of complex methods. For instance, a CEO may want to know how a model affects business strategy or revenue, without diving into the underlying technical details.\n\nIn this project you will cater to both audiences. This is done by having regions of your website for both audiences (see website struture)"
  },
  {
    "objectID": "instructions/overview.html#get-started-early",
    "href": "instructions/overview.html#get-started-early",
    "title": "Project instruction:",
    "section": "Get started early",
    "text": "Get started early\nRemember, slow and steady wins the race\nMaking steady, incremental progress on a large project generally makes it more manageable. Rushing to throw something together under a tight deadline often turns the process into a nightmare."
  },
  {
    "objectID": "instructions/overview.html#graduate-level-work",
    "href": "instructions/overview.html#graduate-level-work",
    "title": "Project instruction:",
    "section": "Graduate level work",
    "text": "Graduate level work\nThis is a graduate-level class, so each project should be viewed as specifications, not simple step-by-step requirements. Graduate-level work must be creative, individualized, and of high quality. To achieve an A-level grade, you are expected to exceed the specifications and create unique, novel solutions.\nFor example: If you’re asked to build visualizations to support your data science story, you won’t be told how many or what type of visualizations to create. This is up to you, based on your data and the creativity and quality you want to demonstrate.\nWe want you to move away from expecting someone else to tell you what to do, how to do it, and how much to do. Instead, you’ll adopt a professional approach—reviewing specifications provided in the assignments, determining what’s needed to exceed expectations, and demonstrating professional excellence.\nThere are countless ways to approach the project requirements, so be creative and thoughtful. Instructions outline the minimum requirements, but exceeding them will elevate the quality of your work.\nAutonomy and Critical Thinking:\n\nIn the workplace, step-by-step instructions are rare. You’ll need to interpret broad requirements and deliver professional results. Producing high-quality, accurate work with limited guidance is a key professional skill.\nAt this stage, move away from asking, “Do I have to do XYZ?” Instead, critically analyze challenges. If something is unclear, investigate and break it down fundamentally.\n\nDeveloping problem-solving skills is crucial. While it’s important to work independently for at least 10–20 minutes, if you’re still stuck after 30 minutes, seek help. Being resourceful is important, but knowing when to ask for assistance is equally valuable."
  },
  {
    "objectID": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "href": "instructions/overview.html#the-intersection-of-skills-and-domain-knowledge",
    "title": "Project instruction:",
    "section": "The intersection of skills and domain knowledge",
    "text": "The intersection of skills and domain knowledge\n“Data science” is essentially a collection of useful computational and mathematical skills (statistics, cloud computing, machine learning, coding, etc). However, to maximize your effectiveness, these skills should be applied to a domain of interest (e.g., materials science, finance, healthcare, etc). Focusing and learning about a particular domain will help you specialize and make you more marketable.\nThat beind said, don’t worry about choosing the “perfect” domain—it’s always possible to pivot later, as many of the skills learned, such as problem-solving, critical thinking, and self-education, are transferable across all fields."
  },
  {
    "objectID": "instructions/overview.html#what-is-impact",
    "href": "instructions/overview.html#what-is-impact",
    "title": "Project instruction:",
    "section": "What is “impact”?",
    "text": "What is “impact”?\nImpact in science refers to the significance and influence of research, often measured by metrics like citations, impact factor of journals, and indices like h-index. These metrics reflect how widely recognized and valuable the work is within the scientific community. Such quanities are used to compare researchers and journals, and are used to determine grant funding and career opportunities.\n\nImpact Factor (IF):\n\nA measure of a journal’s influence, calculated by averaging the number of citations to articles published in the journal over the past two years.\nHigher impact factors indicate a more influential journal.\n\nNumber of Citations:\n\nThe total count of how often a researcher’s work is cited by others, reflecting its influence within the scientific community.\nMore citations generally signal broader recognition or relevance of the research.\n\nh-index:\n\nA metric that measures both productivity and citation impact. An h-index of 10 means a researcher has 10 papers each cited at least 10 times.\nHigher h-index indicates more influential and widely recognized work.\n\ni10-index:\n\nCounts the number of a researcher’s publications with at least 10 citations.\nA straightforward measure of citation impact, commonly used by Google Scholar.\n\nImportance of Citations:\n\nCitations indicate that other researchers find the work valuable for their own research, increasing its perceived credibility and impact in the field."
  },
  {
    "objectID": "instructions/overview.html#what-makes-a-good-research-project",
    "href": "instructions/overview.html#what-makes-a-good-research-project",
    "title": "Project instruction:",
    "section": "What makes a good research project?",
    "text": "What makes a good research project?\nProfessional academic or industrial research is all about discovery, improvement, and novelty. You don’t necessarily need to have a project that acheive the following, but here are some guidelines for what makes “high impact” projects:\nIn no particular order:\n\nNovel computational tools: For example, development of a new Python package to tackle a class of problem which doesn’t have an existing suitable tool.\nCreating more user-friendly tools: For example, there might be a great C++ code, but with no python analogue. Python is easier to use, so if you make a pythonic version of an existing tool, it may get higher adoption, provided it is more or less as efficient to the competitors.\nMore efficient tools or methods: Achieving something 1.25x, 2x, 10x or 10000x faster than existing methods, or creating a new code package that is more efficient than a previous one.\nNovel methods: A completely new way of doing something (e.g. new classification algorithm)\nExisting methods applied to new domains: Using established methods to solve problems in a novel domain, e.g. applying a particular classification methodology to a problem or dataset that no one has applied it to before. Extent of impact obviously depends on the importance of the domain or use-case.\nCreation of novel Data Sets: Provides well-curated, clean datasets that can be used to address important scientific questions or problems.(often more useful if there is an accompanying API)\nNew insights or phenomena: Using data analysis techniques to uncover new insights or patterns that address key questions or problems. For example, discovering overarching governing rules (e.g., differential equations) that describe some observed phenomena.\n\nSolves a Major Problem: Addresses a critical or unresolved issue in the field, offering a breakthrough or significant advancement.\n\nRobust Data and Methodology: Employs sound, validated methodologies and high-quality data to ensure credibility and reliability (i.e. doing something rigorously, correctly, and generally better than the competition).\nInterdisciplinary Impact: Influences multiple fields or areas of study, increasing the breadth of its significance.\nHigh Citation Potential: Likely to be widely cited due to its significance, relevance, and applicability across different areas.\n\nYou can, of course, have multiple of these components in a single project, which will increase its prestige.\nThis is especially true when conducting academic research. Publications need to be novel and make a unique contribution to the body of human knowledge; otherwise, they will not be highly cited or considered particularly important."
  },
  {
    "objectID": "instructions/overview.html#time-management",
    "href": "instructions/overview.html#time-management",
    "title": "Project instruction:",
    "section": "Time management",
    "text": "Time management\n80-20 Rule: This rule of thumb suggests that 80% of results come from 20% of causes. In other words, a small number of key factors drive the majority of outcomes. This principle applies to project management, where a few critical steps or decisions often determine the success of a project.\nFor example, it might take one week of steady work (30-40 hours) to complete 80% of a project, while the final 20% could take an additional four weeks.\n\nProjects tend to expand to fill the time available. No creative project—whether a book, song, poem, or paper—is ever truly 100% complete. There is an asymptotic limit as \\(t \\rightarrow \\infty\\), and true perfection is unattainable. The key is knowing when to “call it done.” This might happen at 95% or 99% completion, but eventually, we all have to stop. Strive to take this project as far as possible, but remember to stop and “call it done” at some point."
  },
  {
    "objectID": "instructions/overview.html#visualization-guidelines",
    "href": "instructions/overview.html#visualization-guidelines",
    "title": "Project instruction:",
    "section": "Visualization guidelines",
    "text": "Visualization guidelines\nVisualizations are a critical component of your portfolio. Use them strategically to support your narrative. The more visual representations of your data, the better—higher-quality visualizations will result in a higher grade. Ensure that all graphics follow best practices:\n\nChoose the right chart type: Match the chart to the data (e.g., bar for categories, line for trends).\nMaintain simplicity: Avoid clutter and focus on the essential message.\nUse appropriate scales: Ensure axes have correct and intuitive scaling to avoid misinterpretation.\nLabel axes clearly: Include meaningful axis labels with units (e.g., “Temperature (°C)” or “Revenue (USD)”).\nInclude descriptive titles: Provide a concise, informative title that explains the visualization’s main takeaway.\nEnsure consistency: Use uniform color schemes, fonts, and styles across all charts in a presentation.\nHighlight key data: Use contrasting colors or annotations to draw attention to important points or trends.\nKeep proportions accurate: Maintain correct data-to-visual size relationships to avoid distortion.\nConsider the audience: Tailor the level of detail and style to the audience’s technical proficiency.\nTest readability: Ensure fonts, colors, and elements are clear and legible in various formats and sizes.\nUse interactivity carefully: Interactive features should add clarity, not complexity, to the visual."
  },
  {
    "objectID": "instructions/overview.html#coding",
    "href": "instructions/overview.html#coding",
    "title": "Project instruction:",
    "section": "Coding",
    "text": "Coding\n\nPractice First\nWhile not required, practice is highly beneficial. It’s also a good habit to write your code from scratch, as this will build your problem-solving skills. You can use the code provided by professors as a reference, but always strive to write your own. Starting with a blank page is a valuable practice.\nTo get comfortable with the methods, review and modify the R and Python codes provided in class. Try applying these to your project data and experiment with creating small toy datasets, such as a CSV file or a text corpus. This helps you understand the structure of the data and what the algorithms are doing.\n\n\nPrototype and develop on a small data set\nIf you have big data, ALWAYS prototype on a small subset of the data, so that your code runs fast so that you can develop quickly, without waiting several minutes for each code cell to run. Do this by including a downsampling hyper-parameter at the beginning of your code, e.g. 0.1. When the code is robust and finalized, you can set the downsampling factor to 1, run it on the complete data set, and let your computer run overnight.\nIt is not a crazy idea to do a “trial run” of the project first from start to finish with a very basic dataset, e.g. penguins,diabetes, or iris. Make sure the “toy” data set is similar to your planned real world data set, for example, if you are planning an NLP project, don’t use a image dataset for your development process.\nThis will have the following benefits:\n\nClarifies Workflow: Understand the complete process from start to finish.\nIdentifies Challenges: Spot potential issues early on.\nValidates Assumptions: Ensure methods and approaches are suitable.\nEnhances Skills: Improve technical skills through practice.\nBuilds Confidence: Familiarity with tools and techniques.\nRefines Methods: Test and optimize analytical strategies.\nEstimates Resources: Better planning for time and resource allocation.\nFacilitates Communication: Clearly convey project goals and outcomes.\nDocuments Process: Create a reference for reproducibility.\nGathers Early Feedback: Obtain input for adjustments before full implementation.\n\nhttps://scikit-learn.org/1.5/datasets/toy_dataset.html\nOnce that is working as a starter code-base you can swap it out for your full data later and start developing further for a more realistic real world project ."
  },
  {
    "objectID": "instructions/overview.html#debugging",
    "href": "instructions/overview.html#debugging",
    "title": "Project instruction:",
    "section": "Debugging",
    "text": "Debugging\n\nAlways remember the following Debugging Steps:\n\nStep A: Copy the error message and search it online (Google or similar).\nStep B: Look through forums or documentation to find a solution.\nStep C: Implement the solution.\nStep D: If you’re still stuck, ask for help from classmates, TAs, or professors.\nStep E: Move on to the next issue and repeat the process.\n\n\n(Note: You can also use ChatGPT for debugging, but be cautious as the solutions may sometimes be inaccurate or incomplete.)"
  },
  {
    "objectID": "instructions/overview.html#file-types",
    "href": "instructions/overview.html#file-types",
    "title": "Project instruction:",
    "section": "File Types",
    "text": "File Types\nYou can decide when to use .qmd vs .ipynb for structuring your code, but I recommend the following guidelines:\n\nIf the file contains any code (either in R or Python), ALWAYS use .ipynb.\nDo not mix R and Python in the same notebook.\nIf the file is purely markdown without code, use .qmd.\nUse Quarto includes to modularize your content (see below for more details). This is also demonstrated in the project skeleton."
  },
  {
    "objectID": "instructions/overview.html#quarto-includes",
    "href": "instructions/overview.html#quarto-includes",
    "title": "Project instruction:",
    "section": "Quarto Includes",
    "text": "Quarto Includes\nQuarto includes (e.g., {{&lt; include _content.qmd &gt;}}) are highly recommended for modularizing and organizing your content. While optional, they offer several advantages.\nNote: You can include a .qmd file in a .ipynb file, but not vice versa.\n\nWhy Use Quarto Includes?\n\nModularization: Breaking your project into smaller, reusable chunks simplifies the management of complex documents. You can work on specific sections without altering the entire project.\nReusability: Includes allow you to reuse content blocks across multiple documents, making them ideal for repetitive sections like headers or footers.\nConsistency: By using includes, you ensure uniformity across your documents. Updating an include file will automatically apply the changes wherever it’s used.\nSimplifies Collaboration: In team settings, includes allow different contributors to work on separate sections simultaneously, reducing merge conflicts and making the project easier to maintain.\nImproved Organization: Includes help keep your main files clean and focused by loading content from separate, well-organized files. This makes your project more manageable and easier to navigate."
  },
  {
    "objectID": "instructions/overview.html#citation",
    "href": "instructions/overview.html#citation",
    "title": "Project instruction:",
    "section": "Citation",
    "text": "Citation\nALWAY CITE CONTENT OR IDEAS TAKEN FROM EXTERNAL SOURCES: e.g. websites, llm tools, papers\nALWAYS BE TRANSPARENT WHEN YOU ARE USING LLM TOOLS:\nPlease follow these guidelines:\n\nGeneral Tasks: Create and regularly update a dedicated LLM Transparency page to document how you are using LLM tools.\n\nThis page can serve as a “catch-all” for use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\n\nContent Creation: If non-original content (code or text) is generated by an LLM, you must also cite it on specific pages, just like any external source.\n\nFor non-original content, always provide a citation.\nCite the LLM tool after each chunk of text or code it generates, using a BibTeX. For example1"
  },
  {
    "objectID": "instructions/overview.html#acceptable-use-cases",
    "href": "instructions/overview.html#acceptable-use-cases",
    "title": "Project instruction:",
    "section": "Acceptable use cases",
    "text": "Acceptable use cases\nNote: Various useful non-LLM research tools can be found here at the following link\n\nTraditional research tools\n\nYou can use LLM tools for the following use cases\n\nAI research tools\nThese include\n\nre-formating text with LLM tools.\nCode explaination “describe what this code is doing in prose”\nText summarization\nProofreading\n\nchatGPT for project brainstorming\nUsing LLM tools to comment your code qualifies as an acceptible use case in this project. You can also use or code re-formatters such as black to increase the readability of your code."
  },
  {
    "objectID": "instructions/overview.html#unacceptable-use-cases",
    "href": "instructions/overview.html#unacceptable-use-cases",
    "title": "Project instruction:",
    "section": "Unacceptable use cases",
    "text": "Unacceptable use cases\nDO NOT use ChatGPT or other LLM tools to write large portions of your text or code.\n\nEffect on grade\nIf it is clear that you have used LLM tools to write large portions of your code or text, your grade will reflect this, likely in the range of 0% to 50% of the total points, depending on the quality of the work. LLM outputs still require significant polishing to fit into a well-written, cohesive narrative. If it is evident that you simply inserted large portions of LLM-generated content into your assignment without taking the time to refine it into a high-quality submission, your grade will reflect this, even if the usage does not rise to the level of plagiarism.\n\n\nPlagiarism investigation\nIn extreme cases,the following actions will occur.\n\nOne-on-One Investigation: You will meet with department faculty for a thorough review of your project. You will be asked to explain your work in detail, including what specific chunks of code do, why you made certain decisions, and how you reached your conclusions.\nReferral to the Honor Council: If, during this meeting, it is determined that you do not have sufficient understanding of the content that you claimed to have created, the case will be documented and sent to the honor council. This can result in a permanent mark on your transcript and may even lead to expulsion from the university."
  },
  {
    "objectID": "instructions/topic-selection.html",
    "href": "instructions/topic-selection.html",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena\n\n\n\n\n\nNarrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation.\n\n\n\nThe data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "instructions/topic-selection.html#select-a-broad-topic-area",
    "href": "instructions/topic-selection.html#select-a-broad-topic-area",
    "title": "Topic selection",
    "section": "",
    "text": "Start by selecting a broad topic area.\nHere are some examples:\n\nBio and Health\n\nClimate\n\nFinance and Economics\n\nPublic Policy\n\nMaterials Discovery\n\nTransportation\n\nEducation\n\nCrime and Punishment\n\nPolitics and Government\n\nZoology and Botany\n\nSocial Phenomena"
  },
  {
    "objectID": "instructions/topic-selection.html#narrow-your-focus",
    "href": "instructions/topic-selection.html#narrow-your-focus",
    "title": "Topic selection",
    "section": "",
    "text": "Narrow your focus to a topic that can realistically be addressed in a data driven way.\n\ne.g. Study the effect of climate change on extreme weather\nAvoid commonly used topics from Kaggle.\n\nClaim your topic in the “project topic page in the shared documeent”\n\nclick here to claim your topic\n\n\nEven if multiple students choose similar topics, each portfolio must be original. Portfolios that are too similar will be reviewed for plagiarism and may result in Honor Council violation."
  },
  {
    "objectID": "instructions/topic-selection.html#data-science-questions",
    "href": "instructions/topic-selection.html#data-science-questions",
    "title": "Topic selection",
    "section": "",
    "text": "The data science life cycle starts with well-posed questions, similar to the scientific method. A data science question is a broad idea that can be broken down into 5 to 10 smaller questions, guiding your investigation.\n\n“What effect is climate change having on frequency of extreme weather events? e.g hurricane, drought, forest fires, etc”\n\nHere are some additional example questions:\n\nHealth & Medicine\n\nWhat factors predict heart disease across different age groups?\n\nHow does cancer treatment effectiveness vary by demographics?\n\nCan wearables predict the onset of diabetes?\n\nClimate & Environment\n\nWhat is the impact of deforestation on regional climates?\n\nEducation\n\nHow do socioeconomic factors affect student performance?\n\nWhat is the impact of remote learning post-pandemic?\n\nSocial Science & Public Policy\n\nHow does income inequality correlate with crime rates?\n\nWhat factors influence voter turnout?\n\nFinance & Economics\n\nWhat indicators predict stock market crashes?\n\nHow does inflation impact consumer spending?\n\nTransportation\n\nHow has ride-sharing impacted taxi services?\n\nWhat are the busiest transportation hubs, and how can congestion be reduced?\n\nCrime & Law Enforcement\n\nWhat factors predict recidivism in former inmates?\n\nHow do different policing strategies impact crime rates?\n\nSports & Entertainment\n\nWhat factors predict an athlete’s long-term performance?\n\nCan machine learning predict sports match outcomes?\n\nTechnology & Social Media\n\nHow do online reviews impact product sales?\n\nWhat strategies drive viral social media campaigns?\n\n\nChoose a topic you’re passionate about, and develop creative, “outside-the-box” questions to guide your project throughout the course."
  },
  {
    "objectID": "report/report.html",
    "href": "report/report.html",
    "title": "Report",
    "section": "",
    "text": "Introduction\nFormula 1 (F1) is a sport that combine cutting-edge technology, strategic decision-making, and driver skill to create a dynamic and competitive environment. Every race involves millions of data points – ranging from tire temperatures to lap times – that influence the outcomes. This project aims to explore how data can be utilized to uncover hidden insights into team and driver performance, pit stop strategies, and race dynamics. The goal is to simplify complex analysis into actionable findings that highlight the factors driving success in F1 racing.\n\n\nObjective\nThe project aims to answer key questions about race performance and strategies:\n\nHow does a driver’s performance evolve when they switch teams, and what role do constructors play in shaping their success?\nCan patterns or trends in pit stop data reveal insights into team strategies and race outcomes?\nCan data-driven models predict race outcomes, such as point finishes, podium placements, or race wins, based on race and pit stop data?\nWhat influence do pit stop duration, frequency, and timing have on overall race success for drivers and teams?\nAre certain circuit types, like high-speed tracks or technical layouts, better suited to specific constructors?\n\n\n\nKey Findings\n\nImpact of Team Performance on Driver’s Success\n\n  \nA significant portion of the total championship points for most drivers has been earned while driving for top-performing teams such as Ferrari, Red Bull, and Mercedes. This highlights the crucial role that team performance plays in a driver’s success. A clear correlation is observed between the points scored in a season and the team a driver represents, emphasizing the importance of car performance, engineering excellence, and team strategy.\nAnalyzing the career trajectories of drivers like Daniel Ricciardo, Kimi Räikkönen, and Sebastian Vettel, a discernible pattern emerges: they initially excelled while driving for championship-winning teams, followed by moves to midfield teams later in their careers, often before retirement. This transition reflects the competitive nature of Formula 1 and the significance of being part of a top-performing constructor.\nAdditionally, some drivers switched teams despite achieving substantial championship points in the preceding season. This observation underscores the intense competition for seats in Formula 1, where only 20 positions are available across the grid. The combination of driver performance, team expectations, and future potential dictates these decisions, reinforcing the high-stakes environment of the sport.\n\nCyclical Nature of Team Success in Formula 1\n\n\n\n\nTeam Performance\n\n\nThe analysis of constructor performance reveals clear periods of highs and lows, emphasizing the dynamic and cyclical nature of Formula 1. Success in the sport is not guaranteed and often alternates between periods of dominance and rebuilding phases, driven by factors such as technical regulations, innovation, and strategic decisions.\n\nRed Bull Racing: A rapid rise beginning in 2009 coincided with their dominance during the Sebastian Vettel era (2010–2013), where they secured multiple championships. Following a slight dip between 2014 and 2020, Red Bull has experienced another significant upward trajectory, led by Max Verstappen in recent years.\nMercedes: Entering the sport as a constructor in 2010, Mercedes rose to dominance during the hybrid engine era starting in 2014. Their decade-long supremacy was marked by exceptional car performance and driver consistency. However, a decline observed post-2021 coincides with Red Bull’s resurgence and the challenges of adapting to new regulation changes.\nFerrari: Ferrari’s performance has been marked by noticeable fluctuations. A steep decline in 2020, attributed to car performance struggles, was followed by a sharp recovery in 2022, signaling their return to competitiveness and underscoring their resilience as a historic team.\nMcLaren: While McLaren achieved strong results in the early 2000s, their performance significantly declined after 2012, particularly during the hybrid era. However, a gradual recovery since 2018 reflects significant improvements in car design, team operations, and overall strategy.\n\nThis analysis highlights that Formula 1 success is not solely determined by driver skill. Instead, it hinges on a team’s ability to innovate, adapt, and build competitive cars under constantly evolving technical regulations.\n\nMinimizing Failures for Consistent Success\n\nOne of the key differentiating factors between top-performing teams and those struggling for competitiveness in Formula 1 lies in their ability to minimize mechanical failures and accidents, ensuring both driver performance and vehicle reliability contribute to consistent results.\n\nFerrari serves as an excellent example of a top-performing team, with 77.9% of their races completed successfully, a low 6.0% attributed to accidents, and 11.9% due to mechanical failures. This highlights Ferrari’s strong engineering capabilities and their ability to ensure reliability across seasons.\nOn the other hand, teams like Haas face significant challenges, completing only 27.8% of their races successfully, while experiencing 10.1% accidents and 16.5% mechanical failures. Such high failure rates indicate gaps in vehicle reliability and race execution, which hinder their ability to compete consistently.\nWilliams, historically a strong team, shows mixed results. While they completed 47.2% of their races successfully, they recorded 11.4% accidents and 13.1% mechanical failures. Additionally, a notable 28.4% of their races ended with drivers being lapped, emphasizing struggles with car performance and speed relative to competitors.\n\nThese findings demonstrate that success in Formula 1 is a balance of driver skill, strategic decision-making, and, crucially, vehicle reliability. Teams with lower rates of accidents and failures, like Ferrari, gain a competitive edge, whereas higher failure rates, as seen with Haas and Williams, highlight the difficulties faced by midfield and lower-tier teams in maintaining consistency and achieving race finishes.\n\nRelationship Between Pit Stop Data and Race Results\n\nAn important insight derived from the analysis is the relationship between pit stop data and race results. While pit stop duration, frequency, and timing are crucial factors that influence whether a driver scores points, the data reveals that it is challenging to determine the exact finishing position solely based on pit stop variables.\n\nPredicting Pit Stop Duration Using Race Variables\n\nThe analysis demonstrates that pit stop duration can be effectively predicted using variables such as lap time, pit stop number, and lap number. These features provide meaningful insights into the time efficiency of pit stops, allowing teams to optimize their strategy during the race. Accurate predictions of pit stop duration can further help teams reduce time loss, improve competitiveness, and ensure that drivers return to the track with minimal delays."
  },
  {
    "objectID": "report/report.html#guidelines-for-creating-a-good-narrative",
    "href": "report/report.html#guidelines-for-creating-a-good-narrative",
    "title": "Final Report",
    "section": "",
    "text": "Clear Purpose: Define the core message or objective early on.\nKnow Your Audience: Adjust language, tone, and detail based on the audience’s understanding.\nStrong Opening: Start with a hook that sets context and stakes.\nLogical Flow: Structure with a clear beginning, middle, and end.\nKey Insights: Highlight the most important points; avoid unnecessary details or jargon.\nData Support: Use data to enhance the narrative without overwhelming.\nVisuals: Incorporate charts to simplify ideas and engage the audience.\nActionable Takeaways: Conclude with recommendations or next steps.\nAuthenticity: Use storytelling to make the content engaging and relatable.\nRevise: Edit for clarity and impact, removing unnecessary content."
  },
  {
    "objectID": "report/report.html#report-content",
    "href": "report/report.html#report-content",
    "title": "Final Report",
    "section": "",
    "text": "These are just examples, you can use any structure that is suitable for your project.\n\n\n\nIntroduction: Provide an accessible overview and explain the motivation and importance of the research. Example: “This study explores how climate change affects local ecosystems, vital for wildlife conservation.”\nObjective: Clearly define the research goal and relate it to real-world challenges. Example: “We aim to analyze the effects of air pollution on public health in urban areas.”\nKey Findings: Present insights without technical terms, focusing on the impact. Example: “Air pollution increases the risk of respiratory diseases by 20%.”\nMethodology Overview: Briefly explain relevant methods. Example: “We analyzed air quality data from 50 cities and surveyed 10,000 residents.”\nVisualizations: Use simple graphs and infographics to convey findings. Example: A map showing pollution levels and a bar chart of health risks.\nSocietal Implications: Highlight the broader impact. Example: “This study highlights the need for better air quality policies.”\nCall to Action: Offer recommendations based on findings. Example: “We recommend city planners invest in green spaces.”\nConclusion: Recap the main findings and societal impact. Example: “Understanding pollution’s health impacts will help create healthier cities.”\n\n\n\n\n\nExecutive Summary: Summarize key findings and their relevance to business goals. Example: “This report predicts customer churn and offers strategies to reduce churn by 15%.”\nObjective: Define the problem or question addressed. Example: “This project aims to identify the drivers of customer churn.”\nKey Insights: Present the most important, actionable insights. Example: “Customers who interact with support twice within 30 days are 25% less likely to churn.”\nVisualizations: Use clear graphs to convey key insights. Example: A bar chart showing churn likelihood based on engagement.\nBusiness Implications: Explain how findings impact business outcomes and offer specific recommendations. Example: “Focus retention efforts on low-engagement customers.”\nRecommendations: Provide clear, actionable steps with projections. Example: “Implement an automated retention campaign, reducing churn by 10%.”\nConclusion: Summarize findings and suggest next steps. Example: “Addressing churn drivers can reduce loss and improve profitability.”\nAppendix (Optional): Additional charts or explanations for further insights."
  },
  {
    "objectID": "report/report.html#final-tips",
    "href": "report/report.html#final-tips",
    "title": "Final Report",
    "section": "",
    "text": "Simplicity: Avoid jargon; focus on business-relevant insights.\nVisual Focus: Prioritize charts and graphs over dense text.\nEmphasize Impact: Always link data insights to business outcomes."
  },
  {
    "objectID": "technical-details/data-collection/closing.html",
    "href": "technical-details/data-collection/closing.html",
    "title": "Summary",
    "section": "",
    "text": "This section should be written for a technical audience, focusing on detailed analysis, factual reporting, and clear presentation of data. The following serves as a guide, but feel free to adjust as needed.\n\n\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions.\n\n\n\n\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable.\n\n\n\n\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#challenges",
    "href": "technical-details/data-collection/closing.html#challenges",
    "title": "Summary",
    "section": "",
    "text": "Discuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#benchmarks",
    "href": "technical-details/data-collection/closing.html#benchmarks",
    "title": "Summary",
    "section": "",
    "text": "Compare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/closing.html#conclusion-and-future-steps",
    "title": "Summary",
    "section": "",
    "text": "Summarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/data-collection/methods.html",
    "href": "technical-details/data-collection/methods.html",
    "title": "Methods",
    "section": "",
    "text": "Methods\nIn this section, provide an overview summary of the methods used on this page. This should include a brief description of the key techniques, algorithms, tools, or processes employed in your work. Make sure to outline the approach taken for data collection, processing, analysis, or any specific technical steps relevant to the project.\nIf you are developing a package, include a reference to the relevant documentation and provide a link here for easy access. Ensure that the package details are properly documented in its dedicated section, but mentioned and connected here for a complete understanding of the methods used in this project."
  },
  {
    "objectID": "technical-details/eda/instructions.html",
    "href": "technical-details/eda/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/eda/instructions.html#suggested-page-structure",
    "href": "technical-details/eda/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/eda/instructions.html#what-to-address",
    "href": "technical-details/eda/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThe EDA (Exploratory Data Analysis) tab in your portfolio serves as a crucial foundation for your project. It provides a thorough overview of the dataset, highlights patterns, identifies potential issues, and prepares the data for further analysis. Follow these instructions to document your EDA effectively:\nThe goal of EDA is to gain a deeper understanding of the dataset and its relevance to your project’s objectives. It involves summarizing key data characteristics, identifying patterns, anomalies, and preparing for future analysis phases.\nHere are suggestions for things to include on this page\nUnivariate Analysis:\n\nNumerical Variables:\n\nProvide summary statistics (mean, median, standard deviation).\nVisualize distributions using histograms or density plots.\n\nCategorical Variables:\n\nPresent frequency counts and visualize distributions using bar charts or pie charts.\n\nKey Insights:\n\nHighlight any notable trends or patterns observed.\n\n\nBivariate and Multivariate Analysis:\n\nCorrelation Analysis:\n\nAnalyze relationships between numerical variables using a correlation matrix.\nVisualize with heatmaps or pair plots and discuss any strong correlations.\n\nCrosstabulations:\n\nFor categorical variables, use crosstabs to explore relationships and visualize them with grouped bar plots.\n\nFeature Pairings:\n\nAnalyze relationships between key variables, particularly those related to your target.\nVisualize with scatter plots, box plots, or violin plots.\n\n\nData Distribution and Normalization:\n\nSkewness and Kurtosis:\nAnalyze and discuss the distribution of variables.\nApply transformations (e.g., log transformation) if needed for skewed data.\nNormalization:\nApply normalization or scaling techniques (e.g., min-max scaling, z-score).\nDocument and visualize the impact of normalization.\n\nStatistical Insights:\n\nConduct basic statistical tests (e.g., T-tests, ANOVA, chi-square) to explore relationships between variables.\nSummarize the statistical results and their implications for your analysis.\n\nData Visualization and Storytelling:\n\nVisual Summary:\nPresent key insights using charts and visualizations (e.g., Matplotlib, Seaborn, Plotly).\nEnsure all visualizations are well-labeled and easy to interpret.\nInteractive Visualizations (Optional):\nInclude interactive elements (e.g., Plotly, Bokeh) to allow users to explore the data further.\n\nConclusions and Next Steps:\n\nSummary of EDA Findings:\nHighlight the main takeaways from the EDA process (key trends, patterns, data quality issues).\nImplications for Modeling:\nDiscuss how your EDA informs the next steps in your project (e.g., feature selection, data transformations).\nOutline any further data cleaning or preparation required before moving into modeling."
  },
  {
    "objectID": "technical-details/progress-log.html",
    "href": "technical-details/progress-log.html",
    "title": "Progress log",
    "section": "",
    "text": "11-23-2024\n\nHome\n\nWorked on project introduction\nDraft data science questions\n\nData Collection\n\nSearch for data sources\nPublic APIs\nF1 Official website\n\n\n11-24-2024\n\nData Collection\n\nrecord data\n\nRace Information - (2000-2023)\nDriver Standings\nCircuit Info\nConstructor Standings - not required - analysing only drivers’ performance\nweather data for each race\n\nText data\n\nnews api\nnews on top 10 drivers\n\n\nData Cleaning\n\nClean Race Info: json\nclean news data - for sentiment analysis\ndrivers standings\nrace information\n\n\n11-25-2024\n\nData Cleaning\n\ninclude url in the race dataset\nfollow the data cleaning process on the collected data\n\ndata collection\n\nweather data\n\nwiki - based on the race url from the dataset\n\n\nEDA\n\nstart woking on EDA\n\nText analysis\n\nword clouds\ninterpretaion\n\n\n\n\n– Focus on DSAN 5100 Project, 6300 Project and Final –\n12-11-2024\n\nComplete EDA\n\nWord Clouds\nPoints / Driver and Constructor\nLine plot: points / driver for different teams - different colors\ndistrubution of DNF per team\nTrend in points for constructors - for top teams only\npair plot for finish categories\nwork on the theme\n\n\n13-11-2024\n\nComplete Unsupervised learning\ncollect data on pit stops\nclean pit stop data\ndata set for clustering - race track features\n\n14-11-2024\n\nComplete Supervised learning\ncreate categorial variables for bivar. and multivar. classification\n\n15-11-2024\n\nTechnical Pages - Topic Introductions\nExplain algorithms\nanalysis\ninterpretations\ncustomize plots - font size\ncode documentation - detailed"
  },
  {
    "objectID": "technical-details/progress-log.html#to-do",
    "href": "technical-details/progress-log.html#to-do",
    "title": "Progress log",
    "section": "To-do",
    "text": "To-do\n\nExplore possible topics by brainstorming with GPT\nwrite a technical methods sections for K-means\nwrite a technical methods sections for PCA\n\n… etc"
  },
  {
    "objectID": "technical-details/progress-log.html#member-1",
    "href": "technical-details/progress-log.html#member-1",
    "title": "Progress log",
    "section": "Member-1:",
    "text": "Member-1:\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nT: 10-15-2024\n\nCoordinate with team member to set up weekly meeting time\n\nM: 10-14-2024\n\nDo a first draft of the project landing page"
  },
  {
    "objectID": "technical-details/progress-log.html#member-2",
    "href": "technical-details/progress-log.html#member-2",
    "title": "Progress log",
    "section": "Member-2",
    "text": "Member-2\nProvide their name, a link to their “About Me” page.\nAlso, describe a log of their project roles.\nWeekly project contribution log:\nW: 10-16-2024\n\nAttend first group meeting"
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html",
    "href": "technical-details/unsupervised-learning/instructions.html",
    "title": "Instructions",
    "section": "",
    "text": "Note: You should remove these instructions once you have read and understood them. They should not be included in your final submission.\nRemember: Exactly what do you put on this page will be specific you your project and data. Some things might “make more sense” on one page rather than another, depending on your workflow. Organize your project in a logical way that makes the most sense to you.\n\n\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\n\n\n\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "href": "technical-details/unsupervised-learning/instructions.html#suggested-page-structure",
    "title": "Instructions",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "href": "technical-details/unsupervised-learning/instructions.html#what-to-address",
    "title": "Instructions",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nThis page is designed to give you hands-on experience with key unsupervised learning techniques, including clustering methods and dimensionality reduction, applied to real-world datasets. Please apply algorithms such as K-Means, DBSCAN, Hierarchical clustering, PCA, and t-SNE to your data. Through this process, you’ll deepen your understanding of how unsupervised learning can reveal hidden patterns and structure in data.\n\n\nThe objective of this section is to explore and demonstrate the effectiveness of PCA and t-SNE in reducing the dimensionality of complex data while preserving essential information and improving visualization.\n\nPCA (Principal Component Analysis):\n\nApply PCA to your dataset.\nDetermine the optimal number of principal components.\nVisualize the reduced-dimensional data.\nAnalyze and interpret the results.\n\nt-SNE (t-distributed Stochastic Neighbor Embedding):\n\nImplement t-SNE on the same dataset.\nExperiment with different perplexity values.\nVisualize the t-SNE output to reveal patterns and clusters.\nCompare the results of t-SNE with those from PCA.\n\nEvaluation and Comparison:\n\nEvaluate the effectiveness of PCA and t-SNE in preserving data structure.\nCompare the visualization capabilities of both techniques.\nDiscuss the trade-offs and scenarios where one technique may perform better than the other.\n\n\n\n\n\nApply clustering techniques (K-Means, DBSCAN, and Hierarchical clustering) to a selected dataset. The goal is to understand how each method works, compare their performance, and interpret the results.\n\nClustering Methods:\n\nApply K-Means, DBSCAN, and Hierarchical clustering to your dataset.\nWrite a technical summary for each method (2–4 paragraphs per method) explaining how it works, its purpose, and any model selection methods used (e.g., Elbow, Silhouette).\n\nResults Section:\n\nDiscuss and visualize the results of each clustering analysis.\nCompare the performance of different clustering methods, noting any insights gained from the analysis.\nVisualize cluster patterns and how they relate (if at all) to existing labels in the dataset.\nUse professional, labeled, and clear visualizations that support your discussion.\n\nConclusion:\n\nSummarize the key findings and their real-world implications in a non-technical way. Focus on the most important results and how they could apply to practical situations."
  },
  {
    "objectID": "technical-details/data-collection/main.html",
    "href": "technical-details/data-collection/main.html",
    "title": "Data Collection",
    "section": "",
    "text": "In this section, we focus on the methods and sources used to collect data that are relevant to the research questions of this project. The quality of the data collected determines the accuracy of insights and predictions. Poor data collection practices can lead to biased analysis, inaccurate results, and ineffective models. Therefore, careful planning and execution of data collection is essential for the success of any data-driven project.\nChallenges in Data Collection Some important points to be considered when collecting data are:\n\nData quality: Ensuring the data is accurate, complete, and relevant to the research questions is crucial.\nInconsistencies: Data form different sources may have different formats, structures, naming conventions, and units of measurement, requiring a thorough understanding of the data for pre-processing.\nEthics and Privacy: Data collection methods must adhere to ethical guidelines, ensuring that no sensitive or private information is collected or misused.\nData Bias: Collecting data from sources that introduces bias can lead to inaccurate results and models.\nTechnical Constraints: Issues such as API rate limits, website restrictions, or incomplete data can hinder data collection.\n\nBy addressing these challenges, it is ensured that the data collected was relevant, accurate, and reliable."
  },
  {
    "objectID": "technical-details/data-collection/main.html#suggested-page-structure",
    "href": "technical-details/data-collection/main.html#suggested-page-structure",
    "title": "Data Collection",
    "section": "",
    "text": "Here’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications.\n\nIn the provide repo, these subsections have been included in the data-collection file as separate .qmd files that can be embedded using the {{&lt; include  &gt;}} tag."
  },
  {
    "objectID": "technical-details/data-collection/main.html#what-to-address",
    "href": "technical-details/data-collection/main.html#what-to-address",
    "title": "Data Collection",
    "section": "",
    "text": "The following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nOn this page, you will focus on data collection, which is an essential step for future analysis. You should have already selected a specific data-science question that can be addressed in a data-driven way.\nIt is recommended that you focus on one or two of the following data formats, text, tabular, image, geospatial, or network data.\nTabular (e.g. CSV files) and text formats are highly recommended, as these are covered most thoroughly in the course. Deviating from these formats may require additional work on your end. Please avoid timeseries data formats, as these require special methods not covered in the course. You can include as many additional formats as you want. Your project will revolve around the data you gather and will include data collection, analysis, visualization, and storytelling."
  },
  {
    "objectID": "technical-details/data-collection/main.html#start-collecting-data",
    "href": "technical-details/data-collection/main.html#start-collecting-data",
    "title": "Data Collection",
    "section": "",
    "text": "Begin gathering your data and document the methods and sources on the Data Collection page of your project. Include screenshots or example tables to illustrate the data collection process without displaying entire datasets. Ensure transparency so anyone can replicate your work."
  },
  {
    "objectID": "technical-details/data-collection/main.html#saving-the-raw-data",
    "href": "technical-details/data-collection/main.html#saving-the-raw-data",
    "title": "Data Collection",
    "section": "",
    "text": "During the collection phase, save the collected data locally to the data/raw-data folder, in the root of the project, for later processing. (Do not sync this folder to GitHub.)\nRemember, the “raw data” should typically be left “pristine”, to ensure replicability.\nLater when you clean the data, you should save the cleaned data to the data/processed-data folder, in the root of the project.\nYou should also save files you download manually from online to this folder"
  },
  {
    "objectID": "technical-details/data-collection/main.html#requirements",
    "href": "technical-details/data-collection/main.html#requirements",
    "title": "Data Collection",
    "section": "",
    "text": "Your data must be relevant to the project’s overall goals and help solve your research questions.\nYou must use at least one API to collect your data.\nEnsure you have at least one regression target: a continuous quantity that can be used for regression prediction with other features.\nEnsure you have at least one binary classification target: a two-class (A,B) label that can be predicted using other features.\nEnsure you have at least one multiclass-classification target: a multi-class (A,B,C …) label that can be predicted using other features.\nDo not use a Kaggle topic—this project is meant to simulate a real-world project. Kaggle datasets are typically too clean and have already been prepped for analysis, which doesn’t align with the project’s goals.\n\nFocus on data that tells a compelling story and supports the techniques covered in the class (e.g., clustering, classification, regression)."
  },
  {
    "objectID": "technical-details/data-collection/main.html#example",
    "href": "technical-details/data-collection/main.html#example",
    "title": "Data Collection",
    "section": "Example",
    "text": "Example\nIn the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use.\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\n# Step 1: Send a request to Wikipedia page\nurl = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\nresponse = requests.get(url)\n\n# Step 2: Parse the page content using BeautifulSoup\nsoup = BeautifulSoup(response.content, 'html.parser')\n\n# Step 3: Find the table containing the data (usually the first table for such lists)\ntable = soup.find('table', {'class': 'wikitable'})\n\n# Step 4: Extract data from the table rows\ncountries = []\npopulations = []\n\n# Iterate over the table rows\nfor row in table.find_all('tr')[1:]:  # Skip the header row\n    cells = row.find_all('td')\n    if len(cells) &gt; 1:\n        country = cells[1].text.strip()  # The country name is in the second column\n        population = cells[2].text.strip()  # The population is in the third column\n        countries.append(country)\n        populations.append(population)\n\n# Step 5: Create a DataFrame to store the results\ndata = pd.DataFrame({\n    'Country': countries,\n    'Population': populations\n})\n\n# Display the scraped data\nprint(data)\n\n# Optionally save to CSV\ndata.to_csv('../../data/raw-data/countries_population.csv', index=False)\n\n                                 Country     Population\n0                                  World  8,119,000,000\n1                                  China  1,409,670,000\n2                          1,404,910,000          17.3%\n3                          United States    335,893,238\n4                              Indonesia    281,603,800\n..                                   ...            ...\n235                   Niue (New Zealand)          1,681\n236                Tokelau (New Zealand)          1,647\n237                         Vatican City            764\n238  Cocos (Keeling) Islands (Australia)            593\n239                Pitcairn Islands (UK)             35\n\n[240 rows x 2 columns]"
  },
  {
    "objectID": "technical-details/data-collection/main.html#challenges",
    "href": "technical-details/data-collection/main.html#challenges",
    "title": "Data Collection",
    "section": "Challenges",
    "text": "Challenges\n\nDiscuss any technical challenges faced during the project, such as data limitations, computational issues, or obstacles encountered during analysis.\nExplain unexpected results and their technical implications.\nIdentify areas for future work, including potential optimizations, further analysis, or scaling solutions."
  },
  {
    "objectID": "technical-details/data-collection/main.html#benchmarks",
    "href": "technical-details/data-collection/main.html#benchmarks",
    "title": "Data Collection",
    "section": "Benchmarks",
    "text": "Benchmarks\n\nCompare your findings to relevant research, industry benchmarks, or intuitive expectations, if applicable."
  },
  {
    "objectID": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "href": "technical-details/data-collection/main.html#conclusion-and-future-steps",
    "title": "Data Collection",
    "section": "Conclusion and Future Steps",
    "text": "Conclusion and Future Steps\n\nSummarize the key technical points and outcomes of the project.\nSuggest potential improvements or refinements to this part of the project.\nBased on the results, provide actionable recommendations for further research or optimization efforts."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html",
    "href": "technical-details/supervised-learning/main.html",
    "title": "Supervised Learning",
    "section": "",
    "text": "Supervised Learning is a branch of Machine Learning where models are trained on labeled data. The primary goal of supervised learning is to learn a mapping function that can predict outputs for unseen data accurately. This approach relies on historical data to identify patterns and relationships, enabling it to generalize well on new, unseen data.\nProcess:\n\nThe model is provided with input-output pairs from historical data. Each data point consists of:\n\nFeatures (X): Independent variables or predictors.\nLabels (Y): Target outputs or dependent variables.\n\nThe model learns the relationship between the input features and corresponding outputs using a loss function that measures prediction error. The goal is to minimize this error.\nOnce trained, the model can predict outputs for new inputs.\nPerformance is evaluated using metrics like accuracy, precision, recall, F1-score for classification tasks, or RMSE, MAE for regression tasks.\n\nTypes of Supervised Learning\n\nClassification:\n\n\nThe goal is to categorize inputs into discrete classes or categories.\nAlgorithms: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forest, Naive Bayes, K-Nearest Neighbors (KNN).\n\n\nRegression:\n\n\nThe goal is to predict a continuous output based on input features.\nAlgorithms: Linear Regression, Polynomial Regression, Ridge and Lasso Regression, Support Vector Regression (SVR).\n\nChallenges:\n\nSupervised learning requires large volumes of labeled data, which can be expensive and time-consuming to obtain.\nModels may memorize training data instead of generalizing to new inputs. Regularization and cross-validation techniques help mitigate this.\nIn classification problems, imbalanced classes can affect performance.\n\nIn this project:\nSupervised Learning Algorithms are applied to classify race results and predict pit stop duration."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "href": "technical-details/supervised-learning/main.html#suggested-page-structure",
    "title": "Supervised Learning",
    "section": "Suggested page structure",
    "text": "Suggested page structure\nHere’s one suggested structure for organizing your technical pages. You can adjust this as needed:\nAudience:Remember that these are written for a technical audience. Assume they have completed the DSAN program, but would appreciate refreshers of the important concepts.\n\nIntroduction and Motivation: Briefly outline your plan. What are you doing on this page, and why? Provide context and explain the goals of your analysis.\nOverview of Methods: Give a concise explanation of the methods used. For example, if using K-Means clustering, describe what it is, how it works, the inputs and outputs, and key hyperparameters.\nCode: Include the code you used to implement your workflow.\nSummary and Interpretation of Results: Summarize your findings, interpret the results, and discuss their technical implications."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#what-to-address",
    "href": "technical-details/supervised-learning/main.html#what-to-address",
    "title": "Supervised Learning",
    "section": "What to address",
    "text": "What to address\nThe following is a list of some of the things you should address on this page. This list is not exhaustive, and you should use your judgment to decide what is most relevant to your project.\nPlease do some form of “Feature selection” in your project and include a section on it. Discuss the process you went through to select the features that you used in your model, this should be done for both classification models and regression models. What did you include and why? What did you exclude? What was the reasoning behind your decisions? This section can be included here, or you can make a new page in the dropdown menu for it.\nPlease break this page into a “regression” section, “binary classification” section, and a “Multi-class classification” section. For each case you should try multiple methods, including those discussed in class, and compare and contrast their preformance and results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#data-preprocessing",
    "href": "technical-details/supervised-learning/main.html#data-preprocessing",
    "title": "Supervised Learning",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\n\nNormalization or Standardization: Apply techniques to scale the data appropriately.\nFeature Selection or Extraction: Identify and select the most relevant features for your analysis.\nEncoding Categorical Variables: Convert categorical variables into a suitable format for modeling."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-selection",
    "href": "technical-details/supervised-learning/main.html#model-selection",
    "title": "Supervised Learning",
    "section": "Model Selection",
    "text": "Model Selection\n\nModel Rationale: Explain the reasons for selecting specific models or algorithms.\nOverview of Algorithms: Provide a brief overview of the algorithms used"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "href": "technical-details/supervised-learning/main.html#training-and-testing-strategy",
    "title": "Supervised Learning",
    "section": "Training and Testing Strategy",
    "text": "Training and Testing Strategy\n\nSplit Methods: Detail the splitting methods used (e.g., train-test split, cross-validation).\nDataset Proportions: Specify the proportions used for splitting the dataset."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "href": "technical-details/supervised-learning/main.html#model-evaluation-metrics",
    "title": "Supervised Learning",
    "section": "Model Evaluation Metrics",
    "text": "Model Evaluation Metrics\n\nBinary Classification Metrics: Discuss metrics such as accuracy, precision, recall, F1 score, and ROC-AUC.\nMulticlass Classification Metrics: Include metrics such as confusion matrix and macro/micro F1 score.\nRegression Metrics: Explain metrics such as RMSE, MAE, and R-squared, parity plots, etc."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#results",
    "href": "technical-details/supervised-learning/main.html#results",
    "title": "Supervised Learning",
    "section": "Results",
    "text": "Results\n\nModel Performance Summary: Provide a summary of the model’s performance.\nVisualizations: Include visualizations of results (e.g., ROC curves, feature importance plots)."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#discussion",
    "href": "technical-details/supervised-learning/main.html#discussion",
    "title": "Supervised Learning",
    "section": "Discussion",
    "text": "Discussion\n\nResult Interpretation: Interpret the results obtained from the analysis.\nModel Performance Comparison: Compare the performance of different models.\nInsights Gained: Share insights learned from the analysis."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Introduction",
    "section": "Introduction",
    "text": "Introduction\nOn an average, an astounding 4.7 million businesses are started every year 1, fueling innovation and economic growth. However, up to 90% of the startups fail. The average failure rate in the first year is 10%, but this increases dramatically to 70% within the next 5 years 2.\nIn the United States alone, the venture capital investments exceeded $150 billion in 2023 3, reflecting the significant financial stakes involved.\nThese statistics underscore the crucial need for entrepreneurs, investors, and policy makers to make informed decisions.\nBy leveraging data science techniques, this project aims to predict whether a startup will succeed or fail, providing actionable insights to stakeholders. This will not only help investors mitigate risks but also empowers entrepreneurs to focus on strategies that maximize their chances of success."
  },
  {
    "objectID": "index.html#resources",
    "href": "index.html#resources",
    "title": "Introduction",
    "section": "Resources",
    "text": "Resources"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://www.commerceinstitute.com/new-businesses-started-every-year/ - data↩︎\nhttps://explodingtopics.com/blog/startup-failure-stats↩︎\nhttps://news.crunchbase.com/venture/q3-2024-funding-recap-charts/↩︎"
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Introduction",
    "section": "Research Questions",
    "text": "Research Questions\n\nHow are startups distributed across different domains in the United States, and what patterns or trends emerge from this distributions.\nWhat percentage of startups succeed in each domain, and what distribinguishes high-performing doomains from others?\nWhat are the key dfferences in financial, and operational factors between startups that have closed down, and those that are acquired?\nAre there specific terminologies in the new that drive the sentiment of articles on startups, and can these terms be leveraged to analyze success rates further?\nWhat Machine Learning algortihms can be used to predict the success of a startup based on the data collected, and what are the key features that drive the predictions?"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#clean-the-news-data",
    "href": "technical-details/data-cleaning/main.html#clean-the-news-data",
    "title": "Data Cleaning",
    "section": "Clean the News Data",
    "text": "Clean the News Data\n\n# import required libraries\nimport os\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\nimport nltk\nnltk.download('stopwords')\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/nandinikodali/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n\n\nTrue\n\n\n\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport re\nfrom nltk.corpus import stopwords\n\n# Load English stop words\nstop_words = set(stopwords.words('english'))\n\ndef string_cleaner(input_string):\n    try:\n        # Remove unwanted punctuation\n        out = re.sub(r\"[.,;@#?!&$-]+\", \" \", input_string)  # Replace punctuation with space\n        \n        # Remove escape characters like \\u201e\n        out = re.sub(r'\\\\u[0-9a-fA-F]{4}', '', out)\n        \n        # Remove extra whitespace\n        out = re.sub(r'\\s+', ' ', out).strip()\n        \n        # Convert to lowercase\n        out = out.lower()\n        \n        # Remove stop words and words of length &lt;= 3\n        words = out.split()\n        words = [word for word in words if len(word) &gt; 3 and word not in stop_words]\n        \n        # Join words back into a single string\n        out = ' '.join(words)\n        \n    except Exception as e:\n        print(f\"Error cleaning string: {e}\")\n        out = ''\n    \n    return out\n\n# Example Usage\ntext = r\"thema kdssdn\\u201eformel 1\\u201c lesen sie jetzt j\\u201everstappen im training\"\ncleaned_text = string_cleaner(text)\nprint(f\"Cleaned Text: {cleaned_text}\")\n\nCleaned Text: thema kdssdnformel lesen jetzt jverstappen training\n\n\n\n# function for cleaning the text\nstop_words = set(stopwords.words('english'))\n\ndef string_cleaner(input_string):\n    try:\n        out = re.sub(r\"\"\"\n                    [,.;@#?!&$-]+\n                    \\ *          \n                    \"\"\",\n                    \" \",\n                    input_string, flags=re.VERBOSE)\n        out = re.sub(r'\\\\u[0-9a-fA-F]{4}', '', out)\n        out = re.sub('[’.]+', '', out)\n\n        out = re.sub(r'\\s+', ' ', out)\n        out = out.lower()\n        words = out.split()\n        words = [\n            word for word in words\n            if len(word) &gt; 3 and word not in stop_words and not re.search(r'\\d', word)\n        ]\n        out = ' '.join(words)\n    except:\n        print(\"ERROR\")\n        out = ''\n    return out\n\n\n# Function to clean news data\ndef clean_news_data(raw_data_dir, clean_data_dir):\n    \n    # Iterate through raw data files\n    for file_name in os.listdir(raw_data_dir):\n        if file_name.endswith(\"_raw_text.json\"):  # Process only raw data files\n            \n            # Load the raw data\n            raw_file_path = os.path.join(raw_data_dir, file_name)\n            with open(raw_file_path, 'r') as raw_file:\n                raw_data = json.load(raw_file)\n            \n            # Clean the data\n            clean_data = {}\n            for article in raw_data:\n                title = article.get('title', '')\n                description = article.get('description', '')\n                \n                if title and description:\n                    clean_title = string_cleaner(title)\n                    clean_description = string_cleaner(description)\n                    clean_data[clean_title] = clean_description\n            \n            # Save the cleaned data to a new file\n            clean_file_name = file_name.replace(\"_raw_text.json\", \"_clean_news.json\")\n            clean_file_path = os.path.join(clean_data_dir, clean_file_name)\n            with open(clean_file_path, 'w') as clean_file:\n                json.dump(clean_data, clean_file, indent=4)\n\n\n# Define directories\n\n# Directory with raw data files\nraw_data_dir = \"../../data/raw-data/News_Drivers\"  \n\n# Directory for cleaned data files\nclean_data_dir = \"../../data/processed-data/News_drivers\"  \n\n\n# Clean the news data\nclean_news_data(raw_data_dir, clean_data_dir)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#clean-the-drivers-standings",
    "href": "technical-details/data-cleaning/main.html#clean-the-drivers-standings",
    "title": "Data Cleaning",
    "section": "Clean the Drivers Standings",
    "text": "Clean the Drivers Standings\n\ninput_file = \"../../data/raw-data/Driver_standings/driver_standings_2000_2023.json\"\noutput_file = \"../../data/processed-data/driver_standings_2000_2023.csv\"\n\n\nwith open(input_file, 'r') as f:\n    data = json.load(f)\n\n# Prepare a list to store extracted records\ncleaned_data = []\n\n# Loop through each season in the JSON\nfor season, season_data in data.items():\n    standings_lists = season_data.get('MRData', {}).get('StandingsTable', {}).get('StandingsLists', [])\n    \n    for standings in standings_lists:\n        driver_standings = standings.get('DriverStandings', [])\n        \n        for entry in driver_standings:\n            # Extract required fields\n            position = entry.get('position', '')\n            points = entry.get('points', '')\n            wins = entry.get('wins', '')\n            driver = entry.get('Driver', {})\n            constructors = entry.get('Constructors', [])\n            \n            # Extract driver and constructor details\n            given_name = driver.get('givenName', '')\n            family_name = driver.get('familyName', '')\n            constructor_id = constructors[0].get('constructorId', '') if constructors else ''\n            constructor_name = constructors[0].get('name', '') if constructors else ''\n            \n            # Append the record to the cleaned data list\n            cleaned_data.append({\n                \"Season\": season,\n                \"Position\": position,\n                \"FirstName\": given_name,\n                \"LastName\": family_name,\n                \"Constructor_ID\": constructor_id,\n                \"Constructor_Name\": constructor_name,\n                \"Points\": points,\n                \"Wins\": wins\n            })\n\n\ndf = pd.DataFrame(cleaned_data)\ndf['driverName'] = df['FirstName'] + \" \" + df['LastName']\ndf = df.drop(['FirstName', 'LastName'], axis=1)\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#clean-the-circuit-information",
    "href": "technical-details/data-cleaning/main.html#clean-the-circuit-information",
    "title": "Data Cleaning",
    "section": "Clean the Circuit Information",
    "text": "Clean the Circuit Information\n\ninput_file = \"../../data/raw-data/circuit_data/circuit_data.json\"\noutput_file = \"../../data/processed-data/circuit_data_clean.csv\"\n\n\n# Ensure the output directory exists\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# Read the JSON file\nwith open(input_file, 'r') as f:\n    data = json.load(f)\n\n# Extract circuit data\ncircuits = data.get('MRData', {}).get('CircuitTable', {}).get('Circuits', [])\n\n# Prepare a list to store extracted records\ncleaned_data = []\n\nfor circuit in circuits:\n    circuit_id = circuit.get('circuitId', '')\n    circuit_name = circuit.get('circuitName', '')\n    country = circuit.get('Location', {}).get('country', '')\n    latitude = circuit.get('Location', {}).get('lat', '')\n    longitude = circuit.get('Location', {}).get('long', '')\n    \n    # Append to the list\n    cleaned_data.append({\n        \"Circuit_ID\": circuit_id,\n        \"Circuit_Name\": circuit_name,\n        \"Country\": country,\n        \"Latitude\": latitude,\n        \"Longitude\": longitude\n    })\n\n# Convert the list to a Pandas DataFrame\ndf = pd.DataFrame(cleaned_data)\n\n# Save the DataFrame to a CSV file\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#clean-the-race-data",
    "href": "technical-details/data-cleaning/main.html#clean-the-race-data",
    "title": "Data Cleaning",
    "section": "Clean the Race data",
    "text": "Clean the Race data\n\ninput_file = \"../../data/raw-data/race_data_2000.json\"\noutput_file = \"../../data/processed-data/race_data/race_data_2000_clean.csv\"\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n\n# testing on a single file\n# Read the JSON file\nwith open(input_file, 'r') as f:\n    data = json.load(f)\n\n# Extract races from the JSON\nraces = data.get('MRData', {}).get('RaceTable', {}).get('Races', [])\n\n# Prepare a list to hold flattened race results\nall_results = []\n\n# Loop through each race and flatten its data\nfor race in races:\n    race_info = {  # Extract race-level details\n        \"season\": race.get(\"season\", \"\"),\n        \"round\": race.get(\"round\", \"\"),\n        \"raceName\": race.get(\"raceName\", \"\"),\n        \"circuitName\": race.get(\"Circuit\", {}).get(\"circuitName\", \"\"),\n        \"locality\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"locality\", \"\"),\n        \"country\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"country\", \"\"),\n        \"lat\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"lat\", \"\"),\n        \"long\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"long\", \"\"),\n        \"date\": race.get(\"date\", \"\"),\n    }\n    \n    # Extract results and combine with race-level details\n    results = race.get(\"Results\", [])\n    for result in results:\n        # Combine race-level and result-level data\n        combined_data = {**race_info, **result}\n        # Add flattened driver and constructor details\n        combined_data.update({\n            \"driverId\": result.get(\"Driver\", {}).get(\"driverId\", \"\"),\n            \"driverGivenName\": result.get(\"Driver\", {}).get(\"givenName\", \"\"),\n            \"driverFamilyName\": result.get(\"Driver\", {}).get(\"familyName\", \"\"),\n            \"constructorId\": result.get(\"Constructor\", {}).get(\"constructorId\", \"\"),\n            \"constructorName\": result.get(\"Constructor\", {}).get(\"name\", \"\"),\n            \"status\": result.get(\"status\", \"\"),\n            \"timeMillis\": result.get(\"Time\", {}).get(\"millis\", \"\"),\n            \"time\": result.get(\"Time\", {}).get(\"time\", \"\")\n        })\n        all_results.append(combined_data)\n\n# Convert to a Pandas DataFrame\ndf = pd.DataFrame(all_results)\n\n# Save the DataFrame to a CSV file\ndf.to_csv(output_file, index=False)\n\n\nrace_df = pd.read_csv(\"../../data/processed-data/race_data/race_data_2000_clean.csv\")\nrace_df.head()\n\n\n\n\n\n\n\n\nseason\nround\nraceName\ncircuitName\nlocality\ncountry\nlat\nlong\ndate\nnumber\n...\nlaps\nstatus\nTime\ndriverId\ndriverGivenName\ndriverFamilyName\nconstructorId\nconstructorName\ntimeMillis\ntime\n\n\n\n\n0\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n3\n...\n58\nFinished\n{'millis': '5641987', 'time': '1:34:01.987'}\nmichael_schumacher\nMichael\nSchumacher\nferrari\nFerrari\n5641987.0\n1:34:01.987\n\n\n1\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n4\n...\n58\nFinished\n{'millis': '5653402', 'time': '+11.415'}\nbarrichello\nRubens\nBarrichello\nferrari\nFerrari\n5653402.0\n+11.415\n\n\n2\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n9\n...\n58\nFinished\n{'millis': '5661996', 'time': '+20.009'}\nralf_schumacher\nRalf\nSchumacher\nwilliams\nWilliams\n5661996.0\n+20.009\n\n\n3\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n22\n...\n58\nFinished\n{'millis': '5686434', 'time': '+44.447'}\nvilleneuve\nJacques\nVilleneuve\nbar\nBAR\n5686434.0\n+44.447\n\n\n4\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n11\n...\n58\nFinished\n{'millis': '5687152', 'time': '+45.165'}\nfisichella\nGiancarlo\nFisichella\nbenetton\nBenetton\n5687152.0\n+45.165\n\n\n\n\n5 rows × 26 columns\n\n\n\n\nrace_df.columns\n\nIndex(['season', 'round', 'raceName', 'circuitName', 'locality', 'country',\n       'lat', 'long', 'date', 'number', 'position', 'positionText', 'points',\n       'Driver', 'Constructor', 'grid', 'laps', 'status', 'Time', 'driverId',\n       'driverGivenName', 'driverFamilyName', 'constructorId',\n       'constructorName', 'timeMillis', 'time'],\n      dtype='object')\n\n\n\nrace_df = race_df.drop(columns=['positionText', 'Driver', 'Constructor', 'Time'], axis=1)\n\n\nrace_df.head()\n\n\n\n\n\n\n\n\nseason\nround\nraceName\ncircuitName\nlocality\ncountry\nlat\nlong\ndate\nnumber\n...\ngrid\nlaps\nstatus\ndriverId\ndriverGivenName\ndriverFamilyName\nconstructorId\nconstructorName\ntimeMillis\ntime\n\n\n\n\n0\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n3\n...\n3\n58\nFinished\nmichael_schumacher\nMichael\nSchumacher\nferrari\nFerrari\n5641987.0\n1:34:01.987\n\n\n1\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n4\n...\n4\n58\nFinished\nbarrichello\nRubens\nBarrichello\nferrari\nFerrari\n5653402.0\n+11.415\n\n\n2\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n9\n...\n11\n58\nFinished\nralf_schumacher\nRalf\nSchumacher\nwilliams\nWilliams\n5661996.0\n+20.009\n\n\n3\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n22\n...\n8\n58\nFinished\nvilleneuve\nJacques\nVilleneuve\nbar\nBAR\n5686434.0\n+44.447\n\n\n4\n2000\n1\nAustralian Grand Prix\nAlbert Park Grand Prix Circuit\nMelbourne\nAustralia\n-37.8497\n144.968\n2000-03-12\n11\n...\n9\n58\nFinished\nfisichella\nGiancarlo\nFisichella\nbenetton\nBenetton\n5687152.0\n+45.165\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n## Cleaning all the race_data and appending them in to a single csv file \n\n# input output directory\ninput_dir = \"../../data/raw-data/\"\n# output directory\noutput_file = \"../../data/processed-data/all_race_results_cleaned.csv\"\n\n# creating an output file\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# initialize a list to hold all results\nall_combined_results = []\n\n# process each JSON file in the input directory\n# they are the only .json files in the directory\nfor file_name in os.listdir(input_dir):\n    # process only JSON files\n    if file_name.endswith(\".json\"): \n        file_path = os.path.join(input_dir, file_name)\n        print(f\"Processing file: {file_path}\")\n        \n        # read the JSON file\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        \n        # extract races from the JSON\n        races = data.get('MRData', {}).get('RaceTable', {}).get('Races', [])\n        \n        # prepare a list to hold flattened race results for this file\n        file_results = []\n\n        # loop through each race and flatten its data\n        for race in races:\n            # extract required information \n            race_info = { \n                \"season\": race.get(\"season\", \"\"),\n                \"round\": race.get(\"round\", \"\"),\n                \"raceName\": race.get(\"raceName\", \"\"),\n                \"circuitName\": race.get(\"Circuit\", {}).get(\"circuitName\", \"\"),\n                \"locality\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"locality\", \"\"),\n                \"country\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"country\", \"\"),\n                \"lat\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"lat\", \"\"),\n                \"long\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"long\", \"\"),\n                \"date\": race.get(\"date\", \"\"),\n            }\n            \n            # extract results and combine with useful details\n            results = race.get(\"Results\", [])\n            for result in results:\n                # combine race-level and result-level data\n                combined_data = {**race_info, **result}\n                # add flattened driver and constructor details\n                combined_data.update({\n                    \"driverId\": result.get(\"Driver\", {}).get(\"driverId\", \"\"),\n                    \"driverGivenName\": result.get(\"Driver\", {}).get(\"givenName\", \"\"),\n                    \"driverFamilyName\": result.get(\"Driver\", {}).get(\"familyName\", \"\"),\n                    \"constructorId\": result.get(\"Constructor\", {}).get(\"constructorId\", \"\"),\n                    \"constructorName\": result.get(\"Constructor\", {}).get(\"name\", \"\"),\n                    \"status\": result.get(\"status\", \"\"),\n                    \"timeMillis\": result.get(\"Time\", {}).get(\"millis\", \"\"),\n                    \"time\": result.get(\"Time\", {}).get(\"time\", \"\")\n                })\n                file_results.append(combined_data)\n\n        # append the results for this file to the combined list\n        all_combined_results.extend(file_results)\n\n# aonvert the combined results to a Pandas DataFrame\ndf = pd.DataFrame(all_combined_results)\n\n# aave the combined DataFrame to a CSV file\ndf.to_csv(output_file, index=False)\n\n\nProcessing file: ../../data/raw-data/race_data_2010.json\nProcessing file: ../../data/raw-data/race_data_2006.json\nProcessing file: ../../data/raw-data/race_data_2007.json\nProcessing file: ../../data/raw-data/race_data_2011.json\nProcessing file: ../../data/raw-data/race_data_2016.json\nProcessing file: ../../data/raw-data/race_data_2000.json\nProcessing file: ../../data/raw-data/race_data_2020.json\nProcessing file: ../../data/raw-data/race_data_2021.json\nProcessing file: ../../data/raw-data/race_data_2001.json\nProcessing file: ../../data/raw-data/race_data_2017.json\nProcessing file: ../../data/raw-data/race_data_2002.json\nProcessing file: ../../data/raw-data/race_data_2014.json\nProcessing file: ../../data/raw-data/race_data_2022.json\nProcessing file: ../../data/raw-data/race_data_2018.json\nProcessing file: ../../data/raw-data/race_data_2019.json\nProcessing file: ../../data/raw-data/race_data_2023.json\nProcessing file: ../../data/raw-data/race_data_2015.json\nProcessing file: ../../data/raw-data/race_data_2003.json\nProcessing file: ../../data/raw-data/race_data_2008.json\nProcessing file: ../../data/raw-data/race_data_2004.json\nProcessing file: ../../data/raw-data/race_data_2012.json\nProcessing file: ../../data/raw-data/race_data_2013.json\nProcessing file: ../../data/raw-data/race_data_2005.json\nProcessing file: ../../data/raw-data/race_data_2009.json\n\n\n\ndf = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")\n\n\ndf.shape\n\n(2400, 27)\n\n\n\ndf.columns\n\nIndex(['season', 'round', 'raceName', 'circuitName', 'locality', 'country',\n       'lat', 'long', 'date', 'number', 'position', 'positionText', 'points',\n       'Driver', 'Constructor', 'grid', 'laps', 'status', 'Time', 'FastestLap',\n       'driverId', 'driverGivenName', 'driverFamilyName', 'constructorId',\n       'constructorName', 'timeMillis', 'time'],\n      dtype='object')\n\n\n\ndf = df.drop(columns=['positionText', 'number', 'FastestLap', 'Driver', 'Constructor', 'Time'], axis=1)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nseason\nround\nraceName\ncircuitName\nlocality\ncountry\nlat\nlong\ndate\nposition\n...\ngrid\nlaps\nstatus\ndriverId\ndriverGivenName\ndriverFamilyName\nconstructorId\nconstructorName\ntimeMillis\ntime\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n1\n...\n3\n49\nFinished\nalonso\nFernando\nAlonso\nferrari\nFerrari\n5960396.0\n1:39:20.396\n\n\n1\n2010\n1\nBahrain Grand Prix\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n2\n...\n2\n49\nFinished\nmassa\nFelipe\nMassa\nferrari\nFerrari\n5976495.0\n+16.099\n\n\n2\n2010\n1\nBahrain Grand Prix\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n3\n...\n4\n49\nFinished\nhamilton\nLewis\nHamilton\nmclaren\nMcLaren\n5983578.0\n+23.182\n\n\n3\n2010\n1\nBahrain Grand Prix\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n4\n...\n1\n49\nFinished\nvettel\nSebastian\nVettel\nred_bull\nRed Bull\n5999195.0\n+38.799\n\n\n4\n2010\n1\nBahrain Grand Prix\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n5\n...\n5\n49\nFinished\nrosberg\nNico\nRosberg\nmercedes\nMercedes\n6000609.0\n+40.213\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "technical-details/data-collection/main.html#footnotes",
    "href": "technical-details/data-collection/main.html#footnotes",
    "title": "Data Collection",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2010 Bahrain Grand Prix↩︎\nWhat are APIs↩︎\nHow do APIs work↩︎\nTypes of APIs↩︎\nErgast Developer API↩︎"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#combine-the-race-data",
    "href": "technical-details/data-cleaning/main.html#combine-the-race-data",
    "title": "Data Cleaning",
    "section": "Combine the Race data",
    "text": "Combine the Race data\n\n## Cleaning all the race_data and appending them in to a single csv file \n\n# input output directory\ninput_dir = \"../../data/raw-data/\"\n# output directory\noutput_file = \"../../data/processed-data/all_race_results_cleaned.csv\"\n\n# creating an output file\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# initialize a list to hold all results\nall_combined_results = []\n\n# process each JSON file in the input directory\n# they are the only .json files in the directory\nfor file_name in os.listdir(input_dir):\n    # process only JSON files\n    if file_name.endswith(\".json\"): \n        file_path = os.path.join(input_dir, file_name)\n        #print(f\"Processing file: {file_path}\")\n        \n        # read the JSON file\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        \n        # extract races from the JSON\n        races = data.get('MRData', {}).get('RaceTable', {}).get('Races', [])\n        \n        # prepare a list to hold flattened race results for this file\n        file_results = []\n\n        # loop through each race and flatten its data\n        for race in races:\n            # extract required information \n            race_info = { \n                \"season\": race.get(\"season\", \"\"),\n                \"round\": race.get(\"round\", \"\"),\n                \"raceName\": race.get(\"raceName\", \"\"),\n                \"url\": race.get(\"url\",\"\"),\n                \"circuitName\": race.get(\"Circuit\", {}).get(\"circuitName\", \"\"),\n                \"locality\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"locality\", \"\"),\n                \"country\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"country\", \"\"),\n                \"lat\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"lat\", \"\"),\n                \"long\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"long\", \"\"),\n                \"date\": race.get(\"date\", \"\"),\n            }\n            \n            # extract results and combine with useful details\n            results = race.get(\"Results\", [])\n            for result in results:\n                # combine race-level and result-level data\n                combined_data = {**race_info, **result}\n                # add flattened driver and constructor details\n                combined_data.update({\n                    \"driverId\": result.get(\"Driver\", {}).get(\"driverId\", \"\"),\n                    \"driverGivenName\": result.get(\"Driver\", {}).get(\"givenName\", \"\"),\n                    \"driverFamilyName\": result.get(\"Driver\", {}).get(\"familyName\", \"\"),\n                    \"constructorId\": result.get(\"Constructor\", {}).get(\"constructorId\", \"\"),\n                    \"constructorName\": result.get(\"Constructor\", {}).get(\"name\", \"\"),\n                    \"status\": result.get(\"status\", \"\"),\n                    \"timeMillis\": result.get(\"Time\", {}).get(\"millis\", \"\"),\n                    \"time\": result.get(\"Time\", {}).get(\"time\", \"\")\n                })\n                file_results.append(combined_data)\n\n        # append the results for this file to the combined list\n        all_combined_results.extend(file_results)\n\n# aonvert the combined results to a Pandas DataFrame\ndf = pd.DataFrame(all_combined_results)\n\n# aave the combined DataFrame to a CSV file\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#cleaning-weather-data",
    "href": "technical-details/data-cleaning/main.html#cleaning-weather-data",
    "title": "Data Cleaning",
    "section": "Cleaning Weather Data",
    "text": "Cleaning Weather Data\n\nweather_df = pd.read_csv(\"../../data/raw-data/weather/race_data_with_weather.csv\")\n\n\nweather_df.isnull().sum()\n\nseason      0\nraceName    0\nurl         0\nweather     0\ndtype: int64\n\n\n\nweather_df['weather']\n\n0                                                  Sunny\n1                      Overcast with light rain at start\n2                                     Mainly cloudy, dry\n3                                           Cloudy, rain\n4                                     Mainly cloudy, dry\n                             ...                        \n117    Sunny with temperatures reaching up to 27 °C (...\n118    Dry start, with heavy rain and thunderstorm/mo...\n119                                                 Rain\n120                                                Sunny\n121                                          Warm, Sunny\nName: weather, Length: 122, dtype: object\n\n\n\nwe will try to categorise the weather description into one of the following categories:\n\nSunny\nCloudy\nRainy\nWindy\n\n\ndef classify_weather(weather_description):\n\n    weather_description = weather_description.lower()\n    \n    if \"sunny\" in weather_description or \"fine\" in weather_description or \"clear\" in weather_description or \"dry\" in weather_description:\n        return \"Sunny\"\n    elif \"cloudy\" in weather_description or \"overcast\" in weather_description or \"cloud\" in weather_description:\n        return \"Cloudy\"\n    elif \"rain\" in weather_description or \"thunderstorms\" in weather_description or \"drizzle\" in weather_description:\n        return \"Rainy\"\n    elif \"windy\" in weather_description:\n        return \"Windy\"\n    # If no match, classify as \"Not Available\"\n    else:\n        return \"Not Available\" \n\n\n# call classify_weather()\nweather_df['weather_class'] = weather_df['weather'].apply(classify_weather)\n\n# Save the updated DataFrame to a CSV file\noutput_csv = \"../../data/processed-data/classified_weather_data.csv\"\nweather_df.to_csv(output_csv, index=False)\n\n\nweather_df['weather_class'].value_counts()\n\nweather_class\nSunny            95\nCloudy           24\nRainy             2\nNot Available     1\nName: count, dtype: int64\n\n\nThe weather data for 2006 European Grand Prix is not available on wikipedia. - Using longitude, latitude and the date: The weather was Sunny\n\nweather_df['weather_class'] = weather_df['weather_class'].replace('Not Available', 'Sunny')\n\n\noutput_csv = \"../../data/processed-data/classified_weather_data.csv\"\nweather_df.to_csv(output_csv, index=False)\n\n\n# merge race results and weather information\nrace_df = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")\nweather_df = pd.read_csv(\"../../data/processed-data/classified_weather_data.csv\")\n\nmerged_df = race_df.merge(weather_df[['url','weather_class']], on='url', how='left')\n\nmerged_df.to_csv(\"../../data/processed-data/race_weather_merged.csv\", index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#clean-the-merged-data",
    "href": "technical-details/data-cleaning/main.html#clean-the-merged-data",
    "title": "Data Cleaning",
    "section": "Clean the merged data",
    "text": "Clean the merged data\n\nmain_df = pd.read_csv(\"../../data/processed-data/race_weather_merged.csv\")\nmain_df.head()\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\nlat\nlong\ndate\n...\nTime\nFastestLap\ndriverId\ndriverGivenName\ndriverFamilyName\nconstructorId\nconstructorName\ntimeMillis\ntime\nweather_class\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5960396', 'time': '1:39:20.396'}\n{'rank': '1', 'lap': '45', 'Time': {'time': '1...\nalonso\nFernando\nAlonso\nferrari\nFerrari\n5960396.0\n1:39:20.396\nSunny\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5976495', 'time': '+16.099'}\n{'rank': '5', 'lap': '38', 'Time': {'time': '1...\nmassa\nFelipe\nMassa\nferrari\nFerrari\n5976495.0\n+16.099\nSunny\n\n\n2\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5983578', 'time': '+23.182'}\n{'rank': '4', 'lap': '42', 'Time': {'time': '1...\nhamilton\nLewis\nHamilton\nmclaren\nMcLaren\n5983578.0\n+23.182\nSunny\n\n\n3\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5999195', 'time': '+38.799'}\n{'rank': '12', 'lap': '32', 'Time': {'time': '...\nvettel\nSebastian\nVettel\nred_bull\nRed Bull\n5999195.0\n+38.799\nSunny\n\n\n4\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '6000609', 'time': '+40.213'}\n{'rank': '13', 'lap': '45', 'Time': {'time': '...\nrosberg\nNico\nRosberg\nmercedes\nMercedes\n6000609.0\n+40.213\nSunny\n\n\n\n\n5 rows × 29 columns\n\n\n\n\nmain_df.columns\n\nIndex(['season', 'round', 'raceName', 'url', 'circuitName', 'locality',\n       'country', 'lat', 'long', 'date', 'number', 'position', 'positionText',\n       'points', 'Driver', 'Constructor', 'grid', 'laps', 'status', 'Time',\n       'FastestLap', 'driverId', 'driverGivenName', 'driverFamilyName',\n       'constructorId', 'constructorName', 'timeMillis', 'time',\n       'weather_class'],\n      dtype='object')\n\n\n\n# drop un-needed columns\nmain_df = main_df.drop(['lat', 'long', 'number', 'positionText', 'Driver', 'Constructor', 'Time', 'FastestLap'], axis=1)\n\n\nmain_df.columns\n\nIndex(['season', 'round', 'raceName', 'url', 'circuitName', 'locality',\n       'country', 'date', 'position', 'points', 'grid', 'laps', 'status',\n       'driverId', 'driverGivenName', 'driverFamilyName', 'constructorId',\n       'constructorName', 'timeMillis', 'time', 'weather_class'],\n      dtype='object')\n\n\n\nmain_df.isnull().sum()\n\nseason                 0\nround                  0\nraceName               0\nurl                    0\ncircuitName            0\nlocality               0\ncountry                0\ndate                   0\nposition               0\npoints                 0\ngrid                   0\nlaps                   0\nstatus                 0\ndriverId               0\ndriverGivenName        0\ndriverFamilyName       0\nconstructorId          0\nconstructorName        0\ntimeMillis          1291\ntime                1291\nweather_class          0\ndtype: int64\n\n\nThe missing values in ‘timeMillis' and’time' columns are of those drivers who did not finish the race. Therefore, we will drop these columns and try to analyse the performance based on other metrics.\n\n# drop un-needed columns\nmain_df = main_df.drop(['timeMillis', 'time'], axis=1)\n\n\nmain_df['constructorName'].value_counts()\n\nconstructorName\nFerrari           235\nMcLaren           234\nWilliams          229\nRed Bull          184\nRenault           142\nSauber            139\nMercedes          134\nToro Rosso        127\nForce India       100\nHaas F1 Team       79\nToyota             76\nJordan             57\nBAR                56\nMinardi            54\nAlfa Romeo         50\nJaguar             45\nBMW Sauber         40\nAlphaTauri         40\nLotus F1           38\nAlpine F1 Team     30\nAston Martin       30\nHonda              28\nArrows             26\nHRT                24\nMarussia           24\nSuper Aguri        24\nCaterham           24\nRacing Point       20\nProst              18\nBenetton           18\nVirgin             16\nManor Marussia     16\nLotus              16\nBrawn              10\nMF1                 9\nSpyker              8\nName: count, dtype: int64\n\n\n\nmain_df = main_df.drop(['constructorId'], axis=1)\n\nSome of the team names were changed in the process of rebranding or due to a change in ownership. For accurate analysis, we will replace the older versions of the constructors’ names with the current ones.\n\nconstructor_mapping= {\n    \"Jaguar\" : \"Red Bull\",\n    \"BMW Sauber\" : \"Sauber\",\n    \"Alfa Romeo\" : \"Sauber\",\n    \"BAR\" : \"Mercedes\",\n    \"Honda\" : \"Mercedes\",\n    \"Brawn\" : \"Mercedes\",\n    \"Minardi\" : \"AlphaTauri\",\n    \"Toro Rosso\" : \"AlphaTauri\",\n    \"Force India\" : \"Aston Martin\",\n    \"Jordan\" : \"Aston Martin\",\n    \"Racing Point\" : \"Aston Martin\",\n    \"MF1\" : \"Aston Martin\",\n    \"Spyker\" : \"Aston Martin\",\n    \"Lotus F1\" : \"Alpine F1 Team\",\n    \"Renault\" : \"Alpine F1 Team\",\n    \"Benetton\" : \"Alpine F1 Team\",\n    \"Manor Marussia\" : \"Marussia\",\n    \"Virgin\" : \"Marussia\",\n    \"Lotus\" : \"Caterham\",\n\n}\n\n\nmain_df['constructorName'] = main_df['constructorName'].replace(constructor_mapping)\n\n\nmain_df['constructorName'].value_counts()\n\nconstructorName\nFerrari           235\nMcLaren           234\nRed Bull          229\nWilliams          229\nSauber            229\nMercedes          228\nAlpine F1 Team    228\nAston Martin      224\nAlphaTauri        221\nHaas F1 Team       79\nToyota             76\nMarussia           56\nCaterham           40\nArrows             26\nSuper Aguri        24\nHRT                24\nProst              18\nName: count, dtype: int64\n\n\n\nmain_df['driverName'] = main_df['driverGivenName'] + \" \" + main_df['driverFamilyName']\n\n\nmain_df = main_df.drop(['driverGivenName','driverFamilyName'], axis = 1)\n\n\nmain_df.head(2)\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\ndate\nposition\npoints\ngrid\nlaps\nstatus\ndriverId\nconstructorName\nweather_class\ndriverName\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n1\n25.0\n3\n49\nFinished\nalonso\nFerrari\nSunny\nFernando Alonso\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n2\n18.0\n2\n49\nFinished\nmassa\nFerrari\nSunny\nFelipe Massa\n\n\n\n\n\n\n\n\nmain_df['status'].unique()\n\narray(['Finished', '+1 Lap', '+2 Laps', 'Electrical', 'Hydraulics',\n       'Overheating', 'Gearbox', 'Suspension', 'Accident', '+5 Laps',\n       'Wheel', 'Engine', 'Spun off', 'Collision', '+3 Laps', '+4 Laps',\n       '+10 Laps', 'Throttle', 'Clutch', 'Technical', 'Mechanical',\n       'Driveshaft', 'Transmission', 'Steering', 'Puncture', 'Brakes',\n       'Retired', 'Tyre', 'Fuel pressure', '+9 Laps', 'Water leak',\n       'Disqualified', 'Did not qualify', '+42 Laps', 'Engine misfire',\n       'Power Unit', 'Oil pressure', 'Safety concerns', 'Fuel system',\n       '+6 Laps', 'Electronics', 'Collision damage', 'Wheel nut',\n       'Battery', 'Oil leak', '+7 Laps', 'Stalled', 'Exhaust',\n       'Vibrations', 'Broken wing', 'Fuel', 'Wheel rim', 'Power loss',\n       '107% Rule', '+8 Laps', 'ERS', 'Withdrew', 'Cooling system',\n       'Water pump', 'Fuel leak', 'Front wing', 'Turbo', 'Damage',\n       'Out of fuel', 'Radiator', 'Oil line', 'Fuel rig',\n       'Launch control', 'Not classified', 'Pneumatics', 'Differential'],\n      dtype=object)\n\n\n\n# classifying different categories under status\ndef classify_status(status):\n    if status == 'Finished':\n        return 'Finished'\n    elif 'Lap' in status:  # Handles all with 'Lap'\n        return 'Lapped'\n    elif status in ['Accident', 'Collision', 'Spun off', 'Withdrew']:\n        return 'Accident'\n    else:\n        return 'Mechanical'\n\nmain_df['status'] = main_df['status'].apply(classify_status)\n\n\nmain_df['status'].value_counts()\n\nstatus\nFinished      1105\nLapped         693\nMechanical     412\nAccident       190\nName: count, dtype: int64\n\n\n\nmain_df.to_csv(\"../../data/processed-data/race_info.csv\", index=False)\n\n\n# Finish category - new categorical variable\ndata = pd.read_csv(\"../../data/processed-data/race_info.csv\")\ndata['FinishCategory'] = ''\nfor i in range(len(data)):\n    if data['position'][i] in [1,2,3]:\n        data['FinishCategory'][i] = \"Podium\"\n    \n    elif data['position'][i] in [4,5,6,7,8,9,10]:\n        data['FinishCategory'][i] = \"Points Finish\"\n\n    else:\n        data['FinishCategory'][i] = \"No Points\"\n\n\ndata.head()\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\ndate\nposition\npoints\ngrid\nlaps\nstatus\ndriverId\nconstructorName\nweather_class\ndriverName\nFinishCategory\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n1\n25.0\n3\n49\nFinished\nalonso\nFerrari\nSunny\nFernando Alonso\nPodium\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n2\n18.0\n2\n49\nFinished\nmassa\nFerrari\nSunny\nFelipe Massa\nPodium\n\n\n2\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n3\n15.0\n4\n49\nFinished\nhamilton\nMcLaren\nSunny\nLewis Hamilton\nPodium\n\n\n3\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n4\n12.0\n1\n49\nFinished\nvettel\nRed Bull\nSunny\nSebastian Vettel\nPoints Finish\n\n\n4\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n5\n10.0\n5\n49\nFinished\nrosberg\nMercedes\nSunny\nNico Rosberg\nPoints Finish\n\n\n\n\n\n\n\n\ndata.to_csv(\"../../data/processed-data/race_info.csv\", index=False)\n\n\n# race track features\ndata = pd.read_csv(\"../../data/raw-data/circuit_data/merged_circuit_features.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\n0\n2020\nPre-Season Test 1\n4312.438437\n323\n70.673953\n1\n4\n\n\n1\n2020\nPre-Season Test 2\n4312.438437\n323\n70.673953\n1\n4\n\n\n2\n2020\nAustrian Grand Prix\n4312.438437\n323\n70.673953\n1\n4\n\n\n3\n2020\nStyrian Grand Prix\n4292.610384\n300\n46.556886\n2\n6\n\n\n4\n2020\nHungarian Grand Prix\n4348.049386\n318\n58.114374\n0\n6\n\n\n\n\n\n\n\n\ndata.isnull().sum()\n\nYear                   0\nGrand Prix             0\nTrack Length (m)       0\nMax Speed (km/h)       0\nFull Throttle (%)      0\nNumber of Corners      0\nNumber of Straights    0\ndtype: int64\n\n\n\nreq_cols = [\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]\nscaler = StandardScaler()\n\n# Apply the scaler to the dataframe\ndata[req_cols] = scaler.fit_transform(data[req_cols])\n\n\ndata.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\n\n\n\n\n\n\n\n\ndata.to_csv(\"../../data/processed-data/race_track_features.csv\")"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#clustering",
    "href": "technical-details/unsupervised-learning/main.html#clustering",
    "title": "Unsupervised Learning",
    "section": "Clustering",
    "text": "Clustering\n\n# import required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster, set_link_color_palette\n\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n# load the data\ndf = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\nUnnamed: 7\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\nNaN\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\nNaN\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\nNaN\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\nNaN\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\nNaN\n\n\n\n\n\n\n\n\nK-Means\n\n# hyperparameter tuning\nreq_cols = [\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\", \"Number of Corners\", \"Number of Straights\"]\n\n# Initialize lists to store evaluation metrics\nem = []  # For inertia (WCSS)\nss = []  # For silhouette scores\nfor i in range(2,10):\n    kmeans = KMeans(n_clusters = i).fit(df[req_cols])\n    \n    # intertia = within cluster sum of sqaures (WCSS)\n    em.append(kmeans.inertia_)\n    # silhouette scores\n    score = silhouette_score(df[req_cols], kmeans.labels_)\n    ss.append(score)\n\n\nk_values = range(2, 10)\n\nfig, ax1 = plt.subplots(figsize=(10, 6))\n\n# First y-axis for Intertia \nax1.plot(k_values, em, 'blue', marker='o', label='Inertia Score') \nax1.set_xlabel(\"Number of Clusters (k)\", fontsize=14)\nax1.set_ylabel(\"Intertia\", color ='blue', fontsize=14)\nax1.tick_params(axis='y', labelcolor ='blue', labelsize=14)\n\n# Second y-axis for silhouette scores\nax2 = ax1.twinx()\nax2.plot(k_values, ss, 'darkslateblue',marker='o', label='Silhouette Score') \nax2.set_ylabel(\"Silhouette Score\", color = 'darkslateblue', fontsize=14)\nax2.tick_params(axis='y', labelcolor ='darkslateblue', labelsize=14)\n\n# Title\nplt.title(\"Elbow Curve and Silhouette Score vs Number of Clusters\", fontsize=14)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Initializing and fitting the KMeans model with the optimal number of clusters\nclusters = 5\nkmeans = KMeans(n_clusters=clusters, random_state=123, n_init='auto')\nkmeans.fit(df[req_cols])\n\n# Predicting cluster labels\ndf['cluster-kmeans'] = kmeans.labels_\ndf.drop(columns=[\"Unnamed: 7\"], inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\nclusters-dbscan\ncluster-kmeans\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\n-1\n2\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\n-1\n1\n\n\n\n\n\n\n\n\ncustom_colors = [\"cyan\", \"deepskyblue\", \"blue\", \"darkslateblue\", \"royalblue\"]\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=df['Max Speed (km/h)'], y=df['Full Throttle (%)'], hue=df['cluster-kmeans'], palette=custom_colors)\nplt.title('KMeans Clustering (bill length vs bill depth)')\n\nplt.xlabel('Max Speed')\nplt.ylabel('Full Throttle')\nplt.show()\n\n\n\n\n\n\n\n\n\n\nDBSCAN\n\n\nfeatures = df[[\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\", \"Number of Corners\", \"Number of Straights\"]]\n\n\neps_range = np.arange(0.1, 2.0, 0.1)\n# Range for min_samples to test\nmin_samples_range = range(1, 20)\n\n# Variables to store the best results\nbest_score = -1\nopt_eps = None\nbest_min_samples = None\nbest_cluster_count = None\n\n# Loop through each combination of eps and min_samples\nfor eps in eps_range:\n    for min_samples in min_samples_range:\n        # Initialize DBSCAN with the current parameters\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n        # Predict cluster labels\n        labels = dbscan.fit_predict(features)\n        \n        # Ignore configurations that result in a single cluster or all points being noise\n        if len(set(labels)) &gt; 1:\n            # Calculate silhouette score for the current parameters\n            score = silhouette_score(features, labels)\n            \n            # Update best parameters if the current score is the highest\n            if score &gt; best_score:\n                best_score = score\n                opt_eps = eps\n                best_min_samples = min_samples\n                # Count clusters excluding noise\n                best_cluster_count = len(set(labels)) - (1 if -1 in labels else 0)\n\n# Print the optimal parameters and corresponding silhouette score\nprint(\"Best Silhouette Score:\", best_score)\nprint(\"Optimal eps:\", opt_eps)\nprint(\"Optimal min_samples:\", best_min_samples)\nprint(\"Optimal number of clusters:\", best_cluster_count)\n\nBest Silhouette Score: 0.4901591287514251\nOptimal eps: 0.4\nOptimal min_samples: 5\nOptimal number of clusters: 1\n\n\n\nbest_min_samples = best_min_samples\noptimal_eps = opt_eps\n\n# Initializing and fitting the DBSCAN model with the optimal parameters\ndbscan = DBSCAN(eps = opt_eps, min_samples = best_min_samples)\nlabels_final = dbscan.fit_predict(features)\n\n\ndf['clusters-dbscan'] = labels_final\n\n\n\nHierarchical Clustering\n\nset_link_color_palette([\"lightskyblue\", \"cornflowerblue\", \"blue\", \"steelblue\"])\n\ndf = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\ndf = df[df[\"Year\"] == 2023]\n\nfeatures = [\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\", \"Number of Corners\", \"Number of Straights\"]\nX = df[features]\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\nlinkage_matrix = linkage(X_scaled, method='ward')  \n\nplt.figure(figsize=(10, 7))\ndendrogram(linkage_matrix, labels=df[\"Grand Prix\"].values, leaf_rotation=90, leaf_font_size=10,color_threshold=6)\nplt.title(\"Hierarchical Clustering Dendrogram - 2023\", fontsize = 14)\n\nplt.xlabel(\"Grand Prix\", fontsize = 14)\nplt.ylabel(\"Distance\", fontsize = 14)\nplt.show()\n\nmax_distance = 6\nclusters = fcluster(linkage_matrix, max_distance, criterion='distance')\n\n\n\n\n\n\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\nclusters-dbscan\ncluster-kmeans\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\n-1\n2\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\n-1\n1"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#k-means",
    "href": "technical-details/unsupervised-learning/main.html#k-means",
    "title": "Unsupervised Learning",
    "section": "K-Means",
    "text": "K-Means\n\n# hyperparameter tuning\nreq_cols = [\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\", \"Number of Corners\", \"Number of Straights\"]\n\n# Initialize lists to store evaluation metrics\nem = []  # For inertia (WCSS)\nss = []  # For silhouette scores\nfor i in range(2,10):\n    kmeans = KMeans(n_clusters = i).fit(df[req_cols])\n    \n    # intertia = within cluster sum of sqaures (WCSS)\n    em.append(kmeans.inertia_)\n    # silhouette scores\n    score = silhouette_score(df[req_cols], kmeans.labels_)\n    ss.append(score)\n\n\nk_values = range(2, 10)\n\nfig, ax1 = plt.subplots(figsize=(10, 6))\n\n# First y-axis for Intertia \nax1.plot(k_values, em, 'blue', marker='o', label='Inertia Score') \nax1.set_xlabel(\"Number of Clusters (k)\", fontsize=14)\nax1.set_ylabel(\"Intertia\", color ='blue', fontsize=14)\nax1.tick_params(axis='y', labelcolor ='blue', labelsize=14)\n\n# Second y-axis for silhouette scores\nax2 = ax1.twinx()\nax2.plot(k_values, ss, 'darkslateblue',marker='o', label='Silhouette Score') \nax2.set_ylabel(\"Silhouette Score\", color = 'darkslateblue', fontsize=14)\nax2.tick_params(axis='y', labelcolor ='darkslateblue', labelsize=14)\n\n# Title\nplt.title(\"Elbow Curve and Silhouette Score vs Number of Clusters\", fontsize=14)\nfig.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Initializing and fitting the KMeans model with the optimal number of clusters\nclusters = 5\nkmeans = KMeans(n_clusters=clusters, random_state=123, n_init='auto')\nkmeans.fit(df[req_cols])\n\n# Predicting cluster labels\ndf['cluster-kmeans'] = kmeans.labels_\ndf.drop(columns=[\"Unnamed: 7\"], inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\nclusters-dbscan\ncluster-kmeans\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n-1\n1\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\n-1\n2\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\n-1\n1\n\n\n\n\n\n\n\n\ncustom_colors = [\"cyan\", \"deepskyblue\", \"blue\", \"darkslateblue\", \"royalblue\"]\n\nplt.figure(figsize=(8, 6))\nsns.scatterplot(x=df['Max Speed (km/h)'], y=df['Full Throttle (%)'], hue=df['cluster-kmeans'], palette=custom_colors)\nplt.title('KMeans Clustering (bill length vs bill depth)')\n\nplt.xlabel('Max Speed')\nplt.ylabel('Full Throttle')\nplt.show()"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#dbscan",
    "href": "technical-details/unsupervised-learning/main.html#dbscan",
    "title": "Unsupervised Learning",
    "section": "DBSCAN",
    "text": "DBSCAN\nApplied to the data reduced to 2D space using the PCA algorithm\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) is a clustering algorithm that identifies clusters based on the density of data points. Unlike K-Means, which requires the number of clusters to be predefined, DBSCAN automatically detects clusters of arbitrary shapes and sizes and can identify outliers (noise) in the data.\nHyperparameters:\n\nEpsilon (ε): The maximum distance between two points for them to be considered as neighbors.\nMinPts: The minimum number of points required to form a dense region (a cluster).\n\nTypes of Points:\n\nCore Points: A point is a core point if it has at least MinPts neighbors within distance ε.\nBorder Points: A point that is within ε of a core point but does not meet the MinPts requirement itself.\nNoise Points: Any point that is neither a core point nor a border point is treated as noise or an outlier. DBSCAN groups points into clusters by iteratively expanding the dense regions (clusters) starting from core points.\n\nKey Characteristics:\n\nDensity-Based: Works well for identifying clusters of arbitrary shapes (e.g., circular, elongated).\nNoise Detection: Automatically identifies outliers as noise, which is useful for datasets with anomalies.\nNo Need to Specify k: Unlike K-Means, DBSCAN does not require the number of clusters as input.\nParameter Sensitivity: The choice of ε and MinPts can significantly impact results and requires tuning.\n\n\ndef optimize_dbscan_with_heatmap(X, eps_range, min_samples_range):\n    best_score = -1\n    best_eps = None\n    best_min_samples = None\n    best_labels = None\n    best_cluster_count = None\n\n    # initialize a dictionary to store silhouette scores for heatmap\n    score_dict = {}\n\n    # loop through each eps value in the range\n    for eps in eps_range:\n        score_dict[eps] = []\n        # loop through min_samples values\n        for min_samples in min_samples_range:\n            # Initialize DBSCAN\n            model = DBSCAN(eps=eps, min_samples=min_samples)\n            labels = model.fit_predict(X)\n\n            # calculate silhouette score if there are more than 1 cluster\n            if len(set(labels)) &gt; 1: \n                score = silhouette_score(X, labels)\n            else:\n                score = -1  \n            # append the score to score dictionary\n            score_dict[eps].append(score)\n\n            # update the score if a better score if found\n            if score &gt; best_score:\n                best_score = score\n                best_eps = eps\n                best_min_samples = min_samples\n                best_labels = labels\n                # count clusters excluding noise\n                best_cluster_count = len(set(labels)) - (1 if -1 in labels else 0)\n\n    # convert the score dictionary to a DataFrame for heatmap\n    score_df = pd.DataFrame(score_dict, index=min_samples_range).T\n\n    # plot the heatmap\n    plt.figure(figsize=(12, 8))\n    sns.heatmap(score_df, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=min_samples_range, yticklabels=eps_range)\n    plt.title(\"Silhouette Score Heatmap for DBSCAN\")\n    plt.xlabel(\"Min Samples\")\n    plt.ylabel(\"Eps\")\n    plt.show()\n\n    # output the optimal parameters\n    print(f\"Best Silhouette Score: {best_score}\")\n    print(f\"Optimal eps: {best_eps}\")\n    print(f\"Optimal min_samples: {best_min_samples}\")\n\n    return best_eps, best_min_samples, best_cluster_count, best_labels\n\neps_range = np.arange(0.1, 1.0, 0.1) \nmin_samples_range = range(2, 10)  \n\nbest_eps, best_min_samples, best_cluster_count, best_labels = optimize_dbscan_with_heatmap(X, eps_range, min_samples_range)\n\n\n\n\n\n\n\n\nBest Silhouette Score: 0.7156373392601885\nOptimal eps: 0.6\nOptimal min_samples: 3\n\n\n\n# apply DBCSAN with optimal paramters \ndbscan = DBSCAN(eps=best_eps, min_samples=best_min_samples)\ndbscan_labels = dbscan.fit_predict(X_pca)\n\ncustom_colors = [\"lightskyblue\", \"cornflowerblue\", \"blue\", \"steelblue\", \"deepskyblue\", \"cyan\"]\ncustom_cmap = ListedColormap(custom_colors)\n\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=dbscan_labels, cmap=custom_cmap, alpha=0.7)\nplt.title(\"DBSCAN Clustering Results on PCA-transformed Data (2D)\")\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.colorbar(scatter, label=\"Cluster Labels\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nnum_clusters = len(set(dbscan_labels)) - (1 if -1 in dbscan_labels else 0)\nprint(f\"Number of clusters found: {num_clusters}\")\n\nNumber of clusters found: 6"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "href": "technical-details/unsupervised-learning/main.html#hierarchical-clustering",
    "title": "Unsupervised Learning",
    "section": "Hierarchical Clustering",
    "text": "Hierarchical Clustering\nHierarchical Clustering is an unsupervised machine learning technique used to group data into a hierarchy of clusters. It builds a tree-like structure called a dendrogram, which visually represents the relationships between data points and how clusters are formed step by step.\nTypes:\nAgglomerative Clustering (Bottom-Up):\n\nstarts with each data point as an individual cluster.\nIteratively combines the closest clusters based on a distance metric.\nContinues until all points are merged into a single cluster.\nThe dendrogram can be cut at different levels to obtain the desired number of clusters.\n\nDivisive Clustering (Top-Down):\n\nStarts with all data points in a single cluster.\nIteratively splits clusters into smaller subclusters until each data point becomes its own cluster.\n\nDistance Metric Common metrics include:\n\nEuclidean Distance: Measures the straight-line distance between points.\nManhattan Distance: Measures distance as the sum of absolute differences.\nCosine Distance: Measures the angle between two vectors.\n\nIn this project: Hierarchical clustering is applied to understand groupings within the racetrack features dataset.\n\n# load the data\ndf = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\ndf.drop(columns=['Unnamed: 7'], inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\n\n\n\n\n\n\n\n\nset_link_color_palette([\"lightskyblue\", \"cornflowerblue\", \"blue\", \"steelblue\"])\n\ndf = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\n# only circuits from 2023\ndf = df[df[\"Year\"] == 2023]\ndf.dropna\n\n# features used for clustering\nfeatures = [\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\", \"Number of Corners\", \"Number of Straights\"]\nX = df[features]\n\n# standardize the features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# ward's linkage method - minimizes variance within clusters\nlinkage_matrix = linkage(X_scaled, method='ward')  \n\nplt.figure(figsize=(10, 7))\ndendrogram(linkage_matrix, labels=df[\"Grand Prix\"].values, leaf_rotation=90, leaf_font_size=10,color_threshold=6)\nplt.title(\"Hierarchical Clustering Dendrogram - 2023\", fontsize = 14)\n\nplt.xlabel(\"Grand Prix\", fontsize = 14)\nplt.ylabel(\"Distance\", fontsize = 14)\nplt.show()\n\nmax_distance = 5\nclusters = fcluster(linkage_matrix, max_distance, criterion='distance')\n\n\n\n\n\n\n\n\n\n\n# group circuits based on clusters\nmax_distance = 5\nclusters = fcluster(linkage_matrix, max_distance, criterion='distance')\ndf[\"Cluster\"] = clusters\ngrouped_df = df.groupby(\"Cluster\")[features].mean()\ngrouped_df.T\n\n\n\n\n\n\n\nCluster\n1\n2\n3\n4\n\n\n\n\nTrack Length (m)\n1.116553\n-0.524224\n0.103169\n-2.269306\n\n\nMax Speed (km/h)\n0.884510\n0.015604\n0.793869\n-2.591115\n\n\nFull Throttle (%)\n0.877347\n0.155178\n-0.538241\n-2.058832\n\n\nNumber of Corners\n-0.360778\n-0.853982\n0.947287\n2.298239\n\n\nNumber of Straights\n-0.788297\n-0.488102\n0.750199\n0.862772"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#pitstop-data",
    "href": "technical-details/data-cleaning/main.html#pitstop-data",
    "title": "Data Cleaning",
    "section": "Pitstop data",
    "text": "Pitstop data\n\ndf = pd.read_csv(\"../../data/raw-data/pitstop_data.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nYear\nRound\nRaceName\nDriverID\nLap\nStop\nTime\nDuration\n\n\n\n\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n1\n1\n17:05:23\n26.898\n\n\n1\n2011\n1\nAustralian Grand Prix\nmichael_schumacher\n1\n1\n17:05:52\n25.021\n\n\n2\n2011\n1\nAustralian Grand Prix\nwebber\n11\n1\n17:20:48\n23.426\n\n\n3\n2011\n1\nAustralian Grand Prix\nalonso\n12\n1\n17:22:34\n23.251\n\n\n4\n2011\n1\nAustralian Grand Prix\nmassa\n13\n1\n17:24:10\n23.842\n\n\n\n\n\n\n\nPivoting involves reshaping data by rearranging rows into columns. It is used to transform long-format data (many rows for each entity) to wide-format (one row per entity with multiple columns).\nHere, each Stop Number becomes a seperate set of columns, Lap1, Lap2, Duration1, Duration2 and so on.\n\ntransformed = (\n    # group the data by year, round, race name and driver\n    # create Pit stop number for each pitstop of a driver in the data\n    # pit stop number will be used for pivoting\n    df.assign(PitStopNumber=lambda x: x.groupby(['Year', 'Round', 'RaceName', 'DriverID']).cumcount() + 1)\n    # index[] - unique identifiers\n    # columns - used to create new columns for each pit stop\n    # values - values to be assigned to the new columns in the pivot table\n    .pivot(index=['Year', 'Round', 'RaceName', 'DriverID'], columns='PitStopNumber', values=['Lap', 'Stop', 'Time', 'Duration'])\n)\n\n# after pivoting, cols have multi-index structure\n# combine col name with pit stop number for easier handling\ntransformed.columns = [f\"{col[0]}{col[1]}\" for col in transformed.columns]\n\n# reset index to turn it into a DataFrame\ntransformed.reset_index(inplace=True)\n\n# handling missing values - replace with 0\ntransformed = transformed.fillna(0)\n\ntransformed.head(2)\n\n\n\n\n\n\n\n\nYear\nRound\nRaceName\nDriverID\nLap1\nLap2\nLap3\nLap4\nLap5\nLap6\n...\nTime5\nTime6\nTime7\nDuration1\nDuration2\nDuration3\nDuration4\nDuration5\nDuration6\nDuration7\n\n\n\n\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n1\n17\n35\n0\n0\n0\n...\n0\n0\n0\n26.898\n24.463\n26.348\n0\n0\n0\n0\n\n\n1\n2011\n1\nAustralian Grand Prix\nalonso\n12\n27\n42\n0\n0\n0\n...\n0\n0\n0\n23.251\n24.733\n24.181\n0\n0\n0\n0\n\n\n\n\n2 rows × 32 columns\n\n\n\n\ntransformed.shape\n\n(5118, 32)\n\n\nMinMaxx Scaling: Transforms the fature range by scaling its min and max values.\nFormula: \\[X_{scaled} = \\frac{X - X_{min}}{X_{max} - X_{min}}\\]\n\n# convert relevant columns to minutes\nfrom sklearn.preprocessing import MinMaxScaler\ndef convert_to_minutes(value):\n    try:\n        # convert seconds to minutes\n        return float(value) / 60  \n    except:\n        # default to 0 for invalid values\n        return 0  \n# List of time and duration columns\ntime_and_duration_cols = [\n    \"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\",\n    \"Duration1\", \"Duration2\", \"Duration3\", \"Duration4\", \"Duration5\", \"Duration6\", \"Duration7\"\n]\n\n# convert each column to minutes\nfor col in time_and_duration_cols:\n    if col in transformed.columns:\n        transformed[col] = transformed[col].apply(convert_to_minutes)\n\n# handling missing values - replace with 0\ntransformed.fillna(0, inplace=True)\n\n# select all columns except `Year`, `Round`, `RaceName`, and `DriverID` for scaling\nscalable_columns = transformed.drop(columns=[\"Year\", \"Round\", \"RaceName\", \"DriverID\"]).columns\n\n# apply MinMax scaling\nscaler = MinMaxScaler()\ntransformed[scalable_columns] = scaler.fit_transform(transformed[scalable_columns])\n\nprint(transformed.head())\n\n   Year  Round               RaceName     DriverID      Lap1      Lap2  \\\n0  2011      1  Australian Grand Prix  alguersuari  0.000000  0.229730   \n1  2011      1  Australian Grand Prix       alonso  0.174603  0.364865   \n2  2011      1  Australian Grand Prix     ambrosio  0.206349  0.513514   \n3  2011      1  Australian Grand Prix  barrichello  0.190476  0.310811   \n4  2011      1  Australian Grand Prix        buemi  0.222222  0.391892   \n\n       Lap3      Lap4  Lap5  Lap6  ...  Time5  Time6  Time7  Duration1  \\\n0  0.479452  0.000000   0.0   0.0  ...    0.0    0.0    0.0   0.453661   \n1  0.575342  0.000000   0.0   0.0  ...    0.0    0.0    0.0   0.392151   \n2  0.000000  0.000000   0.0   0.0  ...    0.0    0.0    0.0   0.426017   \n3  0.383562  0.512821   0.0   0.0  ...    0.0    0.0    0.0   0.398762   \n4  0.000000  0.000000   0.0   0.0  ...    0.0    0.0    0.0   0.427417   \n\n   Duration2  Duration3  Duration4  Duration5  Duration6  Duration7  \n0   0.428042   0.457423   0.000000        0.0        0.0        0.0  \n1   0.432766   0.419802   0.000000        0.0        0.0        0.0  \n2   0.462739   0.000000   0.000000        0.0        0.0        0.0  \n3   0.662386   0.293259   0.469108        0.0        0.0        0.0  \n4   0.404192   0.000000   0.000000        0.0        0.0        0.0  \n\n[5 rows x 32 columns]\n\n\n\ntransformed.to_csv(\"../../data/processed-data/pitstop.csv\")"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#feature-selection",
    "href": "technical-details/supervised-learning/main.html#feature-selection",
    "title": "Supervised Learning",
    "section": "Feature Selection",
    "text": "Feature Selection\n\n# import required libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_selection import RFE, mutual_info_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n\n\n# Define the binary target variable\ndf = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\ndf['points_category'] = df['position'].apply(lambda x: 1 if x &lt;= 10 else 0)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYear\nRound\nRaceName\nDriverID\nLap1\nLap2\nLap3\nLap4\nLap5\n...\nDuration2\nDuration3\nDuration4\nDuration5\nDuration6\nDuration7\nconstructorName\nposition\ngrid\npoints_category\n\n\n\n\n0\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n-1.492522\n-0.208201\n1.339923\n-0.281099\n-0.15023\n...\n0.733521\n1.882538\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nAlphaTauri\n11.0\n12.0\n0\n\n\n1\n1\n2011\n1\nAustralian Grand Prix\nalonso\n-0.414215\n0.355923\n1.723412\n-0.281099\n-0.15023\n...\n0.755727\n1.679973\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nFerrari\n4.0\n5.0\n1\n\n\n2\n2\n2011\n1\nAustralian Grand Prix\nambrosio\n-0.218159\n0.976458\n-0.577525\n-0.281099\n-0.15023\n...\n0.896614\n-0.580401\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nMarussia\n14.0\n22.0\n0\n\n\n3\n3\n2011\n1\nAustralian Grand Prix\nbarrichello\n-0.316187\n0.130273\n0.956433\n3.039918\n-0.15023\n...\n1.835036\n0.998617\n3.860649\n-0.130754\n-0.060017\n-0.01976\nWilliams\n16.0\n17.0\n0\n\n\n4\n4\n2011\n1\nAustralian Grand Prix\nbuemi\n-0.120131\n0.468747\n-0.577525\n-0.281099\n-0.15023\n...\n0.621420\n-0.580401\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nAlphaTauri\n8.0\n10.0\n1\n\n\n\n\n5 rows × 37 columns\n\n\n\n\n# Define features and target\nfeatures = [\n    \"Year\",\n    \"Lap1\", \"Lap2\", \"Lap3\", \"Lap4\", \"Lap5\",\n    \"Stop1\", \"Stop2\", \"Stop3\", \"Stop4\", \"Stop5\", \n    \"Duration1\", \"Duration2\", \"Duration3\", \"Duration4\", \"Duration5\",\n    \"Duration6\", \"Duration7\", \"grid\"\n]\nX = df[features]\ny = df[\"points_category\"]  \n\n\n# 1. Correlation Analysis\nplt.figure(figsize=(10, 6))\ncorrelations = X.corrwith(y)\ncorrelations.sort_values().plot(kind='barh', color='deepskyblue')\nplt.title(\"Correlation Between Features and Target\")\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Correlation Coefficient\", fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.grid(axis='x')\nplt.show()\n\n\n\n\n\n\n\n\n\n# 2. Recursive Feature Elimination (RFE)\nmodel = LogisticRegression(max_iter=1000, random_state=42)\nrfe = RFE(model, n_features_to_select=10)\nrfe.fit(X, y)\nrfe_ranking = pd.Series(rfe.ranking_, index=features).sort_values()\n\nplt.figure(figsize=(10, 6))\nrfe_ranking.plot(kind='barh', color='deepskyblue')\nplt.title(\"Feature Importance by Recursive Feature Elimination (RFE)\", fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"RFE Ranking (Lower is Better)\", fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.grid(axis='x')\nplt.show()\n\n\n\n\n\n\n\n\n\n# 3. Mutual Information\nmutual_info = mutual_info_classif(X, y, random_state=42)\nmutual_info_series = pd.Series(mutual_info, index=features).sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nmutual_info_series.plot(kind='barh', color='deepskyblue')\nplt.title(\"Feature Importance by Mutual Information\", fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.xlabel(\"Mutual Information Score\", fontsize=14)\nplt.grid(axis='x')\nplt.show()\n\n\n\n\n\n\n\n\nfeatures: grid, Year, Duration1, Duration2, Duration3, Duration4, Lap1, Lap2, Lap3, Stop2"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#binary-classification",
    "href": "technical-details/supervised-learning/main.html#binary-classification",
    "title": "Supervised Learning",
    "section": "Binary Classification",
    "text": "Binary Classification\n\nDecision Trees\n\ndf_transformed = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\ndf_transformed['points_category'] = df_transformed['position'].apply(lambda x: 1 if x &lt;= 10 else 0)\n\n\n# Selected features\nX = df_transformed[['grid', 'Duration1', 'Duration2', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3']]\ny = df_transformed['points_category']  \n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n# Initialize the Decision Tree Classifier\ndt = DecisionTreeClassifier(random_state=42)\n\n# Fit the model\ndt.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = dt.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report - Decision Tree:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(dt, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Decision Tree\")\nplt.show()\n\nClassification Report - Decision Tree:\n              precision    recall  f1-score   support\n\n           0       0.64      0.69      0.67       114\n           1       0.71      0.66      0.68       128\n\n    accuracy                           0.67       242\n   macro avg       0.67      0.67      0.67       242\nweighted avg       0.68      0.67      0.67       242\n\n\n\n\n\n\n\n\n\n\n\n# Define max_depth range\nmax_depth_range = range(1, 25)\n\n# Store metrics for each max_depth\ntrain_accuracies_y1 = []  # Accuracy for y=1\ntest_accuracies_y1 = []\ntrain_recalls_y0 = []     # Recall for y=0\ntest_recalls_y0 = []\n\ntrain_accuracies = []     # Overall accuracy\ntest_accuracies = []\ntrain_recalls = []        # Overall recall for y=1\ntest_recalls = []\n\nfor max_depth in max_depth_range:\n    # Initialize Decision Tree Classifier\n    dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n    dt.fit(X_train, y_train)\n    \n    # Predictions on training and test data\n    y_train_pred = dt.predict(X_train)\n    y_test_pred = dt.predict(X_test)\n    \n    # Calculate overall accuracy\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n    \n    # Calculate recall for y=1\n    train_recalls.append(recall_score(y_train, y_train_pred, pos_label=1))\n    test_recalls.append(recall_score(y_test, y_test_pred, pos_label=1))\n    \n    # Accuracy for y=1\n    train_accuracies_y1.append(accuracy_score(y_train[y_train == 1], y_train_pred[y_train == 1]))\n    test_accuracies_y1.append(accuracy_score(y_test[y_test == 1], y_test_pred[y_test == 1]))\n    \n    # Recall for y=0\n    train_recalls_y0.append(recall_score(y_train, y_train_pred, pos_label=0))\n    test_recalls_y0.append(recall_score(y_test, y_test_pred, pos_label=0))\n\n# Plotting all metrics in a (2, 2) grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Subplot 1: Overall Accuracy\naxes[0, 0].plot(max_depth_range, train_accuracies, 'o-', label='Training', color='blue')\naxes[0, 0].plot(max_depth_range, test_accuracies, 'o-', label='Test', color='deepskyblue')\naxes[0, 0].set_title(\"Accuracy\")\naxes[0, 0].set_xlabel(\"Max Depth\")\naxes[0, 0].set_ylabel(\"Accuracy\")\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# Subplot 2: Recall for y=1\naxes[1, 0].plot(max_depth_range, train_recalls, 'o-', label='Training', color='blue')\naxes[1, 0].plot(max_depth_range, test_recalls, 'o-', label='Test', color='deepskyblue')\naxes[1, 0].set_title(\"Recall for y=1\")\naxes[1, 0].set_xlabel(\"Max Depth\")\naxes[1, 0].set_ylabel(\"Recall\")\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# Subplot 3: Recall for y=0\naxes[1, 1].plot(max_depth_range, train_recalls_y0, 'o-', label='Training', color='blue')\naxes[1, 1].plot(max_depth_range, test_recalls_y0, 'o-', label='Test', color='deepskyblue')\naxes[1, 1].set_title(\"Recall for y=0\")\naxes[1, 1].set_xlabel(\"Max Depth\")\naxes[1, 1].set_ylabel(\"Recall\")\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nfig.delaxes(axes[0, 1])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Define the parameter grid with max_depth fixed as 5\nparam_grid = {\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 5]\n}\n\n# Perform Grid Search with max_depth fixed as 5\ngrid_search = GridSearchCV(DecisionTreeClassifier(max_depth=5, random_state=123), \n                           param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Evaluate the best model\nbest_dt = grid_search.best_estimator_\ny_pred_best = best_dt.predict(X_test)\n\n# Evaluation Metrics for Best Model\nprint(\"Classification Report - Optimized Decision Tree with max_depth=5:\")\nprint(classification_report(y_test, y_pred_best))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(best_dt, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Decision Tree with max_depth=5\")\nplt.show()\n\nBest Parameters: {'min_samples_leaf': 2, 'min_samples_split': 5}\nClassification Report - Optimized Decision Tree with max_depth=5:\n              precision    recall  f1-score   support\n\n           0       0.81      0.75      0.78       114\n           1       0.79      0.84      0.82       128\n\n    accuracy                           0.80       242\n   macro avg       0.80      0.79      0.80       242\nweighted avg       0.80      0.80      0.80       242\n\n\n\n\n\n\n\n\n\n\n\n# ROC Curve for the Initial Decision Tree\ny_proba_initial = dt.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_initial, tpr_initial, _ = roc_curve(y_test, y_proba_initial)\nroc_auc_initial = auc(fpr_initial, tpr_initial)\n\n# ROC Curve for the Optimized Decision Tree\ny_proba_optimized = best_dt.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_optimized, tpr_optimized, _ = roc_curve(y_test, y_proba_optimized)\nroc_auc_optimized = auc(fpr_optimized, tpr_optimized)\n\n# Plot both ROC curves\nplt.figure(figsize=(10, 6))\nplt.plot(fpr_initial, tpr_initial, label=f'Initial Decision Tree (AUC = {roc_auc_initial:.2f})', color='deepskyblue', lw=2)\nplt.plot(fpr_optimized, tpr_optimized, label=f'Optimized Decision Tree (AUC = {roc_auc_optimized:.2f})', color='blue', lw=2)\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  \nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves - Decision Tree')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\n\nRandom Forest\n\ndf_transformed = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\ndf_transformed['points_category'] = df_transformed['position'].apply(lambda x: 1 if x &lt;= 10 else 0)\nX = df_transformed[['grid', 'Year', 'Duration1', 'Duration2', 'Duration3', 'Duration4', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3', 'Stop4']]\ny = df_transformed['points_category']  \n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n# Initialize Random Forest Classifier\nrf = RandomForestClassifier(random_state=123)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict\ny_pred = rf.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Random Forest\")\nplt.show()\n\n# Accuracy Score\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.77      0.79      0.78       114\n           1       0.81      0.79      0.80       128\n\n    accuracy                           0.79       242\n   macro avg       0.79      0.79      0.79       242\nweighted avg       0.79      0.79      0.79       242\n\n\n\n\n\n\n\n\n\n\nAccuracy: 0.7892561983471075\n\n\n\n# Define hyperparameters\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Perform Grid Search\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=123),\n                           param_grid=param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# Evaluate with best parameters\nbest_rf = grid_search.best_estimator_\ny_pred_best = best_rf.predict(X_test)\n\n# Classification Report\nprint(\"Classification Report - Optimized Random Forest:\")\nprint(classification_report(y_test, y_pred_best))\n\nBest Parameters: {'max_depth': 20, 'min_samples_split': 2, 'n_estimators': 100}\nClassification Report - Optimized Random Forest:\n              precision    recall  f1-score   support\n\n           0       0.77      0.78      0.78       114\n           1       0.80      0.80      0.80       128\n\n    accuracy                           0.79       242\n   macro avg       0.79      0.79      0.79       242\nweighted avg       0.79      0.79      0.79       242\n\n\n\n\n# Confusion Matrix for Optimized Random Forest\nConfusionMatrixDisplay.from_estimator(best_rf, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Random Forest\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# ROC Curve for the Initial Random Forest\ny_proba_initial = rf.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_initial, tpr_initial, _ = roc_curve(y_test, y_proba_initial)\nroc_auc_initial = auc(fpr_initial, tpr_initial)\n\n# ROC Curve for the Optimized Random Forest\ny_proba_optimized = best_rf.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_optimized, tpr_optimized, _ = roc_curve(y_test, y_proba_optimized)\nroc_auc_optimized = auc(fpr_optimized, tpr_optimized)\n\n# Plot ROC Curves\nplt.figure(figsize=(10, 6))\nplt.plot(fpr_initial, tpr_initial, label=f'Initial Random Forest (AUC = {roc_auc_initial:.2f})', color='deepskyblue', lw=2)\nplt.plot(fpr_optimized, tpr_optimized, label=f'Optimized Random Forest (AUC = {roc_auc_optimized:.2f})', color='blue', lw=2)\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  # Diagonal line for random guessing\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves - Random Forest')\nplt.legend(loc=\"lower right\")\nplt.show()"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#multiclass-classification",
    "href": "technical-details/supervised-learning/main.html#multiclass-classification",
    "title": "Supervised Learning",
    "section": "Multiclass Classification",
    "text": "Multiclass Classification\n\nRandom Forest Classifier\n\n# Create a new column with binned classes based on 'position'\ndef position_to_class(position):\n    if 1 &lt;= position &lt;= 5:\n        return 1\n    elif 6 &lt;= position &lt;= 10:\n        return 2\n    elif 11 &lt;= position &lt;= 15:\n        return 3\n    elif 16 &lt;= position &lt;= 20:\n        return 4\n    else:\n        return None  \n\ndf_transformed['position_class'] = df_transformed['position'].apply(position_to_class)\n\ndf_transformed = df_transformed.dropna(subset=['position_class'])\n\n# Define features and target\nX = df_transformed[['grid', 'Year', 'Duration1', 'Duration2', 'Duration3', 'Duration4', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3', 'Stop4']]\ny = df_transformed['position_class']\n\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n# Initialize Random Forest Classifier\nrf_multi = RandomForestClassifier(random_state=42)\n\n# Train the model\nrf_multi.fit(X_train, y_train)\n\n# Predict\ny_pred_multi = rf_multi.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report - Multi-class Random Forest:\")\nprint(classification_report(y_test, y_pred_multi))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(rf_multi, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Multi-class Random Forest\")\nplt.show()\n\nClassification Report - Multi-class Random Forest:\n              precision    recall  f1-score   support\n\n         1.0       0.67      0.77      0.71        65\n         2.0       0.38      0.41      0.40        63\n         3.0       0.40      0.38      0.39        61\n         4.0       0.50      0.37      0.42        46\n\n    accuracy                           0.49       235\n   macro avg       0.49      0.48      0.48       235\nweighted avg       0.49      0.49      0.49       235\n\n\n\n\n\n\n\n\n\n\n\n# Define hyperparameters\nparam_grid_multi = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# Perform Grid Search\ngrid_search_multi = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n                                 param_grid=param_grid_multi, cv=5, scoring='accuracy')\ngrid_search_multi.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best Parameters:\", grid_search_multi.best_params_)\n\n# Evaluate with best parameters\nbest_rf_multi = grid_search_multi.best_estimator_\ny_pred_best_multi = best_rf_multi.predict(X_test)\n\n# Classification Report\nprint(\"Classification Report - Optimized Multi-class Random Forest:\")\nprint(classification_report(y_test, y_pred_best_multi))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(best_rf_multi, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Multi-class Random Forest\")\nplt.show()\n\nBest Parameters: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 200}\nClassification Report - Optimized Multi-class Random Forest:\n              precision    recall  f1-score   support\n\n         1.0       0.66      0.74      0.70        65\n         2.0       0.37      0.40      0.38        63\n         3.0       0.41      0.46      0.43        61\n         4.0       0.50      0.28      0.36        46\n\n    accuracy                           0.49       235\n   macro avg       0.48      0.47      0.47       235\nweighted avg       0.49      0.49      0.48       235\n\n\n\n\n\n\n\n\n\n\n\n\nKNN\n\n# Range of k values to test\nk_values = range(1, 25)\n\n# Lists to store training and test accuracy\ntrain_accuracies = []\ntest_accuracies = []\n\nfor k in k_values:\n    # Initialize KNN with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    # Train the model\n    knn.fit(X_train, y_train)\n    \n    # Predict on training and test data\n    y_train_pred = knn.predict(X_train)\n    y_test_pred = knn.predict(X_test)\n    \n    # Calculate accuracy for training and test data\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n\n# Plot accuracy vs. k value\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, train_accuracies, label=\"Training Accuracy\", marker='o', color='blue')\nplt.plot(k_values, test_accuracies, label=\"Test Accuracy\", marker='o', color='deepskyblue')\nplt.xticks(k_values)\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs. Number of Neighbors (k)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Initialize KNN with k=19\nknn = KNeighborsClassifier(n_neighbors=19)\n\n# Train the model\nknn.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = knn.predict(X_test)\n\n# Evaluate Performance\nprint(\"Classification Report - KNN (k=19):\")\nprint(classification_report(y_test, y_pred))\n\n# Plot Confusion Matrix\nConfusionMatrixDisplay.from_estimator(knn, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - KNN (k=19)\")\nplt.show()\n\nClassification Report - KNN (k=19):\n              precision    recall  f1-score   support\n\n         1.0       0.72      0.83      0.77        65\n         2.0       0.46      0.59      0.52        63\n         3.0       0.45      0.36      0.40        61\n         4.0       0.48      0.33      0.39        46\n\n    accuracy                           0.54       235\n   macro avg       0.53      0.53      0.52       235\nweighted avg       0.53      0.54      0.53       235"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#regression",
    "href": "technical-details/supervised-learning/main.html#regression",
    "title": "Supervised Learning",
    "section": "Regression",
    "text": "Regression\n\ndata = pd.read_csv(\"../../data/processed-data/pitstop_long.csv\")\ndata.drop(columns=['Unnamed: 0'], inplace=True)\ndata.head()\n\n\n\n\n\n\n\n\nYear\nRound\nRaceName\nDriverID\nLap\nStop\nTime\nDuration\n\n\n\n\n0\n0.0\n0.0\nAustralian Grand Prix\nalguersuari\n0.000000\n0.0\n0.0\n0.451650\n\n\n1\n0.0\n0.0\nAustralian Grand Prix\nmichael_schumacher\n0.000000\n0.0\n0.0\n0.420133\n\n\n2\n0.0\n0.0\nAustralian Grand Prix\nwebber\n0.129870\n0.0\n0.0\n0.393351\n\n\n3\n0.0\n0.0\nAustralian Grand Prix\nalonso\n0.142857\n0.0\n0.0\n0.390412\n\n\n4\n0.0\n0.0\nAustralian Grand Prix\nmassa\n0.155844\n0.0\n0.0\n0.400336\n\n\n\n\n\n\n\n\n# Dropping non-numeric columns (RaceName and DriverID) for regression\ndata = data.drop(columns=[\"RaceName\", \"DriverID\"])\n\n# Features and Target\nX = data.drop(columns=[\"Duration\"])  # Features\ny = data[\"Duration\"]  # Target\n\n# Train-Test Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# 1. Linear Regression\nlinear_reg = LinearRegression()\nlinear_reg.fit(X_train, y_train)\ny_pred_lr = linear_reg.predict(X_test)\n\nprint(\"Linear Regression Metrics:\")\nprint(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_lr):.4f}\")\nprint(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred_lr):.4f}\")\nprint(f\"R-squared: {r2_score(y_test, y_pred_lr):.4f}\")\n\n# 2. Support Vector Regression (SVR)\nsvr = make_pipeline(StandardScaler(), SVR(kernel='rbf', C=1.0, epsilon=0.1))\nsvr.fit(X_train, y_train)\ny_pred_svr = svr.predict(X_test)\n\nprint(\"\\nSupport Vector Regression Metrics:\")\nprint(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_svr):.4f}\")\nprint(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred_svr):.4f}\")\nprint(f\"R-squared: {r2_score(y_test, y_pred_svr):.4f}\")\n\n# 3. Polynomial Regression\npoly_model = make_pipeline(PolynomialFeatures(degree=2), LinearRegression())\npoly_model.fit(X_train, y_train)\ny_pred_poly = poly_model.predict(X_test)\n\nprint(\"\\nPolynomial Regression Metrics:\")\nprint(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_poly):.4f}\")\nprint(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred_poly):.4f}\")\nprint(f\"R-squared: {r2_score(y_test, y_pred_poly):.4f}\")\n\n\nrandom_forest = RandomForestRegressor(random_state=42)\nrandom_forest.fit(X_train, y_train)\ny_pred_rf = random_forest.predict(X_test)\nprint(\"\\nRandom Forest Regressor Metrics:\")\nprint(f\"Mean Absolute Error: {mean_absolute_error(y_test, y_pred_rf):.4f}\")\nprint(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred_rf):.4f}\")\nprint(f\"R-squared: {r2_score(y_test, y_pred_rf):.4f}\")\n\nLinear Regression Metrics:\nMean Absolute Error: 0.0699\nMean Squared Error: 0.0136\nR-squared: 0.0240\n\nSupport Vector Regression Metrics:\nMean Absolute Error: 0.0731\nMean Squared Error: 0.0121\nR-squared: 0.1340\n\nPolynomial Regression Metrics:\nMean Absolute Error: 0.0696\nMean Squared Error: 0.0133\nR-squared: 0.0453\n\nRandom Forest Regressor Metrics:\nMean Absolute Error: 0.0355\nMean Squared Error: 0.0054\nR-squared: 0.6126\n\n\n\n# Plotting function for parity plots\ndef plot_parity(y_actual, y_pred, title):\n    plt.figure(figsize=(8, 6))\n    plt.scatter(y_actual, y_pred, alpha=0.6)\n    plt.plot([y_actual.min(), y_actual.max()], [y_actual.min(), y_actual.max()], 'b--', lw=2)\n    plt.xlabel(\"Actual Duration\")\n    plt.ylabel(\"Predicted Duration\")\n    plt.title(title)\n    plt.grid(True)\n    plt.show()\n\n# Visualize parity plots for each algorithm\n# 1. Linear Regression\nplot_parity(y_test, y_pred_lr, \"Parity Plot - Linear Regression\")\n\n# 2. Support Vector Regression (SVR)\nplot_parity(y_test, y_pred_svr, \"Parity Plot - Support Vector Regression\")\n\n# 3. Polynomial Regression\nplot_parity(y_test, y_pred_poly, \"Parity Plot - Polynomial Regression\")\n\n# 4. Random Forest Regressor\nplot_parity(y_test, y_pred_rf, \"Parity Plot - Random Forest Regressor\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Parameter grid for tuning\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 5]\n}\n\n# GridSearchCV for RandomForestRegressor\ngrid_search = GridSearchCV(\n    RandomForestRegressor(random_state=42),\n    param_grid,\n    cv=5,\n    scoring='neg_mean_squared_error',\n    verbose=2,\n    n_jobs=-1\n)\n\n\n\n# Fit the model\ngrid_search.fit(X_train, y_train)\n\n# Best parameters and model\nbest_rf = grid_search.best_estimator_\n\n# Predict on test set\ny_pred_rf = best_rf.predict(X_test)\n\n\n\n# Evaluation Metrics\nmae = mean_absolute_error(y_test, y_pred_rf)\nmse = mean_squared_error(y_test, y_pred_rf)\nr2 = r2_score(y_test, y_pred_rf)\n\nprint(\"\\nOptimized Random Forest Regressor Metrics:\")\nprint(f\"Mean Absolute Error: {mae:.4f}\")\nprint(f\"Mean Squared Error: {mse:.4f}\")\nprint(f\"R-squared: {r2:.4f}\")\n\n\nOptimized Random Forest Regressor Metrics:\nMean Absolute Error: 0.0348\nMean Squared Error: 0.0051\nR-squared: 0.6367\n\n\n\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_pred_rf, alpha=0.7, label=\"Predicted vs Actual\", color=\"steelblue\")\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'b--', lw=2, label=\"Perfect Parity Line\")  # Red dashed line for perfect prediction\nplt.xlabel(\"Actual Duration\")\nplt.ylabel(\"Predicted Duration\")\nplt.title(\"Parity Plot - Optimized Random Forest Regressor\")\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "technical-details/data-cleaning/DataCollection.html",
    "href": "technical-details/data-cleaning/DataCollection.html",
    "title": "Introduction",
    "section": "",
    "text": "In this section, we focus on the methods and sources used to collect data that are relevant to the research questions of this project. The quality of the data collected determines the accuracy of insights and predictions. Poor data collection practices can lead to biased analysis, inaccurate resutls, and ineffective models. Therefore, carful planning and execution of data collection are essential for the success of any data-driven project.\nChallenges in Data Collection We might not always find the data we need from a single source. Some important points to be considered when collecting data: - Data quality: Ensuring the data is accurate, complete, and relevant to the research questions is crucial. - inconistencies: Data form different sources may have different formats, structures, naming conventions, and units of measurement, requring thorough understanding of the data for preprocessing. - Ethics and Privacy: Data collection methods must adheere to ethical guidelines, ensuring that no sensitive or private information is collected or misused. - Data Bias: Collecting data from sources that introduces bias can lead to inaccurate results and models. - Technical Constraints: Issues such as API rate limits, website restrctions, or incomplete data can hinder data collection.\nBy addressing these challenges, it is ensured that the data collected was relevant, accurate, and reliable."
  },
  {
    "objectID": "technical-details/data-cleaning/DataCollection.html#web-scraping",
    "href": "technical-details/data-cleaning/DataCollection.html#web-scraping",
    "title": "Introduction",
    "section": "Web Scraping",
    "text": "Web Scraping"
  },
  {
    "objectID": "technical-details/data-cleaning/instructions.html#web-scraping",
    "href": "technical-details/data-cleaning/instructions.html#web-scraping",
    "title": "Introduction",
    "section": "Web Scraping",
    "text": "Web Scraping"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#web-scraping",
    "href": "technical-details/data-cleaning/main.html#web-scraping",
    "title": "Data Cleaning",
    "section": "Web Scraping",
    "text": "Web Scraping"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#web-scraping-1",
    "href": "technical-details/data-cleaning/main.html#web-scraping-1",
    "title": "Data Cleaning",
    "section": "Web Scraping",
    "text": "Web Scraping"
  },
  {
    "objectID": "technical-details/data-collection/main.html#web-scraping",
    "href": "technical-details/data-collection/main.html#web-scraping",
    "title": "Data Collection",
    "section": "Web Scraping",
    "text": "Web Scraping\nWeb Scraping is an automatic way to collect data from websites. It involves the use of automated scripts or tools to interact with the website’s structure to retrieve information. The data is extracted using selectors like tags, classes, or IDs. While web scraping can be tailored to collect a variety od data types from multiple web pages, one should be aware of the website’s terms of service and ethical scraping practices, and manage rate limits to avoid being blocked.\n\nIn this Project: Using Python libraries like requests and BeautifulSoup, weather information was extracted from infoboxes of each race’s Wikipedia pages.\n\nProcess:\n- The URLs for each race were obtained from the race data collected, these links were used to locate the Wikipedia pages of each race.\n- HTTP requests were sent to the Wikipedia pages to retrieve the HTML content.\n- Parsing the HTML using BeautifulSoup locate the infobox containing the race metadata.\n- Locating and extracting the Weather field from the table.\n1"
  },
  {
    "objectID": "technical-details/data-collection/DataCollection.html",
    "href": "technical-details/data-collection/DataCollection.html",
    "title": "Introduction",
    "section": "",
    "text": "In this section, we focus on the methods and sources used to collect data that are relevant to the research questions of this project. The quality of the data collected determines the accuracy of insights and predictions. Poor data collection practices can lead to biased analysis, inaccurate results, and ineffective models. Therefore, careful planning and execution of data collection is essential for the success of any data-driven project.\nChallenges in Data Collection Some important points to be considered when collecting data are:\n\nData quality: Ensuring the data is accurate, complete, and relevant to the research questions is crucial.\nInconsistencies: Data form different sources may have different formats, structures, naming conventions, and units of measurement, requiring a thorough understanding of the data for pre-processing.\nEthics and Privacy: Data collection methods must adhere to ethical guidelines, ensuring that no sensitive or private information is collected or misused.\nData Bias: Collecting data from sources that introduces bias can lead to inaccurate results and models.\nTechnical Constraints: Issues such as API rate limits, website restrictions, or incomplete data can hinder data collection.\n\nBy addressing these challenges, it is ensured that the data collected was relevant, accurate, and reliable."
  },
  {
    "objectID": "technical-details/data-collection/DataCollection.html#web-scraping",
    "href": "technical-details/data-collection/DataCollection.html#web-scraping",
    "title": "Introduction",
    "section": "Web Scraping",
    "text": "Web Scraping\nWeb Scraping is an automatic way to collect data from websites. It involves the use of automated scripts or tools to interact with the website’s structure to retrieve information. The data is extracted using selectors like tags, classes, or IDs. While web scraping can be tailored to collect a variety od data types from multiple web pages, one should be aware of the website’s terms of service and ethical scraping practices, and manage rate limits to avoid being blocked.\n\nIn this Project: Using Python libraries like requests and BeautifulSoup, weather information was extracted from infoboxes of each race’s Wikipedia pages.\n\nProcess:\n- The URLs for each race were obtained from the race data collected, these links were used to locate the Wikipedia pages of each race.\n- HTTP requests were sent to the Wikipedia pages to retrieve the HTML content.\n- Parsing the HTML using BeautifulSoup locate the infobox containing the race metadata.\n- Locating and extracting the Weather field from the table.\n1"
  },
  {
    "objectID": "technical-details/data-collection/main.html#apis",
    "href": "technical-details/data-collection/main.html#apis",
    "title": "Data Collection",
    "section": "APIs",
    "text": "APIs\nAPIs (Application Programming Interfaces) are mechanisms that enable two software components to communicate with each other using a set of definitions and protocols. APIs enable developers to access data or functionality from a system without having to know the underlying implementation details, making it easier to integrate data from multiple sources. They act as intermediaries, providing a structured way for programs to request and retrieve information or services.2\nHow do APIs Work?3  APIs facilitate communication between applications, systems, or devices through a structured request-response cycle\n\nAPI Client: The process begins with an API client, which sends a request to the API server, which can be triggered by user interaction or external events.\nAPI Request: An API request typically contains the following components: \n\nEndpoint: URL that provides access to a specific resource.\nMethod: Indicates that action to be performed on the resource.\nParameters: Variables that are passed along with the request to customize the response.\nHeaders: Key-value pairs that provide additional details about the request, such as authentication tokens or the content format.\nRequest Body: Includes actual data required for operations like creating, updating, or deleting resources.\n\nAPI Server: Receives the request and performs actions such as authenticating the client, validating the input, and processing the request by retrieving or updating the requested data.\nAPI Response: The API server returns a response to the client, this typically includes:\n\n\nStatus Code: A numerical code indicating the result of the request (e.g., 200 for success, 201 for resource creation, or 404 for resource not found).\nHeaders: Additional metadata about the response.\nResponse Body: The data requested by the client, or an error message.\n\n\nAPI Architectural Styles:4 \n\nREST (Representational State of Resource): A widely used style for data exchange over the internet. In RESTful APIs, resources are accessed through endpoints, and standard HTTP methods such as GET, POST, PUT, and DELETE are used to perform operations on these resources.\nSOAP (Simple Object Access Protocol): Protocol that uses XML to facilitate the transfer of highly structured messages between client and sever. While it provides features for security and reliability, it can be slower compared to other architectural styles.\nGraphQL: An open-source query language designed to allow clients fetch only the data they need via a single API endpoint. This eliminates the need for multiple requests, making it valuable for applications that operate over slower or less reliable network connections.\n\nIn this Project: \nThe Ergast Developer API was used to collect race, driver standings, circuit information, and pitstop data. “The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes”5."
  },
  {
    "objectID": "technical-details/data-collection/main.html#requred-libraries",
    "href": "technical-details/data-collection/main.html#requred-libraries",
    "title": "Data Collection",
    "section": "Requred Libraries",
    "text": "Requred Libraries\n\nimport requests\nimport pandas as pd\nimport numpy as np\nimport json\nimport os\nimport re\nfrom datetime import datetime\nimport time\nfrom bs4 import BeautifulSoup\nimport csv\n\n\nRace Information for 2000-2023 Seasons\n\ndef get_race_results(url, year, offset, limit=1000):\n    full_url = f\"{url}/{year}/results.json?limit={limit}&offset={offset}\"\n    result = requests.get(full_url)\n    return result.json()\n\n\n# Testing for 2023 season\nseason_2023_json = get_race_results(url='http://ergast.com/api/f1', year=2023, offset=0)\n\n# Save the data to a JSON file\nwith open('../../data/raw-data/race_data_2023.json', 'w') as outfile:\n    json.dump(season_2023_json, outfile)\n\n\n# collecting data from 2000 to 2022\n\n# function to loop through years and fetch the results\ndef race_data(start_year, end_year, output_dr, url):\n\n    for year in range(start_year, end_year + 1):\n        race_data = get_race_results(url, year, offset=0)\n        # save the output \n        output_file = os.path.join(output_dr, f\"race_data_{year}.json\")\n        with open(output_file, 'w') as f:\n            json.dump(race_data, f)\n\n\n# call race_data()\nrace_data(\n    start_year = 2000,\n    end_year = 2009,\n    output_dr = \"../../data/raw-data\",\n    url = 'http://ergast.com/api/f1'\n)\n\n\n\nDriver Standings for 2000-2023 Seasons\n\ndef driverstanding_info(url, season):\n    full_url = f\"{url}/{season}/driverStandings.json\"\n    response = requests.get(full_url)\n    return response.json()\n\n# Function to fetch and save all driver standings for the given seasons\ndef driverstandings_info(start_year, end_year, output_file, url=\"http://ergast.com/api/f1\"):\n    driver_standings_data = {}\n    \n    for year in range(start_year, end_year + 1):\n        data = driverstanding_info(url, year)\n        driver_standings_data[year] = data\n    \n    # Save to output file\n    with open(output_file, 'w') as outfile:\n        json.dump(driver_standings_data, outfile)\n\n# Call the function for seasons 2000–2023\ndriverstandings_info(\n    start_year=2000,\n    end_year=2023,\n    output_file=\"../../data/raw-data/driver_standings/driver_standings_2000_2023.json\"\n)\n\n\n\nCircuit Information for 2000-2023 Seasons\n\ndef circuit_info(output_file, url):\n    results = requests.get(url)\n    \n    # save to output file\n    with open(output_file, 'w') as f:\n        json.dump(results.json(), f)\n        \n\n\ncircuit_info(output_file='../../data/raw-data/circuit_data.json', url = \"http://ergast.com/api/f1/circuits.json\")\n\n\n\nNews of Top 10 drivers in 2024 season (so far)\n\nUsing News-API\nResources: https://jfh.georgetown.domains/centralized-lecture-content/content/data-science/data-collection/share/API-newapi/news-api.html\n\n\nSet Credentials\n\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\nAPI_KEY='86d4dac5a4864ece92da90bc31277e53'\n\n\ndef news_data(topic, API_KEY, total_requests=1, verbose=True):\n    baseURL = \"https://newsapi.org/v2/everything?\"\n\n    # API parameters\n    URLpost = {\n        'apiKey': API_KEY,\n        'q': '+'+topic,\n        'sotBy': 'relevancy',\n        'pageSize': 100,\n        'page': 1\n    }\n    # last name of the drives to avoid spaces in the file names\n    file_name = topic.split()[-1]\n    all_articles = []\n\n    # make an API request \n    for request_num in range(total_requests):\n        response = requests.get(baseURL, params=URLpost)\n        response_data = response.json()\n\n        articles = response_data.get('articles', [])\n        all_articles.extend(articles)\n\n        URLpost['page'] += 1\n\n\n    # output file path\n    output_dr = \"../../data/raw-data/News_Drivers\"\n    output_file = os.path.join(output_dr, f\"{file_name}_raw_text.json\")\n\n    # save to output file\n    with open(output_file, 'w') as f:\n        json.dump(all_articles, f, indent=4)\n    \n    return all_articles\n\n\n\nTop 10 Drivers as of Round 22 (Las Vegas Grand Prix)\n\nMax Verstappen\nLando Norris\nCharles Leclerc\nOscar Piastri\nCarlos Sainz\nGeorge Russell\nLewis Hamilton\nSergio Perez\nFernando Alonso\nNico Hulkenberg\n\n\n# testing \ntext_data = news_data('Max Verstappen', API_KEY, total_requests=1, verbose=True)\n\n\ntext_data = news_data('Lando Norris', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Charles Leclerc', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Oscar Piastri', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Carlos Sainz', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('George Russell', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Lewis Hamilton', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Sergio Perez', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Fernando Alonso', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Nico Hulkenberg', API_KEY, total_requests=1, verbose=True)\n\n\n\n\nFetch Weather data on the day of the race\nThe weather data will be fetched from the wiki page of each race.\n\n# the url for each race is in the race data collected using ergast API\nrace_df = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")\n\n\nrace_data = race_df[['season', 'raceName', 'url']]\n\n\nrace_data = race_data.drop_duplicates()\n\n\nrace_data.head()\n\n\n\n\n\n\n\n\nseason\nraceName\nurl\n\n\n\n\n0\n2010\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\n\n\n24\n2010\nAustralian Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Australian_G...\n\n\n48\n2010\nMalaysian Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Malaysian_Gr...\n\n\n72\n2010\nChinese Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Chinese_Gran...\n\n\n96\n2010\nSpanish Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Spanish_Gran...\n\n\n\n\n\n\n\n\n\ndef get_weather_from_wikipedia(url):\n    response = requests.get(url)\n    bs = BeautifulSoup(response.text, 'html.parser')  \n    \n    # locate the infobox table\n    table = bs.find('table', {'class': 'infobox infobox-table vevent'})\n    if not table:\n        print(f\"No infobox found on the page: {url}\")\n        return \"Not Available\"\n    \n    # search for the \"Weather\" row in the table\n    for row in table.find_all('tr'):\n        # find the header cell with class 'infobox-label'\n        header = row.find('th', {'class': 'infobox-label'})  \n        # check if it contains \"Weather\"\n        if header and 'Weather' in header.text:  # Check if it contains \"Weather\"\n            # find the corresponding data cell with class 'infobox-data'\n            data = row.find('td', {'class': 'infobox-data'})  \n            \n            if data:\n                return data.text.strip()  \n    \n    \nrace_data['weather'] = None\n\n# fetch weather information for each URL\nfor index, row in race_data.iterrows():\n    url = row['url']\n    # for debuggin purpose\n    print(f\"Fetching weather for: {url}\")\n    \n    # get the weather information\n    weather = get_weather_from_wikipedia(url)\n    \n    # update the weather column\n    race_data.at[index, 'weather'] = weather\n\n# save to output file\noutput_csv = \"../../data/raw-data/weather/race_data_with_weather.csv\"\nrace_data.to_csv(output_csv, index=False)\n\nprint(f\"Updated race data saved to: {output_csv}\")\n\n\nos.makedirs('cache', exist_ok=True)\nfastf1.Cache.enable_cache('cache')\n\n\ntrack_data = []\n\ndef extract_track_features(year, race_name):\n    session = fastf1.get_session(year, race_name, 'Q') \n    session.load()\n\n    # Get the fastest lap\n    fastest_lap = session.laps.pick_fastest()\n    telemetry = fastest_lap.get_telemetry()\n\n    # Track Length\n    track_length = telemetry['Distance'].iloc[-1]  # Distance of the fastest lap\n\n    # Max Speed\n    max_speed = telemetry['Speed'].max()\n\n    # Average Speed\n    avg_speed = track_length / fastest_lap['LapTime'].total_seconds()\n\n    # Percentage of Full Throttle\n    full_throttle = telemetry[telemetry['Throttle'] &gt;= 95]\n    perc_full_throttle = (len(full_throttle) / len(telemetry)) * 100\n\n    # Number of Corners\n    telemetry['is_corner'] = telemetry['Speed'] &lt; 100\n    num_corners = (telemetry['is_corner'] & ~telemetry['is_corner'].shift(1, fill_value=False)).sum()\n\n    # Number of Straights\n    telemetry['is_straight'] = telemetry['Speed'] &gt; 150\n    num_straights = (telemetry['is_straight'] & ~telemetry['is_straight'].shift(1, fill_value=False)).sum()\n\n    return {\n        \"Year\": year,\n        \"Grand Prix\": race_name,\n        \"Track Length (m)\": track_length,\n        \"Max Speed (km/h)\": max_speed,\n        \"Full Throttle (%)\": perc_full_throttle,\n        \"Number of Corners\": num_corners,\n        \"Number of Straights\": num_straights\n    }\n\nyear = 2023\nschedule = fastf1.get_event_schedule(year)\n\nfor _, event in schedule.iterrows():  \n    if not pd.isna(event['Session1']):  \n        try:\n            track_features = extract_track_features(year, event['EventName'])\n            track_data.append(track_features)\n        except Exception as e:\n            print(f\"Failed for {event['EventName']} in {year}: {e}\")\n\ndf_tracks = pd.DataFrame(track_data)\n\n\n\n# merging all racetrack features into a single csv\nfolder_path = \"../../data/raw-data/circuit_data/\"\n\ndataframes = []\n\nfor file_name in os.listdir(folder_path):\n    if file_name.endswith('.csv'): \n        file_path = os.path.join(folder_path, file_name)\n        df = pd.read_csv(file_path)\n        dataframes.append(df)\n\nmerged_df = pd.concat(dataframes, ignore_index=True)\n\noutput_file = \"../../data/raw-data/circuit_data/merged_circuit_features.csv\"\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\nmerged_df.to_csv(output_file, index=False)\n\n\n\nPitstop data\n\n# data available from 2011\n\n# Function to fetch pitstop data for a specific race\ndef get_pitstop_data(year, round_number):\n    url = f\"http://ergast.com/api/f1/{year}/{round_number}/pitstops.json?limit=1000\"\n    response = requests.get(url)\n    \n    if response.status_code == 200 and response.text.strip():\n        try:\n            return response.json()\n        except Exception as e:\n            print(f\"Error parsing JSON for {year} Round {round_number}: {e}\")\n            return None\n    else:\n        print(f\"Failed to fetch data for {year} Round {round_number}: {response.status_code}\")\n        return None\n\n# Function to fetch race schedule\ndef get_race_schedule(year):\n    url = f\"http://ergast.com/api/f1/{year}.json\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json().get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n    else:\n        print(f\"Failed to fetch schedule for {year}: {response.status_code}\")\n        return []\n\n# Function to extract and save pitstop data to CSV\ndef fetch_and_save_pitstop_data(start_year, end_year, output_csv):\n    # Create the CSV file and write the header\n    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n        fieldnames = [\"Year\", \"Round\", \"RaceName\", \"DriverID\", \"Lap\", \"Stop\", \"Time\", \"Duration\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        # Loop through years and races\n        for year in range(start_year, end_year + 1):\n            print(f\"Fetching data for year: {year}\")\n            races = get_race_schedule(year)\n            \n            for race in races:\n                round_number = race.get(\"round\")\n                race_name = race.get(\"raceName\")\n                print(f\"Processing {race_name} (Round {round_number}) in {year}\")\n\n                # Fetch pitstop data for this race\n                pitstop_data = get_pitstop_data(year, round_number)\n                if pitstop_data:\n                    races_list = pitstop_data.get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n                    \n                    # Ensure there is race data\n                    if races_list:\n                        pitstops = races_list[0].get(\"PitStops\", [])\n                        \n                        # Write each pitstop to the CSV\n                        for pitstop in pitstops:\n                            writer.writerow({\n                                \"Year\": year,\n                                \"Round\": round_number,\n                                \"RaceName\": race_name,\n                                \"DriverID\": pitstop.get(\"driverId\"),\n                                \"Lap\": pitstop.get(\"lap\"),\n                                \"Stop\": pitstop.get(\"stop\"),\n                                \"Time\": pitstop.get(\"time\"),\n                                \"Duration\": pitstop.get(\"duration\")\n                            })\n                    else:\n                        print(f\"No race data available for {race_name} in {year}\")\n\n\n\noutput_csv = \"../../data/raw-data/pitstop_data.csv\"\nfetch_and_save_pitstop_data(\n    start_year=2000,\n    end_year=2023,\n    output_csv=output_csv\n)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#required-libraries",
    "href": "technical-details/data-collection/main.html#required-libraries",
    "title": "Data Collection",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n\nCode\n# for sending HTTP requests\nimport requests\n# for data manipulations \nimport pandas as pd\n# for numerical computations and array manipulations\nimport numpy as np\n# for parsing and manipulating JSON data\nimport json\n# for interacting with the operating system\nimport os\n# for pattern matching, cleaning and extracting text\nimport re\n# for working with dates and times\nfrom datetime import datetime\nimport time\n# for parsing and extracting data from HTML and XML documents\nfrom bs4 import BeautifulSoup\n# for reading and writing to csv files\nimport csv\n# for car telemetry and results\nimport fastf1"
  },
  {
    "objectID": "technical-details/data-collection/main.html#race-information",
    "href": "technical-details/data-collection/main.html#race-information",
    "title": "Data Collection",
    "section": "Race Information",
    "text": "Race Information\nImportance: Contains information about the season, round, date, grand prix name, location, results,wikipedia url of the race, driver and constructor details.\n\nSeason: The year of the race.\nRound: The number of the race in the season.\nGrand Prix Name: The official name of the race.\nResults: Includes the finishing poistion and points earned.\nDriver Details: Provides information about the driver’s name, ID, nationality, and date of birth.\nConstructor Details: Constructor refers to the team that builds and maintains the cars. It contains information about the constructor’s name, ID, and nationality.\n\nSource: ERGAST API\n\n\nCode\n# funtion to get race results for a specifi year\ndef get_race_results(url, year, offset, limit=1000):\n    full_url = f\"{url}/{year}/results.json?limit={limit}&offset={offset}\"\n    # GET request\n    result = requests.get(full_url)\n    # output = JSON object\n    return result.json()\n\n\n\n\nCode\n# Testing for 2023 season\nseason_2023_json = get_race_results(url='http://ergast.com/api/f1', year=2023, offset=0)\n\n# Save the data to a JSON file\nwith open('../../data/raw-data/race_data_2023.json', 'w') as outfile:\n    json.dump(season_2023_json, outfile)\n\n\n\n\nCode\n# collecting data from 2000 to 2022\n# function to loop through years and fetch the results\ndef race_data(start_year, end_year, output_dr, url):\n\n    for year in range(start_year, end_year + 1):\n        # results for the current year\n        race_data = get_race_results(url, year, offset=0)\n        # save the output \n        output_file = os.path.join(output_dr, f\"race_data_{year}.json\")\n        # save the data to a JSON file\n        with open(output_file, 'w') as f:\n            json.dump(race_data, f)\n\n\n\n\nCode\n# call race_data()\nrace_data(\n    start_year = 2000,\n    end_year = 2009,\n    output_dr = \"../../data/raw-data\",\n    url = 'http://ergast.com/api/f1'\n)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#driver-standings",
    "href": "technical-details/data-collection/main.html#driver-standings",
    "title": "Data Collection",
    "section": "Driver Standings",
    "text": "Driver Standings\nAfter each round, drivers earn points based on their final position. These points are added to their overall tally, and the driver with the most points at the end of the season wins the World Driver’s Championship (WDC).\nImportance: Contains total points earned by each driver in every season from 2000 to 2023. It is crucial for identifying trends in driver performance over the years.\nSource: ERGAST API\n\n\nCode\n# function to fetch driver standings for a specific year\ndef driverstanding_info(url, season):\n    # construct the URL \n    full_url = f\"{url}/{season}/driverStandings.json\"\n    # GET request\n    response = requests.get(full_url)\n    return response.json()\n\n# Function to fetch and save all driver standings for the given seasons\ndef driverstandings_info(start_year, end_year, output_file, url=\"http://ergast.com/api/f1\"):\n    # for storing driver standings data\n    driver_standings_data = {}\n    \n    # loop through each year \n    for year in range(start_year, end_year + 1):\n        # results for the current year\n        data = driverstanding_info(url, year)\n        # add the data with year as key\n        driver_standings_data[year] = data\n    \n    # Save to output file\n    with open(output_file, 'w') as outfile:\n        json.dump(driver_standings_data, outfile)\n\n# Call the function for seasons 2000–2023\ndriverstandings_info(\n    start_year=2000,\n    end_year=2023,\n    output_file=\"../../data/raw-data/driver_standings/driver_standings_2000_2023.json\"\n)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#circuit-information",
    "href": "technical-details/data-collection/main.html#circuit-information",
    "title": "Data Collection",
    "section": "Circuit Information",
    "text": "Circuit Information\nThe race tracks are referred to as circuits.\nImportance: This data includes the circuit name, locality, country as well as its longitude and latitude. These values can be used for collecting weather information on the race day.\nSource: ERGAST API\n\n\nCode\ndef circuit_info(output_file, url):\n    results = requests.get(url)\n    \n    # save to output file\n    with open(output_file, 'w') as f:\n        json.dump(results.json(), f)\n        \n\n\n\n\nCode\ncircuit_info(output_file='../../data/raw-data/circuit_data.json', url = \"http://ergast.com/api/f1/circuits.json\")"
  },
  {
    "objectID": "technical-details/data-collection/main.html#news-of-top-10-drivers",
    "href": "technical-details/data-collection/main.html#news-of-top-10-drivers",
    "title": "Data Collection",
    "section": "News of Top 10 Drivers",
    "text": "News of Top 10 Drivers\nStandings in 2024 season as on 11-24-2024\nSilly Season in F1 refers to the period of speculation, rumors, and announcements surrounding driver lineups for the next season. This period typically begins during the latter half of the season, as drivers, teams, and sponsors negotiate deals for the future. Headlines during the silly season often speculate on whether drivers will extend their contract, switch teams, or retire from the sport, creating a buzz that fuels media interest.\nImportance: Analyzing news coverage about drivers can provide insights into their career trajectories, and potential moves in the upcoming season.\nSource: NEWS API\nResources: NEWS-API DSAN 5000 Lecture Content\n\n\nCode\nbaseURL = \"https://newsapi.org/v2/everything?\"\ntotal_requests=2\nverbose=True\n\nAPI_KEY='86d4dac5a4864ece92da90bc31277e53'\n\n\n\n\nCode\n# function to fetch news articles for a given topic\ndef news_data(topic, API_KEY, total_requests=1, verbose=True):\n    baseURL = \"https://newsapi.org/v2/everything?\"\n\n    # API parameters\n    URLpost = {\n        'apiKey': API_KEY,      # API key\n        'q': '+'+topic,         # topic\n        'sotBy': 'relevancy',   # sort by relevance\n        'pageSize': 100,        # maximum articles per pages = 100\n        'page': 1               # start with page 1\n    }\n    # last name of the drives to avoid spaces in the file names\n    file_name = topic.split()[-1]\n    # initialize a list to store all articles\n    all_articles = []\n\n    # loop through the number of API requests\n    for request_num in range(total_requests):\n        # make the API request\n        response = requests.get(baseURL, params=URLpost)\n        response_data = response.json()\n        # extract artivles from the response\n        articles = response_data.get('articles', [])\n        all_articles.extend(articles)\n\n        # increment the page number for next request\n        URLpost['page'] += 1\n\n\n    # output file path\n    output_dr = \"../../data/raw-data/News_Drivers\"\n    output_file = os.path.join(output_dr, f\"{file_name}_raw_text.json\")\n\n    # save to output file\n    with open(output_file, 'w') as f:\n        json.dump(all_articles, f, indent=4)\n    \n    return all_articles\n\n\n\nTop 10 Drivers as of Round 22 (Las Vegas Grand Prix)\n\nMax Verstappen\nLando Norris\nCharles Leclerc\nOscar Piastri\nCarlos Sainz\nGeorge Russell\nLewis Hamilton\nSergio Perez\nFernando Alonso\nNico Hulkenberg\n\n\n\nCode\n# testing \ntext_data = news_data('Max Verstappen', API_KEY, total_requests=1, verbose=True)\n\n\n\n\nCode\ntext_data = news_data('Lando Norris', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Charles Leclerc', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Oscar Piastri', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Carlos Sainz', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('George Russell', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Lewis Hamilton', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Sergio Perez', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Fernando Alonso', API_KEY, total_requests=1, verbose=True)\ntext_data = news_data('Nico Hulkenberg', API_KEY, total_requests=1, verbose=True)"
  },
  {
    "objectID": "technical-details/data-collection/DataCollection.html#apis",
    "href": "technical-details/data-collection/DataCollection.html#apis",
    "title": "Introduction",
    "section": "APIs",
    "text": "APIs\nAPIs (Application Programming Interfaces) are mechanisms that enable two software components to communicate with each other using a set of definitions and protocols. APIs enable developers to access data or functionality from a system without having to know the underlying implementation details, making it easier to integrate data from multiple sources. They act as intermediaries, providing a structured way for programs to request and retrieve information or services.2\nHow do APIs Work?3  APIs facilitate communication between applications, systems, or devices through a structured request-response cycle\n\nAPI Client: The process begins with an API client, which sends a request to the API server, which can be triggered by user interaction or external events.\nAPI Request: An API request typically contains the following components: \n\nEndpoint: URL that provides access to a specific resource.\nMethod: Indicates that action to be performed on the resource.\nParameters: Variables that are passed along with the request to customize the response.\nHeaders: Key-value pairs that provide additional details about the request, such as authentication tokens or the content format.\nRequest Body: Includes actual data required for operations like creating, updating, or deleting resources.\n\nAPI Server: Receives the request and performs actions such as authenticating the client, validating the input, and processing the request by retrieving or updating the requested data.\nAPI Response: The API server returns a response to the client, this typically includes:\n\n\nStatus Code: A numerical code indicating the result of the request (e.g., 200 for success, 201 for resource creation, or 404 for resource not found).\nHeaders: Additional metadata about the response.\nResponse Body: The data requested by the client, or an error message.\n\n\nAPI Architectural Styles:4 \n\nREST (Representational State of Resource): A widely used style for data exchange over the internet. In RESTful APIs, resources are accessed through endpoints, and standard HTTP methods such as GET, POST, PUT, and DELETE are used to perform operations on these resources.\nSOAP (Simple Object Access Protocol): Protocol that uses XML to facilitate the transfer of highly structured messages between client and sever. While it provides features for security and reliability, it can be slower compared to other architectural styles.\nGraphQL: An open-source query language designed to allow clients fetch only the data they need via a single API endpoint. This eliminates the need for multiple requests, making it valuable for applications that operate over slower or less reliable network connections.\n\nIn this Project: \nThe Ergast Developer API was used to collect race, driver standings, circuit information, and pitstop data. “The Ergast Developer API is an experimental web service which provides a historical record of motor racing data for non-commercial purposes”5."
  },
  {
    "objectID": "technical-details/data-collection/DataCollection.html#footnotes",
    "href": "technical-details/data-collection/DataCollection.html#footnotes",
    "title": "Introduction",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n2010 Bahrain Grand Prix↩︎\nWhat are APIs↩︎\nHow do APIs work↩︎\nTypes of APIs↩︎\nErgast Developer API↩︎"
  },
  {
    "objectID": "technical-details/data-collection/main.html#weather-data",
    "href": "technical-details/data-collection/main.html#weather-data",
    "title": "Data Collection",
    "section": "Weather Data",
    "text": "Weather Data\nImportance: Weather conditions play a crucial role in race strategy, tire choices, and driver performance.\nSource: Wikipedia\n\n\nCode\n# the url for each race is in the race data collected using ergast API\nrace_df = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")\n\n\n\n\nCode\nrace_data = race_df[['season', 'raceName', 'url']]\n\n\n\n\nCode\nrace_data = race_data.drop_duplicates()\n\n\n\n\nCode\nrace_data.head()\n\n\n\n\n\n\n\n\n\nseason\nraceName\nurl\n\n\n\n\n0\n2010\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\n\n\n24\n2010\nAustralian Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Australian_G...\n\n\n48\n2010\nMalaysian Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Malaysian_Gr...\n\n\n72\n2010\nChinese Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Chinese_Gran...\n\n\n96\n2010\nSpanish Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Spanish_Gran...\n\n\n\n\n\n\n\n\n\nCode\n\ndef get_weather_from_wikipedia(url):\n    # GET request\n    response = requests.get(url)\n    # parse the HTML content\n    bs = BeautifulSoup(response.text, 'html.parser')  \n    \n    # locate the infobox table\n    table = bs.find('table', {'class': 'infobox infobox-table vevent'})\n    if not table:\n        print(f\"No infobox found on the page: {url}\")\n        return \"Not Available\"\n    \n    # iterate through each row for the \"Weather\" field \n    for row in table.find_all('tr'):\n        # find the header cell with class 'infobox-label'\n        header = row.find('th', {'class': 'infobox-label'})  \n        # check if it contains \"Weather\"\n        if header and 'Weather' in header.text: \n            # find the corresponding data cell with class 'infobox-data'\n            data = row.find('td', {'class': 'infobox-data'})  \n            # remove any unnecessary spaces\n            if data:\n                return data.text.strip()  \n    \n# column in the dataframe to store weather information   \nrace_data['weather'] = None\n\n# fetch weather information for each URL\nfor index, row in race_data.iterrows():\n    url = row['url']\n    # for debugging purpose\n    #print(f\"Fetching weather for: {url}\")\n    \n    # get the weather information\n    weather = get_weather_from_wikipedia(url)\n    \n    # update the weather column\n    race_data.at[index, 'weather'] = weather\n\n# save to output file\noutput_csv = \"../../data/raw-data/weather/race_data_with_weather.csv\"\nrace_data.to_csv(output_csv, index=False)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#circuit-features",
    "href": "technical-details/data-collection/main.html#circuit-features",
    "title": "Data Collection",
    "section": "Circuit Features",
    "text": "Circuit Features\nAround the F1 season, circuits vary widely in their features—some are known for tight, technical corners, others for long, high-speed straights, and a few for their narrow and challenging layouts. These unique characteristics influence car and driver performance significantly, with certain drivers or car designs excelling on specific track types.\nImportance: Analyzing racetrack features is crucial for understanding how different teams and drivers perform under varying conditions. This information can be used to classify tracks, which can further be studied to identify patterns and trends in race results.\nSource: fastf1\n\n\nCode\n#os.makedirs('cache', exist_ok=True)\n#fastf1.Cache.enable_cache('cache')\n\n\n\n\nCode\n# initialize an empty list\ntrack_data = []\n\n# function to fetch data for specific year and race\ndef extract_track_features(year, race_name):\n    # load session data for the qualifying session\n    session = fastf1.get_session(year, race_name, 'Q') \n    session.load()\n\n    # fastest lap\n    fastest_lap = session.laps.pick_fastest()\n    telemetry = fastest_lap.get_telemetry()\n\n    # track Length\n    track_length = telemetry['Distance'].iloc[-1]  # Distance of the fastest lap\n\n    # max Speed\n    max_speed = telemetry['Speed'].max()\n\n    # average Speed\n    avg_speed = track_length / fastest_lap['LapTime'].total_seconds()\n\n    # percentage of Full Throttle (throttle &gt;= 95%)\n    full_throttle = telemetry[telemetry['Throttle'] &gt;= 95]\n    perc_full_throttle = (len(full_throttle) / len(telemetry)) * 100\n\n    # calcute the number of corners based on telemetry speed (&lt; 100 km/h)\n    telemetry['is_corner'] = telemetry['Speed'] &lt; 100\n    num_corners = (telemetry['is_corner'] & ~telemetry['is_corner'].shift(1, fill_value=False)).sum()\n\n    # calculate the number of straights based on telemetry speed (&gt; 150 km/h)\n    telemetry['is_straight'] = telemetry['Speed'] &gt; 150\n    num_straights = (telemetry['is_straight'] & ~telemetry['is_straight'].shift(1, fill_value=False)).sum()\n\n    return {\n        \"Year\": year,\n        \"Grand Prix\": race_name,\n        \"Track Length (m)\": track_length,\n        \"Max Speed (km/h)\": max_speed,\n        \"Full Throttle (%)\": perc_full_throttle,\n        \"Number of Corners\": num_corners,\n        \"Number of Straights\": num_straights\n    }\n\n# for test\n# data was extracted for seasons: 2018 - 2023\n# data is available from 2018 season onwards in fastf1\nyear = 2023\nschedule = fastf1.get_event_schedule(year)\n\n# iterate through all races in the schedule\nfor _, event in schedule.iterrows():  \n    if not pd.isna(event['Session1']):  \n        try:\n            track_features = extract_track_features(year, event['EventName'])\n            track_data.append(track_features)\n        except Exception as e:\n            print(f\"Failed for {event['EventName']} in {year}: {e}\")\n\ndf_tracks = pd.DataFrame(track_data)\n\n\n\n\n\nCode\n# merging all racetrack features into a single csv\nfolder_path = \"../../data/raw-data/circuit_data/\"\n\ndataframes = []\n\nfor file_name in os.listdir(folder_path):\n    if file_name.endswith('.csv'): \n        file_path = os.path.join(folder_path, file_name)\n        df = pd.read_csv(file_path)\n        dataframes.append(df)\n\nmerged_df = pd.concat(dataframes, ignore_index=True)\n\noutput_file = \"../../data/raw-data/circuit_data/merged_circuit_features.csv\"\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\nmerged_df.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-collection/main.html#pit-stop-data",
    "href": "technical-details/data-collection/main.html#pit-stop-data",
    "title": "Data Collection",
    "section": "Pit stop data",
    "text": "Pit stop data\nPit stop is when the car pulls in the pit lane, a designated area, for a quick maintenance, change of tires, mechanical repairs or any other actions necessary during the race. The teams have to strategically decide when to make a pit stop in order to gain a competitive advantage.\nImportance: The speed and precision of pit crews play a crucial role in minimizing the time drivers lose during a pit stop.\n\nLap: The specific lap during which the pit stop was made.\nStop: Whether it is the first, second, or subsequent stop for the driver.\nDuration: Time spent in the pit lane.\n\nSource: ERGAST API\n\n\nCode\n# data available from 2011\n\n# function to fetch pitstop data for a specific race\ndef get_pitstop_data(year, round_number):\n    # construct the url \n    url = f\"http://ergast.com/api/f1/{year}/{round_number}/pitstops.json?limit=1000\"\n    response = requests.get(url)\n    \n    # check if the request was successful\n    if response.status_code == 200 and response.text.strip():\n        try:\n            # parse and return JSON data\n            return response.json()\n        except Exception as e:\n            print(f\"Error parsing JSON for {year} Round {round_number}: {e}\")\n            return None\n    # in case of error - for tracking failures\n    else:\n        print(f\"Failed to fetch data for {year} Round {round_number}: {response.status_code}\")\n        return None\n\n# function to fetch race schedule for a specific season\ndef get_race_schedule(year):\n    url = f\"http://ergast.com/api/f1/{year}.json\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        return response.json().get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n    else:\n        print(f\"Failed to fetch schedule for {year}: {response.status_code}\")\n        return []\n\n# function to extract and save pitstop data to CSV\ndef fetch_and_save_pitstop_data(start_year, end_year, output_csv):\n    # create the CSV file and write the header\n    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n        fieldnames = [\"Year\", \"Round\", \"RaceName\", \"DriverID\", \"Lap\", \"Stop\", \"Time\", \"Duration\"]\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        # loop through years and races\n        for year in range(start_year, end_year + 1):\n            print(f\"Fetching data for year: {year}\")\n            races = get_race_schedule(year)\n            \n            # loop through each race\n            for race in races:\n                round_number = race.get(\"round\")\n                race_name = race.get(\"raceName\")\n\n                # fetch pitstop data for the current race\n                pitstop_data = get_pitstop_data(year, round_number)\n                if pitstop_data:\n                    # extract the race list from the response\n                    races_list = pitstop_data.get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n                    \n                    # check if the response contains the data\n                    if races_list:\n                        # extract pitstops for the race\n                        pitstops = races_list[0].get(\"PitStops\", [])\n                        \n                        # write each pitstop to the CSV\n                        for pitstop in pitstops:\n                            writer.writerow({\n                                \"Year\": year,\n                                \"Round\": round_number,\n                                \"RaceName\": race_name,\n                                \"DriverID\": pitstop.get(\"driverId\"),\n                                \"Lap\": pitstop.get(\"lap\"),\n                                \"Stop\": pitstop.get(\"stop\"),\n                                \"Time\": pitstop.get(\"time\"),\n                                \"Duration\": pitstop.get(\"duration\")\n                            })\n                    else:\n                        print(f\"No race data available for {race_name} in {year}\")\n\n\n\n\nCode\n# write to output file\noutput_csv = \"../../data/raw-data/pitstop_data.csv\"\n# running the function for all seasons: 2000 - 2023\n# but the data is available only from 2011 season \nfetch_and_save_pitstop_data(\n    start_year=2000, \n    end_year=2023,\n    output_csv=output_csv\n)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#required-libraries",
    "href": "technical-details/data-cleaning/main.html#required-libraries",
    "title": "Data Cleaning",
    "section": "Required Libraries",
    "text": "Required Libraries\n\nimport os\nimport json\nimport re\nimport pandas as pd\nimport numpy as np\nimport nltk\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')\nfrom sklearn.preprocessing import StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/nandinikodali/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#news-data",
    "href": "technical-details/data-cleaning/main.html#news-data",
    "title": "Data Cleaning",
    "section": "News Data",
    "text": "News Data\n\n# load English stop words\nstop_words = set(stopwords.words('english'))\n\ndef string_cleaner(input_string):\n    try:\n        # remove unwanted punctuation\n        out = re.sub(r\"[.,;@#?!&$-]+\", \" \", input_string) \n        \n        # remove escape characters \n        out = re.sub(r'\\\\u[0-9a-fA-F]{4}', '', out)\n        \n        # remove extra whitespace\n        out = re.sub(r'\\s+', ' ', out).strip()\n        \n        # convert to lowercase\n        out = out.lower()\n        \n        # tokenization\n        words = out.split()\n        # remove stop words and words of length &lt;= 3\n        words = [word for word in words if len(word) &gt; 3 and word not in stop_words]\n        \n        # join words back into a single string\n        out = ' '.join(words)\n        \n    except Exception as e:\n        print(f\"Error cleaning string: {e}\")\n        out = ''\n    \n    return out\n\n\n# Function to clean news data\ndef clean_news_data(raw_data_dir, clean_data_dir):\n    \n    # Iterate through raw data files\n    for file_name in os.listdir(raw_data_dir):\n        # Process only raw data files\n        if file_name.endswith(\"_raw_text.json\"):  \n            \n            # Load the raw data\n            raw_file_path = os.path.join(raw_data_dir, file_name)\n            with open(raw_file_path, 'r') as raw_file:\n                raw_data = json.load(raw_file)\n            \n            # inititlize dictionary to store clean the data\n            clean_data = {}\n            for article in raw_data:\n                # extract title\n                title = article.get('title', '')\n                # extract article's description\n                description = article.get('description', '')\n                \n                # clean the title and description\n                if title and description:\n                    # clean the title\n                    clean_title = string_cleaner(title)\n                    # clean the description\n                    clean_description = string_cleaner(description)\n                    # store the clean data in the dictionary\n                    clean_data[clean_title] = clean_description\n            \n            # Save the cleaned data to a new file\n            clean_file_name = file_name.replace(\"_raw_text.json\", \"_clean_news.json\")\n            clean_file_path = os.path.join(clean_data_dir, clean_file_name)\n            with open(clean_file_path, 'w') as clean_file:\n                json.dump(clean_data, clean_file, indent=4)\n\n\n# define directories\n\n# directory with raw data files\nraw_data_dir = \"../../data/raw-data/News_Drivers\"  \n\n# directory for cleaned data files\nclean_data_dir = \"../../data/processed-data/News_drivers\"  \n\n\n# call the function to process the clean data\nclean_news_data(raw_data_dir, clean_data_dir)\n\n\n# raw data\nprint(\"RAW DATA---\")\nraw = \"../../data/raw-data/News_drivers/Alonso_raw_text.json\"\nwith open(raw, 'r') as json_file:\n    data = json.load(json_file)\nfor entry in data[:2]:  \n    title = entry.get(\"title\", \"No Title\")\n    description = entry.get(\"description\", \"No Description\")\n    print(title,\":\",description)\n\nprint(\"\\n\")\n\n# clean data\nprint(\"CLEAN DATA---\")\nclean = \"../../data/processed-data/News_drivers/Alonso_clean_news.json\"\nwith open(clean, 'r') as json_file:\n    data = json.load(json_file)\n\nfor key, value in list(data.items())[:2]: \n    print(key,\":\", value)\n\nRAW DATA---\nNorris calls Verstappen 'dangerous' as Sainz wins in Mexico : Lando Norris cuts Max Verstappen’s lead to 47 points and labels his rival “dangerous” as the championship battle reaches boiling point at the Mexico City Grand Prix.\nHow well do you know Fernando Alonso? : As he prepares for his 400th F1 grand prix in Mexico City this weekend, find out how much you know about Fernando Alonso.\n\n\nCLEAN DATA---\nnorris calls verstappen 'dangerous' sainz wins mexico : lando norris cuts verstappens lead points labels rival dangerous championship battle reaches boiling point mexico city grand prix\nwell know fernando alonso : prepares grand prix mexico city weekend find much know fernando alonso"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#drivers-standings",
    "href": "technical-details/data-cleaning/main.html#drivers-standings",
    "title": "Data Cleaning",
    "section": "Drivers Standings",
    "text": "Drivers Standings\n\ninput_file = \"../../data/raw-data/Driver_standings/driver_standings_2000_2023.json\"\noutput_file = \"../../data/processed-data/driver_standings_2000_2023.csv\"\n\n\nwith open(input_file, 'r') as f:\n    data = json.load(f)\n\n# initialize a list to store extracted records\ncleaned_data = []\n\n# loop through each season in the JSON\nfor season, season_data in data.items():\n    # navigate to the StandingsLists section \n    standings_lists = season_data.get('MRData', {}).get('StandingsTable', {}).get('StandingsLists', [])\n    \n    # loop through each standings entry\n    for standings in standings_lists:\n        # extract driver standings\n        driver_standings = standings.get('DriverStandings', [])\n        \n        # loop through each entry\n        for entry in driver_standings:\n            # Extract required fields\n            position = entry.get('position', '')            # positions in the standings\n            points = entry.get('points', '')                # points earned by the driver\n            wins = entry.get('wins', '')                    # number of wins by the driver\n            driver = entry.get('Driver', {})                # nested dict. containing driver info\n            constructors = entry.get('Constructors', [])    # team info of the driver\n            \n            # extract driver and constructor details\n            given_name = driver.get('givenName', '')\n            family_name = driver.get('familyName', '')\n            constructor_id = constructors[0].get('constructorId', '') if constructors else ''\n            constructor_name = constructors[0].get('name', '') if constructors else ''\n            \n            # append the record to the cleaned data list\n            cleaned_data.append({\n                \"Season\": season,\n                \"Position\": position,\n                \"FirstName\": given_name,\n                \"LastName\": family_name,\n                \"Constructor_ID\": constructor_id,\n                \"Constructor_Name\": constructor_name,\n                \"Points\": points,\n                \"Wins\": wins\n            })\n\n\n# convert the cleaned data into a dataframe for easier manipulation\ndf = pd.DataFrame(cleaned_data)\ndf.head()\n\n\n\n\n\n\n\n\nSeason\nPosition\nFirstName\nLastName\nConstructor_ID\nConstructor_Name\nPoints\nWins\n\n\n\n\n0\n2000\n1\nMichael\nSchumacher\nferrari\nFerrari\n108\n9\n\n\n1\n2000\n2\nMika\nHäkkinen\nmclaren\nMcLaren\n89\n4\n\n\n2\n2000\n3\nDavid\nCoulthard\nmclaren\nMcLaren\n73\n3\n\n\n3\n2000\n4\nRubens\nBarrichello\nferrari\nFerrari\n62\n1\n\n\n4\n2000\n5\nRalf\nSchumacher\nwilliams\nWilliams\n24\n0\n\n\n\n\n\n\n\n\n# check for missing values\ndf.isnull().sum()\n\nSeason              0\nPosition            0\nFirstName           0\nLastName            0\nConstructor_ID      0\nConstructor_Name    0\nPoints              0\nWins                0\ndtype: int64\n\n\n\n# concat FirstName and LastName\ndf['driverName'] = df['FirstName'] + \" \" + df['LastName']\ndf = df.drop(['FirstName', 'LastName'], axis=1)\n\n# write the clean data to a new csv file\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#circuit-information",
    "href": "technical-details/data-cleaning/main.html#circuit-information",
    "title": "Data Cleaning",
    "section": "Circuit Information",
    "text": "Circuit Information\n\ninput_file = \"../../data/raw-data/circuit_data/circuit_data.json\"\noutput_file = \"../../data/processed-data/circuit_data_clean.csv\"\n\n\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# read the JSON file\nwith open(input_file, 'r') as f:\n    data = json.load(f)\n\n# extract circuit data\ncircuits = data.get('MRData', {}).get('CircuitTable', {}).get('Circuits', [])\n\n# prepare a list to store extracted records\ncleaned_data = []\n\nfor circuit in circuits:\n    circuit_id = circuit.get('circuitId', '')\n    circuit_name = circuit.get('circuitName', '')\n    country = circuit.get('Location', {}).get('country', '')\n    latitude = circuit.get('Location', {}).get('lat', '')\n    longitude = circuit.get('Location', {}).get('long', '')\n    \n    # append to the list\n    cleaned_data.append({\n        \"Circuit_ID\": circuit_id,\n        \"Circuit_Name\": circuit_name,\n        \"Country\": country,\n        \"Latitude\": latitude,\n        \"Longitude\": longitude\n    })\n\n# convert the list to a Pandas DataFrame\ndf = pd.DataFrame(cleaned_data)\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nCircuit_ID\nCircuit_Name\nCountry\nLatitude\nLongitude\n\n\n\n\n0\nadelaide\nAdelaide Street Circuit\nAustralia\n-34.9272\n138.617\n\n\n1\nain-diab\nAin Diab\nMorocco\n33.5786\n-7.6875\n\n\n2\naintree\nAintree\nUK\n53.4769\n-2.94056\n\n\n3\nalbert_park\nAlbert Park Grand Prix Circuit\nAustralia\n-37.8497\n144.968\n\n\n4\namericas\nCircuit of the Americas\nUSA\n30.1328\n-97.6411\n\n\n\n\n\n\n\n\n# check for null values\ndf.isnull().sum()\n\nCircuit_ID      0\nCircuit_Name    0\nCountry         0\nLatitude        0\nLongitude       0\ndtype: int64\n\n\n\n# Save the DataFrame to a CSV file\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#race-data",
    "href": "technical-details/data-cleaning/main.html#race-data",
    "title": "Data Cleaning",
    "section": "Race data",
    "text": "Race data\n\n## Cleaning all the race_data and appending them in to a single csv file \n\n# input output directory\ninput_dir = \"../../data/raw-data/\"\n# output directory\noutput_file = \"../../data/processed-data/all_race_results_cleaned.csv\"\n\n# creating an output file\nos.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n# initialize a list to hold all results\nall_combined_results = []\n\n# process each JSON file in the input directory\n# they are the only .json files in the directory\nfor file_name in os.listdir(input_dir):\n    # process only JSON files\n    if file_name.endswith(\".json\"): \n        file_path = os.path.join(input_dir, file_name)\n        #print(f\"Processing file: {file_path}\")\n        \n        # read the JSON file\n        with open(file_path, 'r') as f:\n            data = json.load(f)\n        \n        # extract races from the JSON\n        races = data.get('MRData', {}).get('RaceTable', {}).get('Races', [])\n        \n        # prepare a list to hold flattened race results for this file\n        file_results = []\n\n        # loop through each race and flatten its data\n        for race in races:\n            # extract required information \n            race_info = { \n                \"season\": race.get(\"season\", \"\"),\n                \"round\": race.get(\"round\", \"\"),\n                \"raceName\": race.get(\"raceName\", \"\"),\n                \"url\": race.get(\"url\",\"\"),\n                \"circuitName\": race.get(\"Circuit\", {}).get(\"circuitName\", \"\"),\n                \"locality\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"locality\", \"\"),\n                \"country\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"country\", \"\"),\n                \"lat\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"lat\", \"\"),\n                \"long\": race.get(\"Circuit\", {}).get(\"Location\", {}).get(\"long\", \"\"),\n                \"date\": race.get(\"date\", \"\"),\n            }\n            \n            # extract results and combine with useful details\n            results = race.get(\"Results\", [])\n            for result in results:\n                # combine race-level and result-level data\n                combined_data = {**race_info, **result}\n                # add flattened driver and constructor details\n                combined_data.update({\n                    \"driverId\": result.get(\"Driver\", {}).get(\"driverId\", \"\"),\n                    \"driverGivenName\": result.get(\"Driver\", {}).get(\"givenName\", \"\"),\n                    \"driverFamilyName\": result.get(\"Driver\", {}).get(\"familyName\", \"\"),\n                    \"constructorId\": result.get(\"Constructor\", {}).get(\"constructorId\", \"\"),\n                    \"constructorName\": result.get(\"Constructor\", {}).get(\"name\", \"\"),\n                    \"status\": result.get(\"status\", \"\"),\n                    \"timeMillis\": result.get(\"Time\", {}).get(\"millis\", \"\"),\n                    \"time\": result.get(\"Time\", {}).get(\"time\", \"\")\n                })\n                file_results.append(combined_data)\n\n        # append the results for this file to the combined list\n        all_combined_results.extend(file_results)\n\n# aonvert the combined results to a Pandas DataFrame\ndf = pd.DataFrame(all_combined_results)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\nlat\nlong\ndate\n...\nstatus\nTime\nFastestLap\ndriverId\ndriverGivenName\ndriverFamilyName\nconstructorId\nconstructorName\ntimeMillis\ntime\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '5960396', 'time': '1:39:20.396'}\n{'rank': '1', 'lap': '45', 'Time': {'time': '1...\nalonso\nFernando\nAlonso\nferrari\nFerrari\n5960396\n1:39:20.396\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '5976495', 'time': '+16.099'}\n{'rank': '5', 'lap': '38', 'Time': {'time': '1...\nmassa\nFelipe\nMassa\nferrari\nFerrari\n5976495\n+16.099\n\n\n2\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '5983578', 'time': '+23.182'}\n{'rank': '4', 'lap': '42', 'Time': {'time': '1...\nhamilton\nLewis\nHamilton\nmclaren\nMcLaren\n5983578\n+23.182\n\n\n3\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '5999195', 'time': '+38.799'}\n{'rank': '12', 'lap': '32', 'Time': {'time': '...\nvettel\nSebastian\nVettel\nred_bull\nRed Bull\n5999195\n+38.799\n\n\n4\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\nFinished\n{'millis': '6000609', 'time': '+40.213'}\n{'rank': '13', 'lap': '45', 'Time': {'time': '...\nrosberg\nNico\nRosberg\nmercedes\nMercedes\n6000609\n+40.213\n\n\n\n\n5 rows × 28 columns\n\n\n\n\n# save the combined DataFrame to a CSV file\ndf.to_csv(output_file, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#weather-data",
    "href": "technical-details/data-cleaning/main.html#weather-data",
    "title": "Data Cleaning",
    "section": "Weather Data",
    "text": "Weather Data\n\nweather_df = pd.read_csv(\"../../data/raw-data/weather/race_data_with_weather.csv\")\n\n\nweather_df.isnull().sum()\n\nseason      0\nraceName    0\nurl         0\nweather     0\ndtype: int64\n\n\n\nweather_df['weather']\n\n0                                                  Sunny\n1                      Overcast with light rain at start\n2                                     Mainly cloudy, dry\n3                                           Cloudy, rain\n4                                     Mainly cloudy, dry\n                             ...                        \n117    Sunny with temperatures reaching up to 27 °C (...\n118    Dry start, with heavy rain and thunderstorm/mo...\n119                                                 Rain\n120                                                Sunny\n121                                          Warm, Sunny\nName: weather, Length: 122, dtype: object\n\n\nwe will try to categorise the weather description into one of the following categories:\n\nSunny\nCloudy\nRainy\nWindy\n\n\ndef classify_weather(weather_description):\n\n    weather_description = weather_description.lower()\n    \n    if \"sunny\" in weather_description or \"fine\" in weather_description or \"clear\" in weather_description or \"dry\" in weather_description:\n        return \"Sunny\"\n    elif \"cloudy\" in weather_description or \"overcast\" in weather_description or \"cloud\" in weather_description:\n        return \"Cloudy\"\n    elif \"rain\" in weather_description or \"thunderstorms\" in weather_description or \"drizzle\" in weather_description:\n        return \"Rainy\"\n    elif \"windy\" in weather_description:\n        return \"Windy\"\n    # If no match, classify as \"Not Available\"\n    else:\n        return \"Not Available\" \n\n\n# call classify_weather()\nweather_df['weather_class'] = weather_df['weather'].apply(classify_weather)\n\n\nweather_df['weather_class'].value_counts()\n\nweather_class\nSunny            95\nCloudy           24\nRainy             2\nNot Available     1\nName: count, dtype: int64\n\n\nDue to severe class imbalance, weather_class feature is not a suitable candidate for effective analysis or model training.\n\nprint(weather_df[weather_df['weather_class'] == \"Not Available\"])\n\n   season             raceName  \\\n9    2006  European Grand Prix   \n\n                                                 url        weather  \\\n9  http://en.wikipedia.org/wiki/2006_European_Gra...  Not Available   \n\n   weather_class  \n9  Not Available  \n\n\nThe weather data for 2006 European Grand Prix is not available on wikipedia.\n\nUsing longitude, latitude and the date: The weather was Sunny\n\n\nweather_df['weather_class'] = weather_df['weather_class'].replace('Not Available', 'Sunny')\n\n\n# Save the updated DataFrame to a CSV file\noutput_csv = \"../../data/processed-data/classified_weather_data.csv\"\nweather_df.to_csv(output_csv, index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#race-info-merged",
    "href": "technical-details/data-cleaning/main.html#race-info-merged",
    "title": "Data Cleaning",
    "section": "Race Info Merged",
    "text": "Race Info Merged\n\n# merge race results and weather information\nrace_df = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")\nweather_df = pd.read_csv(\"../../data/processed-data/classified_weather_data.csv\")\n\n# the url contains the season and the race name \n# url can be used to merge the data\nmerged_df = race_df.merge(weather_df[['url','weather_class']], on='url', how='left')\n\nmerged_df.to_csv(\"../../data/processed-data/race_weather_merged.csv\", index=False)\n\n\nmain_df = pd.read_csv(\"../../data/processed-data/race_weather_merged.csv\")\nmain_df.head()\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\nlat\nlong\ndate\n...\nTime\nFastestLap\ndriverId\ndriverGivenName\ndriverFamilyName\nconstructorId\nconstructorName\ntimeMillis\ntime\nweather_class\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5960396', 'time': '1:39:20.396'}\n{'rank': '1', 'lap': '45', 'Time': {'time': '1...\nalonso\nFernando\nAlonso\nferrari\nFerrari\n5960396.0\n1:39:20.396\nSunny\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5976495', 'time': '+16.099'}\n{'rank': '5', 'lap': '38', 'Time': {'time': '1...\nmassa\nFelipe\nMassa\nferrari\nFerrari\n5976495.0\n+16.099\nSunny\n\n\n2\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5983578', 'time': '+23.182'}\n{'rank': '4', 'lap': '42', 'Time': {'time': '1...\nhamilton\nLewis\nHamilton\nmclaren\nMcLaren\n5983578.0\n+23.182\nSunny\n\n\n3\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '5999195', 'time': '+38.799'}\n{'rank': '12', 'lap': '32', 'Time': {'time': '...\nvettel\nSebastian\nVettel\nred_bull\nRed Bull\n5999195.0\n+38.799\nSunny\n\n\n4\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n26.0325\n50.5106\n2010-03-14\n...\n{'millis': '6000609', 'time': '+40.213'}\n{'rank': '13', 'lap': '45', 'Time': {'time': '...\nrosberg\nNico\nRosberg\nmercedes\nMercedes\n6000609.0\n+40.213\nSunny\n\n\n\n\n5 rows × 29 columns\n\n\n\n\nmain_df.columns\n\nIndex(['season', 'round', 'raceName', 'url', 'circuitName', 'locality',\n       'country', 'lat', 'long', 'date', 'number', 'position', 'positionText',\n       'points', 'Driver', 'Constructor', 'grid', 'laps', 'status', 'Time',\n       'FastestLap', 'driverId', 'driverGivenName', 'driverFamilyName',\n       'constructorId', 'constructorName', 'timeMillis', 'time',\n       'weather_class'],\n      dtype='object')\n\n\n\n# drop un-needed columns\nmain_df = main_df.drop(['lat', 'long', 'number', 'positionText', 'Driver', 'Constructor', 'Time', 'FastestLap'], axis=1)\n\n\nmain_df.columns\n\nIndex(['season', 'round', 'raceName', 'url', 'circuitName', 'locality',\n       'country', 'date', 'position', 'points', 'grid', 'laps', 'status',\n       'driverId', 'driverGivenName', 'driverFamilyName', 'constructorId',\n       'constructorName', 'timeMillis', 'time', 'weather_class'],\n      dtype='object')\n\n\n\nmain_df.isnull().sum()\n\nseason                 0\nround                  0\nraceName               0\nurl                    0\ncircuitName            0\nlocality               0\ncountry                0\ndate                   0\nposition               0\npoints                 0\ngrid                   0\nlaps                   0\nstatus                 0\ndriverId               0\ndriverGivenName        0\ndriverFamilyName       0\nconstructorId          0\nconstructorName        0\ntimeMillis          1291\ntime                1291\nweather_class          0\ndtype: int64\n\n\nThe missing values in ‘timeMillis' and’time' columns are of those drivers who did not finish the race. Therefore, we will drop these columns and try to analyse the performance based on other metrics.\n\n# drop un-needed columns\nmain_df = main_df.drop(['timeMillis', 'time'], axis=1)\n\n\nmain_df['constructorName'].value_counts()\n\nconstructorName\nFerrari           235\nMcLaren           234\nWilliams          229\nRed Bull          184\nRenault           142\nSauber            139\nMercedes          134\nToro Rosso        127\nForce India       100\nHaas F1 Team       79\nToyota             76\nJordan             57\nBAR                56\nMinardi            54\nAlfa Romeo         50\nJaguar             45\nBMW Sauber         40\nAlphaTauri         40\nLotus F1           38\nAlpine F1 Team     30\nAston Martin       30\nHonda              28\nArrows             26\nHRT                24\nMarussia           24\nSuper Aguri        24\nCaterham           24\nRacing Point       20\nProst              18\nBenetton           18\nVirgin             16\nManor Marussia     16\nLotus              16\nBrawn              10\nMF1                 9\nSpyker              8\nName: count, dtype: int64\n\n\n\nmain_df = main_df.drop(['constructorId'], axis=1)\n\nSome of the team names were changed in the process of rebranding or due to a change in ownership. For accurate analysis, we will replace the older versions of the constructors’ names with the current ones.\n\nFerrari\nMcLaren\nJaguar\nWilliams\nSauber \\(\\rightarrow\\) BMW Sauber \\(\\rightarrow\\) Sauber \\(\\rightarrow\\) Alfa Romeo \\(\\rightarrow\\) Sauber\nBAR \\(\\rightarrow\\) Honda \\(\\rightarrow\\) Brawn \\(\\rightarrow\\) Mercedes\nBenetton \\(\\rightarrow\\) Renault \\(\\rightarrow\\) Lotus F1 \\(\\rightarrow\\) Renault \\(\\rightarrow\\) Alpine\nJordan \\(\\rightarrow\\) Midland \\(\\rightarrow\\) Spyker \\(\\rightarrow\\) Force India \\(\\rightarrow\\) Aston Martin\nMinardi \\(\\rightarrow\\) Toro Rosso \\(\\rightarrow\\) Scuderia AlphaTauri\nHaas\nToyota\nVirgin Racing \\(\\rightarrow\\) Marussia F1 \\(\\rightarrow\\) Manor Marussia\nLotus \\(\\rightarrow\\) Caterham\nArrows\nSuper Aguri\nHRT\nProst\n\n\nconstructor_mapping= {\n    \"Jaguar\" : \"Red Bull\",\n    \"BMW Sauber\" : \"Sauber\",\n    \"Alfa Romeo\" : \"Sauber\",\n    \"BAR\" : \"Mercedes\",\n    \"Honda\" : \"Mercedes\",\n    \"Brawn\" : \"Mercedes\",\n    \"Minardi\" : \"AlphaTauri\",\n    \"Toro Rosso\" : \"AlphaTauri\",\n    \"Force India\" : \"Aston Martin\",\n    \"Jordan\" : \"Aston Martin\",\n    \"Racing Point\" : \"Aston Martin\",\n    \"MF1\" : \"Aston Martin\",\n    \"Spyker\" : \"Aston Martin\",\n    \"Lotus F1\" : \"Alpine F1 Team\",\n    \"Renault\" : \"Alpine F1 Team\",\n    \"Benetton\" : \"Alpine F1 Team\",\n    \"Manor Marussia\" : \"Marussia\",\n    \"Virgin\" : \"Marussia\",\n    \"Lotus\" : \"Caterham\",\n\n}\n\n\nmain_df['constructorName'] = main_df['constructorName'].replace(constructor_mapping)\n\n\nmain_df['constructorName'].value_counts()\n\nconstructorName\nFerrari           235\nMcLaren           234\nRed Bull          229\nWilliams          229\nSauber            229\nMercedes          228\nAlpine F1 Team    228\nAston Martin      224\nAlphaTauri        221\nHaas F1 Team       79\nToyota             76\nMarussia           56\nCaterham           40\nArrows             26\nSuper Aguri        24\nHRT                24\nProst              18\nName: count, dtype: int64\n\n\n\nmain_df['driverName'] = main_df['driverGivenName'] + \" \" + main_df['driverFamilyName']\n\n\nmain_df = main_df.drop(['driverGivenName','driverFamilyName'], axis = 1)\n\n\nmain_df.head(2)\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\ndate\nposition\npoints\ngrid\nlaps\nstatus\ndriverId\nconstructorName\nweather_class\ndriverName\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n1\n25.0\n3\n49\nFinished\nalonso\nFerrari\nSunny\nFernando Alonso\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n2\n18.0\n2\n49\nFinished\nmassa\nFerrari\nSunny\nFelipe Massa\n\n\n\n\n\n\n\nClassify the satus column in to broader categories. This column provides information on whether the driver has finished the race or not, if not, was it because of a mechanical failure, an accident, or was he lapped. The categories are:\n\nFinished\nLap\nAccident\nMechanical\n\nGrouping the data helps understand the major reasons for the race results without getting overwhelmed by the granular details.\n\nmain_df['status'].unique()\n\narray(['Finished', '+1 Lap', '+2 Laps', 'Electrical', 'Hydraulics',\n       'Overheating', 'Gearbox', 'Suspension', 'Accident', '+5 Laps',\n       'Wheel', 'Engine', 'Spun off', 'Collision', '+3 Laps', '+4 Laps',\n       '+10 Laps', 'Throttle', 'Clutch', 'Technical', 'Mechanical',\n       'Driveshaft', 'Transmission', 'Steering', 'Puncture', 'Brakes',\n       'Retired', 'Tyre', 'Fuel pressure', '+9 Laps', 'Water leak',\n       'Disqualified', 'Did not qualify', '+42 Laps', 'Engine misfire',\n       'Power Unit', 'Oil pressure', 'Safety concerns', 'Fuel system',\n       '+6 Laps', 'Electronics', 'Collision damage', 'Wheel nut',\n       'Battery', 'Oil leak', '+7 Laps', 'Stalled', 'Exhaust',\n       'Vibrations', 'Broken wing', 'Fuel', 'Wheel rim', 'Power loss',\n       '107% Rule', '+8 Laps', 'ERS', 'Withdrew', 'Cooling system',\n       'Water pump', 'Fuel leak', 'Front wing', 'Turbo', 'Damage',\n       'Out of fuel', 'Radiator', 'Oil line', 'Fuel rig',\n       'Launch control', 'Not classified', 'Pneumatics', 'Differential'],\n      dtype=object)\n\n\n\n# classifying different categories under status\ndef classify_status(status):\n    if status == 'Finished':\n        return 'Finished'\n    # if the status contains the word 'Lap' \n    elif 'Lap' in status:  \n        return 'Lapped'\n    elif status in ['Accident', 'Collision', 'Spun off', 'Withdrew']:\n        return 'Accident'\n    # all other statuses are classified as 'Mechanical'\n    else:\n        return 'Mechanical'\n\nmain_df['status'] = main_df['status'].apply(classify_status)\n\n\nmain_df['status'].value_counts()\n\nstatus\nFinished      1105\nLapped         693\nMechanical     412\nAccident       190\nName: count, dtype: int64\n\n\n\nmain_df.to_csv(\"../../data/processed-data/race_info.csv\", index=False)\n\nFinish category - new categorical variable\nIn F1, race outcomes are categorized based on finishing positions:\n\nThe top 3 finishers are celebrated on the podium and referred to as achieving a Podium Finish\nAll drivers who finish in top 10 earn championship points, ranging from 25 points for the winner, 18 for second place, and decreasing to 1 point for the 10th position.\nDrivers finishing outside the top 10 do not earn any championship points.\n\n\ndata = pd.read_csv(\"../../data/processed-data/race_info.csv\")\n# create a new categorical variable \ndata['FinishCategory'] = ''\n# iterate through each row of the dataset\nfor i in range(len(data)):\n    # if the position is 1,2 or 3\n    if data['position'][i] in [1,2,3]:\n        # assign the category as podium\n        data['FinishCategory'][i] = \"Podium\"\n    # if the position is in between 4 and 10\n    elif data['position'][i] in [4,5,6,7,8,9,10]:\n        # assign the category as points finish\n        data['FinishCategory'][i] = \"Points Finish\"\n    # assign no points for poistions outside top 10\n    else:\n        data['FinishCategory'][i] = \"No Points\"\n\n\ndata.head()\n\n\n\n\n\n\n\n\nseason\nround\nraceName\nurl\ncircuitName\nlocality\ncountry\ndate\nposition\npoints\ngrid\nlaps\nstatus\ndriverId\nconstructorName\nweather_class\ndriverName\nFinishCategory\n\n\n\n\n0\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n1\n25.0\n3\n49\nFinished\nalonso\nFerrari\nSunny\nFernando Alonso\nPodium\n\n\n1\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n2\n18.0\n2\n49\nFinished\nmassa\nFerrari\nSunny\nFelipe Massa\nPodium\n\n\n2\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n3\n15.0\n4\n49\nFinished\nhamilton\nMcLaren\nSunny\nLewis Hamilton\nPodium\n\n\n3\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n4\n12.0\n1\n49\nFinished\nvettel\nRed Bull\nSunny\nSebastian Vettel\nPoints Finish\n\n\n4\n2010\n1\nBahrain Grand Prix\nhttp://en.wikipedia.org/wiki/2010_Bahrain_Gran...\nBahrain International Circuit\nSakhir\nBahrain\n2010-03-14\n5\n10.0\n5\n49\nFinished\nrosberg\nMercedes\nSunny\nNico Rosberg\nPoints Finish\n\n\n\n\n\n\n\n\ndata.to_csv(\"../../data/processed-data/race_info.csv\", index=False)"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#race-track-features",
    "href": "technical-details/data-cleaning/main.html#race-track-features",
    "title": "Data Cleaning",
    "section": "Race Track Features",
    "text": "Race Track Features\n\n# race track features\ndata = pd.read_csv(\"../../data/raw-data/circuit_data/merged_circuit_features.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\n0\n2020\nPre-Season Test 1\n4312.438437\n323\n70.673953\n1\n4\n\n\n1\n2020\nPre-Season Test 2\n4312.438437\n323\n70.673953\n1\n4\n\n\n2\n2020\nAustrian Grand Prix\n4312.438437\n323\n70.673953\n1\n4\n\n\n3\n2020\nStyrian Grand Prix\n4292.610384\n300\n46.556886\n2\n6\n\n\n4\n2020\nHungarian Grand Prix\n4348.049386\n318\n58.114374\n0\n6\n\n\n\n\n\n\n\n\ndata.isnull().sum()\n\nYear                   0\nGrand Prix             0\nTrack Length (m)       0\nMax Speed (km/h)       0\nFull Throttle (%)      0\nNumber of Corners      0\nNumber of Straights    0\ndtype: int64\n\n\nStandardization\nStandardization is a preprocessing step used to scale the features is a consistent scale for more accurate and stable modelling. Features with different scales can lead to one feature dominating others during model training, Standardization eliminates disparity.\nFor the Race Track Features, StandardScaler is used, this standardizes the data by “removing the mean and scaling to unit variance”1.\nFormula for Standardization: \\[ Z = \\frac{X - \\mu}{\\sigma} \\] where:\n\n\\(X\\): the original value\n\\(\\mu\\): mean\n\\(\\sigma\\): standard deviation of the feature\n\n\nreq_cols = [\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]\nscaler = StandardScaler()\n\n# Apply the scaler to the dataframe\ndata[req_cols] = scaler.fit_transform(data[req_cols])\n\n\ndata.head()\n\n\n\n\n\n\n\n\nYear\nGrand Prix\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\n0\n2020\nPre-Season Test 1\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n1\n2020\nPre-Season Test 2\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n2\n2020\nAustrian Grand Prix\n-1.000607\n-0.115670\n1.059667\n-0.789651\n-0.938394\n\n\n3\n2020\nStyrian Grand Prix\n-1.024865\n-1.840980\n-1.757479\n-0.275003\n-0.037811\n\n\n4\n2020\nHungarian Grand Prix\n-0.957039\n-0.490737\n-0.407433\n-1.304300\n-0.037811\n\n\n\n\n\n\n\n\ndata.to_csv(\"../../data/processed-data/race_track_features.csv\")"
  },
  {
    "objectID": "technical-details/data-cleaning/main.html#footnotes",
    "href": "technical-details/data-cleaning/main.html#footnotes",
    "title": "Data Cleaning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStandardScaler↩︎"
  },
  {
    "objectID": "technical-details/eda/main.html#introduction",
    "href": "technical-details/eda/main.html#introduction",
    "title": "Exploratory Data Analysis",
    "section": "",
    "text": "The primary goal of Exploratory Data Analysis (EDA) is to gain a comprehensive understanding of the data, its quality, identify any underlying trends or relationships that may influence the analysis and modeling process.\nImportance:\n\nEDA helps detect outliers, and inconsistencies in the data, addressing these issues ensures that the analysis is based on accurate and clean data.\nThrough visualizations and statistical analysis, EDA identifies which features are most relevant to the questions and should be included in the model.\nEDA reveals relationships between variables, such as correlations, and trends, which can help discard certain features or create new derived features.\nBy analyzing the distribution of variables, we can identify central tendencies and variations. This helps us understand how the data is spread out and it it meets assumptions required by certain algorithms.\n\nTechniques:\n\nUnivariate Analysis:\n\nTo understand the distribution, and central tendency of a variable.\nVisualizations: Histograms, Box Plots, kernel Density plots.\nStatistical measures: Mean, Median, Mode, Variance, Standard Deviation.\n\nBivariate Analysis:\n\nTo understand the relationship between two variables\nCategorical-Categorical: Heatmaps.\nNumerical-Categorical: Box plots, Bar Graphs.\nNumerical-Numerical: Scatter plots, correlation coefficients.\n\nMultivariate Analysis:\n\nTo explore relationships between three or more variables simultaneously.\nVisualizations: Heatmaps, Scatter plots.\nDimensionality Reduction: PCA, t-SNE.\n\nData Distribution:\n\nHistograms and density plots are useful to detect skewness or multimodal distributions.\n\nStatistical Analysis:\n\nHypothesis Testing: t-tests, chi-square tests, ANOVA.\nCorrelation: Pearson, Spearman correlation coefficients\nSummary Statistics: Mean, variance, Quartiles.\n\n\nIn this project:\nThere are various datasets, and mutiple independent features such as: Driver Performance, Pit stop durations, race track features, overall performance of the car, which play an important role in determining the outcome of the race. Therefore, EDA is crucial to understand ow these factors influence race results."
  },
  {
    "objectID": "technical-details/eda/main.html#required-libraries",
    "href": "technical-details/eda/main.html#required-libraries",
    "title": "Exploratory Data Analysis",
    "section": "Required Libraries",
    "text": "Required Libraries\n\n# import required libraries\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport json\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.lines as mlines\nfrom matplotlib.cm import get_cmap\nimport seaborn as sns\nfrom scipy.stats import ttest_ind"
  },
  {
    "objectID": "technical-details/eda/main.html#news-data",
    "href": "technical-details/eda/main.html#news-data",
    "title": "Exploratory Data Analysis",
    "section": "News Data",
    "text": "News Data\nIn this section, we will analyze media coverage of select F1 drivers who created significant buzz during the 2024 season. The drivers include Max Verstappen, Carlos Sainz, Lando Norris, Daniel Riccardo, and Lewis Hamilton. These drivers have been at the center of media discussions due to their performance, contract negotiations, and potential moves for the 2025 season.\n\n# Function to generate a word cloud for a single JSON file\ndef generate_wordcloud_top_n(file_path, output_folder, top_n=10):\n    # helper function to plot and save the word cloud\n    def plot_cloud(wordcloud, output_file):\n        plt.figure(figsize=(10, 8))\n        plt.imshow(wordcloud, interpolation='bilinear')\n        plt.axis(\"off\")\n        plt.savefig(output_file) \n\n        plt.close()\n\n    # read the text from the JSON file\n    with open(file_path, 'r', encoding='utf-8') as file:\n        data = json.load(file)\n\n    # concatenate all text content if it's a JSON dictionary\n    if isinstance(data, dict):\n        my_text = ' '.join(data.values())\n\n    # generate word cloud to extract word frequencies\n    wordcloud = WordCloud(\n        width=3000,\n        height=2000,\n        random_state=1,\n        background_color=\"white\",  \n        colormap=\"Blues_r\",  \n        collocations=False,\n        stopwords=STOPWORDS\n    ).generate(my_text)\n\n    # extract word frequencies and get the top N words\n    word_frequencies = wordcloud.words_\n    top_words = dict(list(word_frequencies.items())[:top_n])\n\n    # generate a new word cloud with only the top N words\n    top_wordcloud = WordCloud(\n        width=3000,\n        height=2000,\n        random_state=1,\n        background_color=\"white\",  \n        colormap=\"Blues_r\",\n        collocations=False,\n        stopwords=STOPWORDS\n    ).generate_from_frequencies(top_words)\n\n    # Save and display the word cloud\n    file_name = os.path.basename(file_path).replace('.json', '_wordcloud.png')\n    output_file = os.path.join(output_folder, file_name)\n    plot_cloud(top_wordcloud, output_file)\n\n\n# function to process all JSON files in a folder\ndef generate_wordclouds_for_folder(input_folder, output_folder, top_n=10):\n    os.makedirs(output_folder, exist_ok=True)  \n    # loop through all files in the input folder\n    for file_name in os.listdir(input_folder):\n        # process only JSON files\n        if file_name.endswith('.json'):  \n            file_path = os.path.join(input_folder, file_name)\n            try:\n                generate_wordcloud_top_n(file_path, output_folder, top_n)\n            # for debugging purpose\n            except Exception as e:\n                print(f\"Error processing file {file_path}: {e}\")\n\n\ninput_folder = \"../../data/processed-data/News_Drivers/\" \noutput_folder = \"../../data/eda/WordClouds/\"  \ngenerate_wordclouds_for_folder(input_folder, output_folder, top_n=20)\n\nAnalysis\nMax Verstappen\n\n\n\n\n\n\n\n\n\n\nLeading airlines in terms of flight volume are Southwest Airlines, Skywest Airlines, and Delta Airlines.\n\nHawaiian Airlines, Forntier Airlines, and Allegiant Air have a smaller footprint.\n\nSince the analysis is on data collected from December 2020, the pandemic could have influenced these numbers, specifically the smaller airlines."
  },
  {
    "objectID": "technical-details/eda/main.html#news-data-1",
    "href": "technical-details/eda/main.html#news-data-1",
    "title": "Exploratory Data Analysis",
    "section": "News Data",
    "text": "News Data\n\n\n\n\n\n\n\n\n\n\nLeading airlines in terms of flight volume are Southwest Airlines, Skywest Airlines, and Delta Airlines.\n\nHawaiian Airlines, Forntier Airlines, and Allegiant Air have a smaller footprint.\n\nSince the analysis is on data collected from December 2020, the pandemic could have influenced these numbers, specifically the smaller airlines.\n\n\n\n\n\n\nCode\n# number of points by driver\ndata = pd.read_csv(\"../../data/processed-data/driver_standings_2000_2023.csv\")\n# Calculate total points for each driver\ntotal_points = data.groupby(\"driverName\")[\"Points\"].sum().reset_index()\n\n\ntop_10_drivers = total_points.sort_values(by=\"Points\", ascending=False).head(10)\n\n# Plot the bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.barh(top_10_drivers[\"driverName\"], top_10_drivers[\"Points\"], color=\"deepskyblue\")\n\nfor bar in bars:\n    plt.text(\n        bar.get_width() + 1, \n        bar.get_y() + bar.get_height() / 2,  \n        f'{int(bar.get_width())}',  \n        va='center', fontsize=12, color='black'\n    )\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.xlabel(\"Total Points\", fontsize=14)\nplt.ylabel(\"Driver\", fontsize=14)\nplt.title(\"Total Points per Driver\", fontsize=16)\nplt.gca().invert_yaxis()  \nplt.tight_layout()\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ntotal_points = data.groupby(\"Constructor_Name\")[\"Points\"].sum().reset_index()\n\n\nconstructors = total_points.sort_values(by=\"Points\", ascending=False).head(10)\n\n# Plot the bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.barh(constructors[\"Constructor_Name\"], constructors[\"Points\"], color=\"deepskyblue\")\n\nfor bar in bars:\n    plt.text(\n        bar.get_width() + 1, \n        bar.get_y() + bar.get_height() / 2,  \n        f'{int(bar.get_width())}',  \n        va='center', fontsize=12, color='black'\n    )\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.xlabel(\"Total Points\", fontsize=14)\nplt.ylabel(\"Constructor\", fontsize=14)\nplt.title(\"Total Points per Constructor\", fontsize=16)\nplt.gca().invert_yaxis()  \nplt.tight_layout()\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# points scored by driver over time with different teams\ndef plot_driver_points(data, driver_name, color_list=None):\n\n    driver_data = data[data['driverName'] == driver_name].sort_values(by='Season')\n    \n\n    unique_constructors = driver_data[\"Constructor_Name\"].unique()\n    if color_list:\n\n        constructor_colors = {\n            constructor: color_list[i % len(color_list)]\n            for i, constructor in enumerate(unique_constructors)\n        }\n    else:\n        constructor_colors = {\n            constructor: plt.cm.tab10(i / len(unique_constructors))\n            for i, constructor in enumerate(unique_constructors)\n        }\n    plt.figure(figsize=(10, 6))\n    \n\n    for i in range(len(driver_data) - 1):\n        season_start = driver_data.iloc[i]['Season']\n        season_end = driver_data.iloc[i + 1]['Season']\n        points_start = driver_data.iloc[i]['Points']\n        points_end = driver_data.iloc[i + 1]['Points']\n        constructor = driver_data.iloc[i]['Constructor_Name']\n        \n\n        plt.plot(\n            [season_start, season_end],\n            [points_start, points_end],\n            color=constructor_colors[constructor],\n            linewidth=4\n        )\n    \n\n    for _, row in driver_data.iterrows():\n        plt.text(row['Season'], row['Points'], str(row['Points']), fontsize=12, ha='center')\n    \n\n    plt.title(f\"Points Over Seasons for {driver_name}\", fontsize=16)\n    plt.xticks(fontsize = 14, rotation = 90)\n    plt.yticks(fontsize = 14)\n    plt.xlabel(\"Season\", fontsize=14)\n    plt.ylabel(\"Points\", fontsize=14)\n    plt.tight_layout()\n    plt.xticks(range(int(driver_data[\"Season\"].min()), int(driver_data[\"Season\"].max()) + 1))\n\n\n    legend_elements = [\n        mlines.Line2D([], [], color=constructor_colors[constructor], label=constructor, linewidth=3)\n        for constructor in unique_constructors\n    ]\n    plt.legend(handles=legend_elements, loc=\"upper left\", fontsize=14)\n    \n\n    plt.show()\n\ncolor_list = [\"lightskyblue\", \"cornflowerblue\", \"deepskyblue\", \"powderblue\", \"steelblue\", \"lightblue\"]\n\n\n\n\nCode\nplot_driver_points(data, \"Max Verstappen\", color_list)\n\n\n\n\n\n\n\n\n\n\n\nCode\n# distribution of \"Lapped\", \"Mechanical\", \"Accident\" across Constructors\ndata = pd.read_csv(\"../../data/processed-data/race_info.csv\")\n\n\n\n\nCode\n# Filter the data for specific statuses\nfiltered_data = data[data[\"status\"].isin([\"Lapped\", \"Mechanical\", \"Accident\"])]\n\n# Group by constructor and status to get counts\nstatus_distribution = filtered_data.groupby([\"constructorName\", \"status\"]).size().unstack(fill_value=0)\n\n# Plot the distribution with horizontal bars\nstatus_distribution.plot(\n    kind=\"barh\",\n    figsize=(12, 6),\n    color=['steelblue', 'deepskyblue', 'lightskyblue'],  \n    edgecolor=\"black\"\n)\n\n# Customize the plot\nplt.title(\"Distribution of 'Lapped', 'Mechanical', and 'Accident' Across Constructors\", fontsize=16)\nplt.xlabel(\"Count\", fontsize=14)\nplt.ylabel(\"Constructor\", fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(title=\"Status\", fontsize=12)\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Filter the data for specific statuses\nfiltered_data = data[data[\"status\"].isin([\"Lapped\", \"Mechanical\", \"Accident\"])]\n\n# Group by constructor and status to get counts\nstatus_distribution = filtered_data.groupby([\"constructorName\", \"status\"]).size().unstack(fill_value=0)\n\n# Plot the distribution as a stacked horizontal bar chart\nstatus_distribution.plot(\n    kind=\"barh\",\n    figsize=(12, 6),\n    color=['steelblue', 'deepskyblue', 'lightskyblue'],  \n    edgecolor=\"black\",\n    stacked=True  # Make the bars stacked\n)\n\n# Customize the plot\nplt.title(\"Distribution of 'Lapped', 'Mechanical', and 'Accident' Across Constructors\", fontsize=16)\nplt.xlabel(\"Count\", fontsize=14)\nplt.ylabel(\"Constructor\", fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(title=\"Status\", fontsize=12, loc=\"upper right\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\n\n# Display the plot\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# distrbution of status for each constructor \ndef autopct_format(pct):\n    return f'{pct:.1f}%' if pct &gt; 0 else ''\n\n\nfiltered_data = data[data[\"status\"].isin([\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"])]\n\n\nstatus_distribution = filtered_data.groupby([\"constructorName\", \"status\"]).size().unstack(fill_value=0)\n\n\nstatus_distribution[\"Total\"] = data.groupby(\"constructorName\").size()\n\n\nnum_constructors = len(status_distribution)\nnum_cols = 3\nnum_rows = (num_constructors + num_cols - 1) // num_cols \nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n\n\naxes = axes.flatten()\n\ncolors = ['steelblue', 'deepskyblue', 'lightskyblue', 'powderblue']\n\n\nfor idx, constructor in enumerate(status_distribution.index):\n    status_counts = status_distribution.loc[constructor, [\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"]]\n    total_count = status_distribution.loc[constructor, \"Total\"]\n    \n\n    percentages = (status_counts / total_count) * 100\n\n\n    wedges, texts, autotexts = axes[idx].pie(\n        percentages,\n        labels=None,  \n        autopct=autopct_format,\n        colors=colors,\n        startangle=140,\n        textprops={'fontsize': 10} \n    )\n    axes[idx].set_title(f\"{constructor}\", fontsize=14)\n\n    for autotext in autotexts:\n        autotext.set_fontsize(12)  \n\n\nfor i in range(idx + 1, len(axes)):\n    fig.delaxes(axes[i])\n\n# Add a shared legend\nfig.legend(\n    labels=[\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"],\n    loc=\"lower center\",\n    ncol=1,\n    fontsize=12\n)\n\n# Adjust layout\nplt.tight_layout(rect=[0, 0.1, 1, 1])  \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# trends in teams scoring points across seasons - for \"Red Bull\", \"Ferrari\", \"McLaren\", \"Mercedes\"\ndata = pd.read_csv(\"../../data/processed-data/driver_standings_2000_2023.csv\")\n\n\nfiltered_data = data[data[\"Constructor_Name\"].isin([\"Red Bull\", \"Ferrari\", \"McLaren\", \"Mercedes\"])]\n\n\ngrouped_data = filtered_data.groupby([\"Season\", \"Constructor_Name\"])[\"Points\"].sum().unstack()\n\n\ncolor_list = [\"red\", \"orange\", \"black\", \"deepskyblue\"]\n\n\nunique_constructors = grouped_data.columns\nconstructor_colors = {constructor: color_list[i % len(color_list)] for i, constructor in enumerate(unique_constructors)}\n\n\nplt.figure(figsize=(12, 6))\nfor constructor in grouped_data.columns:\n    plt.plot(\n        grouped_data.index, \n        grouped_data[constructor], \n        marker='o', \n        label=constructor, \n        color=constructor_colors[constructor]\n    )\n\n\nplt.title(\"Trends in Points for Selected Constructors\", fontsize=16)\nplt.xlabel(\"Season\", fontsize=14)\nplt.ylabel(\"Points\", fontsize=14)\nplt.xticks(grouped_data.index, rotation=90, fontsize=12) \nplt.yticks(fontsize=12)\nplt.legend(title=\"Constructor\", fontsize=12)\nplt.tight_layout()\n\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nsns.pairplot(data[['position', 'points', 'laps', 'grid', 'FinishCategory']], hue='FinishCategory', palette={'Podium':'steelblue', 'Points Finish':'lightskyblue', 'No Points': 'dodgerblue'})\n\n\n\n\n\n\n\n\n\n\n\nCode\nrf = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\n\n\n\n\nCode\nsns.pairplot(rf[[\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ncorr_matrix = rf[[\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]].corr()\ncorr_matrix\n\n\n\n\n\n\n\n\n\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\nTrack Length (m)\n1.000000\n0.373616\n0.399017\n-0.058163\n-0.118008\n\n\nMax Speed (km/h)\n0.373616\n1.000000\n0.462422\n-0.246056\n-0.281559\n\n\nFull Throttle (%)\n0.399017\n0.462422\n1.000000\n-0.654084\n-0.667532\n\n\nNumber of Corners\n-0.058163\n-0.246056\n-0.654084\n1.000000\n0.631757\n\n\nNumber of Straights\n-0.118008\n-0.281559\n-0.667532\n0.631757\n1.000000\n\n\n\n\n\n\n\n\n\nCode\nsns.heatmap(corr_matrix)\nplt.show()"
  },
  {
    "objectID": "technical-details/eda/main.html#drivers-performance",
    "href": "technical-details/eda/main.html#drivers-performance",
    "title": "Exploratory Data Analysis",
    "section": "Drivers’ Performance",
    "text": "Drivers’ Performance\n\n# number of points by driver\ndata = pd.read_csv(\"../../data/processed-data/driver_standings_2000_2023.csv\")\n# Calculate total points for each driver\ntotal_points = data.groupby(\"driverName\")[\"Points\"].sum().reset_index()\n\n# plot only top 10 drivers \ntop_10_drivers = total_points.sort_values(by=\"Points\", ascending=False).head(10)\n\n# Plot the bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.barh(top_10_drivers[\"driverName\"], top_10_drivers[\"Points\"], color=\"deepskyblue\")\n\nfor bar in bars:\n    plt.text(\n        bar.get_width() + 1, \n        bar.get_y() + bar.get_height() / 2,  \n        f'{int(bar.get_width())}',  \n        va='center', fontsize=12, color='black'\n    )\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.xlabel(\"Total Points\", fontsize=14)\nplt.ylabel(\"Driver\", fontsize=14)\nplt.title(\"Total Points per Driver\", fontsize=16)\nplt.gca().invert_yaxis()  \nplt.tight_layout()\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.show()\n\n\n\n\n\n\n\n\nInsights:\n\nThe cumulative points reflect a driver’s career longevity, performance, and overall success in the psport\nLewis Hamilton stands out as the clear leader with 4,630 points, significantly ahead of other drivers. This highlights his dominance in the sport over the years, attributed to his consistent race wins, and career with top teams like McLaren and Mercedes in their eras.\nSebastian Vettel holds the second spot with 3,098 points, his performance in Red Bull and Ferrari might have significant contribution to these numbers, since his early years in Sauber, and Toro Rosso, and his performance in Aston Martin were not as successful.\nMax Verstappen has the third highest cumulative total despite entering the sport relatively later. His rapid accumulation of points in the recent years underscores his dominance in the current era of F1.\n\n\n# points scored by driver over time with different teams\ndef plot_driver_points(data, driver_name, color_list=None):\n\n    # filter the data to include only the specified driver\n    # sort by season\n    driver_data = data[data['driverName'] == driver_name].sort_values(by='Season')\n    # list of unique constructors the driver raced for \n    unique_constructors = driver_data[\"Constructor_Name\"].unique()\n    # assign colors to constructors \n    if color_list:\n        constructor_colors = {\n            constructor: color_list[i % len(color_list)]\n            for i, constructor in enumerate(unique_constructors)\n        }\n    else:\n        constructor_colors = {\n            constructor: plt.cm.tab10(i / len(unique_constructors))\n            for i, constructor in enumerate(unique_constructors)\n        }\n    plt.figure(figsize=(10, 6))\n\n    # line plot\n    for i in range(len(driver_data) - 1):\n        season_start = driver_data.iloc[i]['Season']\n        season_end = driver_data.iloc[i + 1]['Season']\n        points_start = driver_data.iloc[i]['Points']\n        points_end = driver_data.iloc[i + 1]['Points']\n        constructor = driver_data.iloc[i]['Constructor_Name']\n        \n\n        plt.plot(\n            [season_start, season_end],\n            [points_start, points_end],\n            color=constructor_colors[constructor],\n            linewidth=4\n        )\n    \n\n    for _, row in driver_data.iterrows():\n        plt.text(row['Season'], row['Points'], str(row['Points']), fontsize=12, ha='center')\n    \n\n    plt.title(f\"Points Over Seasons for {driver_name}\", fontsize=16)\n    plt.xticks(fontsize = 14, rotation = 90)\n    plt.yticks(fontsize = 14)\n    plt.xlabel(\"Season\", fontsize=14)\n    plt.ylabel(\"Points\", fontsize=14)\n    plt.tight_layout()\n    plt.xticks(range(int(driver_data[\"Season\"].min()), int(driver_data[\"Season\"].max()) + 1))\n\n\n    legend_elements = [\n        mlines.Line2D([], [], color=constructor_colors[constructor], label=constructor, linewidth=3)\n        for constructor in unique_constructors\n    ]\n    plt.legend(handles=legend_elements, loc=\"upper left\", fontsize=14)\n    \n\n    plt.show()\n\ncolor_list = [\"lightskyblue\", \"cornflowerblue\", \"deepskyblue\", \"powderblue\", \"steelblue\", \"lightblue\"]\n\n\nplot_driver_points(data, \"Lewis Hamilton\", color_list)\n\n\n\n\n\n\n\n\n    \nInsights:\n\nA significant portion of the total championship points for most drivers has been earned while driving for top-performing teams such as Ferrari and Red Bull.\nThere is a clear correlation between the points scored in a season and the team a driver represented, emphasizing the importance of team performance in a driver’s success.\nObserving the career trajectories of drivers like Daniel Riccardo, Kimi Räikkönen, and Sebastian Vettel, we notice a pattern where they initially drove for top teams, followed by a move to midfield teams before retiring.\nSome drivers switched teams despite scoring a substantial number of championship points in the previous season. This highlights the strategic decisions made by teams in planning their driver line-ups and the drivers’ foresights regarding the potential improvements and competitiveness of cars across the grid."
  },
  {
    "objectID": "technical-details/eda/main.html#constructors-performance",
    "href": "technical-details/eda/main.html#constructors-performance",
    "title": "Exploratory Data Analysis",
    "section": "Constructors’ Performance",
    "text": "Constructors’ Performance\n\ntotal_points = data.groupby(\"Constructor_Name\")[\"Points\"].sum().reset_index()\n\n\nconstructors = total_points.sort_values(by=\"Points\", ascending=False).head(10)\n\n# Plot the bar chart\nplt.figure(figsize=(10, 6))\nbars = plt.barh(constructors[\"Constructor_Name\"], constructors[\"Points\"], color=\"deepskyblue\")\n\nfor bar in bars:\n    plt.text(\n        bar.get_width() + 1, \n        bar.get_y() + bar.get_height() / 2,  \n        f'{int(bar.get_width())}',  \n        va='center', fontsize=12, color='black'\n    )\nplt.xticks(fontsize = 14)\nplt.yticks(fontsize = 14)\nplt.xlabel(\"Total Points\", fontsize=14)\nplt.ylabel(\"Constructor\", fontsize=14)\nplt.title(\"Total Points per Constructor\", fontsize=16)\nplt.gca().invert_yaxis()  \nplt.tight_layout()\n\nplt.gca().spines['top'].set_visible(False)\nplt.gca().spines['right'].set_visible(False)\n\nplt.show()\n\n\n\n\n\n\n\n\nInsights:\n\nThe top 3 teams – Ferrari, Mercedes, and Red Bull – are closely positioned in the cumulative points standings.\nMercedes’ position is particularly notable because they re-entered the sports as a constructor in 2010, while Ferrari and Red Bull have accumulated points across the full 2000-2023 period. Despite the sorter timeframe, Mercedes managed to secure the second position, showcasing their unparalleled dominance 2014 onward.\nFrom domain knowledge, a key differentiating factor for Mercedes has been the consistent performance of their drivers, such as Lewis Hamilton, Nico Rosberg, and Valtteri Bottas, who regularly secured podium finishes during their tenure. This level of consistency has allowed Mercedes to close the gap with Ferrari and Red Bull in a relatively short period.\nFor Ferrari and Red Bull, the points reflect their long-standing success and ability to remain competitive across different eras. Ferrari’s consistent presence at the top, coupled with Red Bull’s dominance during Sebastian Vettel’s era (2010-2013) and Max Verstappen’s current reign, has solidified their positions in the top three\n\n\n# distribution of \"Lapped\", \"Mechanical\", \"Accident\" across Constructors\ndata = pd.read_csv(\"../../data/processed-data/race_info.csv\")\n\n\n# Filter the data for specific statuses\nfiltered_data = data[data[\"status\"].isin([\"Lapped\", \"Mechanical\", \"Accident\"])]\n\n# Group by constructor and status to get counts\nstatus_distribution = filtered_data.groupby([\"constructorName\", \"status\"]).size().unstack(fill_value=0)\n\n# Plot the distribution as a stacked horizontal bar chart\nstatus_distribution.plot(\n    kind=\"barh\",\n    figsize=(12, 6),\n    color=['steelblue', 'deepskyblue', 'lightskyblue'],  \n    edgecolor=\"black\",\n    stacked=True \n)\n\n# Customize the plot\nplt.title(\"Distribution of 'Lapped', 'Mechanical', and 'Accident' Across Constructors\", fontsize=16)\nplt.xlabel(\"Count\", fontsize=14)\nplt.ylabel(\"Constructor\", fontsize=14)\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.legend(title=\"Status\", fontsize=12, loc=\"upper right\")\nplt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n# distrbution of status for each constructor \ndef autopct_format(pct):\n    return f'{pct:.1f}%' if pct &gt; 0 else ''\n\nfiltered_data = data[data[\"status\"].isin([\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"])]\n# Group by constructor and status to get counts\nstatus_distribution = filtered_data.groupby([\"constructorName\", \"status\"]).size().unstack(fill_value=0)\n# total number of entries per constructor\nstatus_distribution[\"Total\"] = data.groupby(\"constructorName\").size()\n# total number of constructors\nnum_constructors = len(status_distribution)\n# for subplot\nnum_cols = 3\nnum_rows = (num_constructors + num_cols - 1) // num_cols \nfig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n\naxes = axes.flatten()\n\ncolors = ['steelblue', 'deepskyblue', 'lightskyblue', 'powderblue']\n# loop through each constructor and create a pie chart\nfor idx, constructor in enumerate(status_distribution.index):\n    # extract status counts for each constructor\n    status_counts = status_distribution.loc[constructor, [\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"]]\n    total_count = status_distribution.loc[constructor, \"Total\"]\n    # calcuate percentage for each status\n    percentages = (status_counts / total_count) * 100\n\n\n    wedges, texts, autotexts = axes[idx].pie(\n        percentages,\n        labels=None,  \n        autopct=autopct_format,\n        colors=colors,\n        startangle=140,\n        textprops={'fontsize': 14} \n    )\n    axes[idx].set_title(f\"{constructor}\", fontsize=14)\n\n    for autotext in autotexts:\n        autotext.set_fontsize(12)  \n\n\nfor i in range(idx + 1, len(axes)):\n    fig.delaxes(axes[i])\n\n# Add a shared legend\nfig.legend(\n    labels=[\"Finished\", \"Lapped\", \"Mechanical\", \"Accident\"],\n    loc=\"lower center\",\n    ncol=1,\n    fontsize=12\n)\n\nplt.tight_layout(rect=[0, 0.1, 1, 1])  \nplt.show()\n\n\n\n\n\n\n\n\nInsights:\n\nFerrari stands out with an impressive 77.9% race completion rate. The low percentage of mechanical failures and lapped races further emphasize their ability to build durable and reliable cars. Mercedes also displays as a high completion rate (66.2%) and a low percentage of mechanical failures and accidents. McLaren is a close third with 59.8% finished races, although it has low percentage of accidents, it has comparatively high percentage of lapped and incomplete races due to mechanical failures out of the top 3.\nWilliams, a team with a glorious past, shows 47.2% completion rate and 28.4% lapped races, highlighting the shift in their performance over the years. Having 11.4% accidents indicates that the drivers have not been performing well, 13.1% mechanical failures indicates struggles with car development, limited resources, and their current inability to compete with the top teams.\nHaas F1 Team a relatively new entrant, records 27.8% completion rate, with notable lapped races (45.6%) and mechanical failures. Their results reflect the challenges faced by newer teams with smaller budgets and sponsorships.\nMidfield and smaller teams face significant challenges in maintaining consistency due to resource limitations, budget caps, and the learning curve required to compete at the highest level. These teams often serve as development platforms for young drivers transitioning from F2 to F1 but struggle to compete with top constructors.\n\n\n# trends in teams scoring points across seasons - for \"Red Bull\", \"Ferrari\", \"McLaren\", \"Mercedes\"\ndata = pd.read_csv(\"../../data/processed-data/driver_standings_2000_2023.csv\")\n\nfiltered_data = data[data[\"Constructor_Name\"].isin([\"Red Bull\", \"Ferrari\", \"McLaren\", \"Mercedes\"])]\n\ngrouped_data = filtered_data.groupby([\"Season\", \"Constructor_Name\"])[\"Points\"].sum().unstack()\n\ncolor_list = [\"red\", \"orange\", \"black\", \"deepskyblue\"]\n\nunique_constructors = grouped_data.columns\nconstructor_colors = {constructor: color_list[i % len(color_list)] for i, constructor in enumerate(unique_constructors)}\n\n\nplt.figure(figsize=(12, 6))\nfor constructor in grouped_data.columns:\n    plt.plot(\n        grouped_data.index, \n        grouped_data[constructor], \n        marker='o', \n        label=constructor, \n        color=constructor_colors[constructor]\n    )\n\n\nplt.title(\"Trends in Points for Selected Constructors\", fontsize=16)\nplt.xlabel(\"Season\", fontsize=14)\nplt.ylabel(\"Points\", fontsize=14)\nplt.xticks(grouped_data.index, rotation=90, fontsize=12) \nplt.yticks(fontsize=12)\nplt.legend(title=\"Constructor\", fontsize=12)\nplt.tight_layout()\n\n\nplt.show()\n\n\n\n\n\n\n\n\nInsights:\n\nThe plot highlights clear periods of highs and lows for each team, emphasizing the dynamic nature of Formula 1. Success in the sport is cyclical, with teams experiencing periods of dominance followed by rebuilding phases.\nRed Bull showed a rapid rise starting in 2009, coinciding with their dominance during the Sebastian Vettel era (2010–2013). After a slight dip between 2014–2020, they have experienced another significant upward trend, driven by Max Verstappen’s dominance in recent years.\nMercedes entered the sport as a constructor in 2010 and saw rapid progress starting in 2014 during the hybrid engine era, where they dominated for nearly a decade. Their decline starting in 2021 coincides with the resurgence of Red Bull and highlights challenges in adapting to regulation changes.\nFerrari has experienced the most noticeable highs and lows among the teams. A steep decline is observed in 2020, likely due to car performance struggles, followed by a sharp recovery in 2022, marking their return to competitiveness.\nMcLaren had strong results in the early 2000s but faced a significant decline after 2012, struggling during the hybrid era. Their gradual recovery post-2018, reflected in the upward trend, signals improvements in car performance and team structure.\nThis analysis underscores that success in Formula 1 is not just about driver skill but also hinges on the ability of teams to innovate, adapt, and build competitive cars under constantly evolving technical regulations.\n\n\nsns.pairplot(data[['position', 'points', 'laps', 'grid', 'FinishCategory']], hue='FinishCategory', palette={'Podium':'steelblue', 'Points Finish':'lightskyblue', 'No Points': 'dodgerblue'})"
  },
  {
    "objectID": "technical-details/eda/main.html#circuit-features",
    "href": "technical-details/eda/main.html#circuit-features",
    "title": "Exploratory Data Analysis",
    "section": "Circuit Features",
    "text": "Circuit Features\n\ndata = pd.read_csv(\"../../data/processed-data/race_track_features.csv\")\n\n\n# Full Throttle (%) vs Number of Straights\n\n#  grouping based on the median of 'Number of Straights'\nmedian_straights = data['Number of Straights'].median()\n\n# tracks with fewer straights than median\nlow_straights = data[data['Number of Straights'] &lt; median_straights]['Full Throttle (%)']\n# tracks with &gt;= median straights\nhigh_straights = data[data['Number of Straights'] &gt;= median_straights]['Full Throttle (%)']\n\n# perform t-test\nt_stat, p_value = ttest_ind(low_straights, high_straights, nan_policy='omit')\n\n# resutls\nprint(f\"T-Statistic: {t_stat:.4f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\nT-Statistic: 8.7778\nP-Value: 0.0000\n\n\nThere is a significant difference in Full Throttle (%) between tracks with low and high straights.\n\n# Full Throttle (%) vs Number of Corners\n\nmedian_corners = data['Number of Corners'].median()\n\n# tracks with lower number of corners than median\nlow_corners = data[data['Number of Corners'] &lt; median_corners]['Full Throttle (%)']\n# tracks with &gt;= median corners\nhigh_corners = data[data['Number of Corners'] &gt;= median_corners]['Full Throttle (%)']\n\n# perform t-test\nt_stat, p_value = ttest_ind(low_corners, high_corners, nan_policy='omit')\n\n# results\nprint(f\"T-Statistic: {t_stat:.4f}\")\nprint(f\"P-Value: {p_value:.4f}\")\n\nT-Statistic: 6.8204\nP-Value: 0.0000\n\n\nThere is a significant difference in Full Throttle (%) between tracks with low and high corners.\n\nsns.pairplot(data[[\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]])\nplt.show()\n\n\n\n\n\n\n\n\n\ncorr_matrix = data[[\"Track Length (m)\", \"Max Speed (km/h)\", \"Full Throttle (%)\",\"Number of Corners\", \"Number of Straights\"]].corr()\ncorr_matrix\n\n\n\n\n\n\n\n\nTrack Length (m)\nMax Speed (km/h)\nFull Throttle (%)\nNumber of Corners\nNumber of Straights\n\n\n\n\nTrack Length (m)\n1.000000\n0.373616\n0.399017\n-0.058163\n-0.118008\n\n\nMax Speed (km/h)\n0.373616\n1.000000\n0.462422\n-0.246056\n-0.281559\n\n\nFull Throttle (%)\n0.399017\n0.462422\n1.000000\n-0.654084\n-0.667532\n\n\nNumber of Corners\n-0.058163\n-0.246056\n-0.654084\n1.000000\n0.631757\n\n\nNumber of Straights\n-0.118008\n-0.281559\n-0.667532\n0.631757\n1.000000\n\n\n\n\n\n\n\n\nsns.heatmap(corr_matrix, cmap=\"Blues_r\")\nplt.show()"
  },
  {
    "objectID": "technical-details/eda/main.html#pitstop-analysis",
    "href": "technical-details/eda/main.html#pitstop-analysis",
    "title": "Exploratory Data Analysis",
    "section": "Pitstop Analysis",
    "text": "Pitstop Analysis\n\ndata = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYear\nRound\nRaceName\nDriverID\nLap1\nLap2\nLap3\nLap4\nLap5\n...\nDuration1\nDuration2\nDuration3\nDuration4\nDuration5\nDuration6\nDuration7\nconstructorName\nposition\ngrid\n\n\n\n\n0\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n-1.492522\n-0.208201\n1.339923\n-0.281099\n-0.15023\n...\n0.471224\n0.733521\n1.882538\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nAlphaTauri\n11.0\n12.0\n\n\n1\n1\n2011\n1\nAustralian Grand Prix\nalonso\n-0.414215\n0.355923\n1.723412\n-0.281099\n-0.15023\n...\n-0.122753\n0.755727\n1.679973\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nFerrari\n4.0\n5.0\n\n\n2\n2\n2011\n1\nAustralian Grand Prix\nambrosio\n-0.218159\n0.976458\n-0.577525\n-0.281099\n-0.15023\n...\n0.204284\n0.896614\n-0.580401\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nMarussia\n14.0\n22.0\n\n\n3\n3\n2011\n1\nAustralian Grand Prix\nbarrichello\n-0.316187\n0.130273\n0.956433\n3.039918\n-0.15023\n...\n-0.058909\n1.835036\n0.998617\n3.860649\n-0.130754\n-0.060017\n-0.01976\nWilliams\n16.0\n17.0\n\n\n4\n4\n2011\n1\nAustralian Grand Prix\nbuemi\n-0.120131\n0.468747\n-0.577525\n-0.281099\n-0.15023\n...\n0.217802\n0.621420\n-0.580401\n-0.277274\n-0.130754\n-0.060017\n-0.01976\nAlphaTauri\n8.0\n10.0\n\n\n\n\n5 rows × 36 columns\n\n\n\n\nnumerical_columns = [\"Lap1\", \"Lap2\", \"Lap3\", \"Lap4\", \"Stop2\", \"Stop3\", \"Stop4\",\n                     \"Duration1\", \"Duration2\", \"Duration3\", \"Duration4\", \"position\", \"grid\"]\ncorrelation_data = data[numerical_columns]\n\n# Calculate correlation matrix\ncorrelation_matrix = correlation_data.corr()\n\nplt.figure(figsize=(12, 8))\nsns.heatmap(correlation_matrix, annot=True, cmap=\"Blues_r\", fmt=\".2f\", linewidths=0.5)\nplt.title(\"Correlation Heatmap\")\nplt.show()"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#principal-component-analysis-pca",
    "href": "technical-details/unsupervised-learning/main.html#principal-component-analysis-pca",
    "title": "Unsupervised Learning",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPCA transforms the data into a set of orthogonal components that capture the maximum variance in the data.\nProcess:\n\n\nThe data is standardized to ensure a consistent scale for all variables.\nCovariance matrix is computed to explore the dependencies between relationships.\nThe eigenvectors determine the directions (Principal Components), and the eigen values indicate the amount of variance captured.\nBased on the dimension, we want to reduce the data to, we select the eigenvectors that capture the most information.\n\n\n# import required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.cluster import Birch, KMeans, DBSCAN, AgglomerativeClustering, SpectralClustering\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster, set_link_color_palette\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\ndf_transformed = pd.read_csv(\"../../data/processed-data/pitstop.csv\")\n\n\ndf_transformed.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nYear\nRound\nRaceName\nDriverID\nLap1\nLap2\nLap3\nLap4\nLap5\n...\nTime5\nTime6\nTime7\nDuration1\nDuration2\nDuration3\nDuration4\nDuration5\nDuration6\nDuration7\n\n\n\n\n0\n0\n2011\n1\nAustralian Grand Prix\nalguersuari\n0.000000\n0.229730\n0.479452\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.453661\n0.428042\n0.457423\n0.000000\n0.0\n0.0\n0.0\n\n\n1\n1\n2011\n1\nAustralian Grand Prix\nalonso\n0.174603\n0.364865\n0.575342\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.392151\n0.432766\n0.419802\n0.000000\n0.0\n0.0\n0.0\n\n\n2\n2\n2011\n1\nAustralian Grand Prix\nambrosio\n0.206349\n0.513514\n0.000000\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.426017\n0.462739\n0.000000\n0.000000\n0.0\n0.0\n0.0\n\n\n3\n3\n2011\n1\nAustralian Grand Prix\nbarrichello\n0.190476\n0.310811\n0.383562\n0.512821\n0.0\n...\n0.0\n0.0\n0.0\n0.398762\n0.662386\n0.293259\n0.469108\n0.0\n0.0\n0.0\n\n\n4\n4\n2011\n1\nAustralian Grand Prix\nbuemi\n0.222222\n0.391892\n0.000000\n0.000000\n0.0\n...\n0.0\n0.0\n0.0\n0.427417\n0.404192\n0.000000\n0.000000\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 33 columns\n\n\n\n\n# features to be used for PCA\nfeatures = [\n    \"Lap1\", \"Lap2\", \"Lap3\", \"Lap4\", \"Lap5\", \"Lap6\", \"Lap7\",\n    \"Stop1\", \"Stop2\", \"Stop3\", \"Stop4\", \"Stop5\", \"Stop6\", \"Stop7\",\n    \"Time1\", \"Time2\", \"Time3\", \"Time4\", \"Time5\", \"Time6\", \"Time7\",\n    \"Duration1\", \"Duration2\", \"Duration3\", \"Duration4\", \"Duration5\", \"Duration6\", \"Duration7\"\n]\n# extract the features from the dataset\nX = df_transformed[features]\n\n\n\n# apply PCA\npca = PCA(n_components=28)\n# transofrm the data to the PCA space\nX_pca = pca.fit_transform(X)\n\n# Explained Variance Ratio for each PC\nexplained_variance_ratio = pca.explained_variance_ratio_\n# cummulative explained variance\ncumulative_explained_variance = np.cumsum(explained_variance_ratio)\n\n\n# Plot Explained Variance\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, marker='o', label='Explained Variance')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Number of Components', fontsize=14)\nplt.ylabel('Explained Variance Ratio', fontsize=14)\nplt.title('Explained Variance by Each Component', fontsize=14)\nplt.grid(True)\nplt.show()\n\n# Plot Cumulative Explained Variance\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, len(cumulative_explained_variance) + 1), cumulative_explained_variance, marker='o', label='Cumulative Variance')\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Number of Components', fontsize=14)\nplt.ylabel('Cumulative Explained Variance', fontsize=14)\nplt.title('Cumulative Explained Variance', fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExplained variance Plot:\n\nDisplays the amount of information (variance) each component captures from the original dataset.\nHigher variance indicates that the component captures more informations - making it more valuable for dimensionality reduction.\n\nCummulative Explained Variance:\n\nDisplays the cummulative sum of explained variance as each component is added.\nHelps us understand how many components are required to reach a desired variance threshold.\n\nInterpretation:\n\nBoth the plots are used to decide the number of components needed to capture the desired percentage of the total variance.\nWe can decide on the number of components based on the elbow of the plot or by setting a specific variance threshold we want to achieve.\nBy reducing dimensions to this optimal number, we can simplify the dataset while preserving most the original information, improiving computational.\n\n\n# Plot PCA results in 2D space using the first 2 principal components\nplt.figure(figsize=(8, 6))\nplt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"PCA Results\", fontsize=14)    \nplt.xlabel(\"PC1\", fontsize=14)\nplt.ylabel(\"PC2\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe PCA scatter plot shows the projection of the high-dimensional (28 features) onto th first 2 principal components.\nInterpretation:\n\nThe plot reveals clear clusters in the data, indicating that there are distinct patterns or groups among pit stop data.\nSince the features include lap numbers, pit stop counts, durations, and times, the clustering likely reflects drivers with similar pit stop strategies or lap time behavior. Difference in pit stop durations, frequency, and lap performance might have contributed to the visible groupings.\nThis PCA plot validates that the first two components effectively reduce the complexity of the data while preserving its structure.\n\nNext Steps:\nUse Clustering algorithms to formally identify groups in the lower dimensional space, these clusters can further be compared to the actual race outcomes – finish positions."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#t-sne",
    "href": "technical-details/unsupervised-learning/main.html#t-sne",
    "title": "Unsupervised Learning",
    "section": "t-SNE",
    "text": "t-SNE\nt-Distributed Stochastic Neighbor Embedding (t-SNE) is an unsupervised machine learning technique designed for non-linear dimensionality reduction, often used for visualizing high-dimensional data in lower dimensions.\nProcess:\n\nt-SNE begins by computing the pairwise similarities between data points in the original high-dimensional space. These similarities are modeled using probability distributions.\nTo preserve these relationships in the lower-dimensional space (e.g., 2D or 3D), t-SNE represents pairwise similarities using the Student-t distribution. This distribution, known for its heavier tails compared to Gaussian distributions, ensures that dissimilar points are pushed farther apart while similar points remain clustered together.\nThe algorithm aims to minimize the Kullback-Leibler (KL) divergence, a measure of difference between two probability distributions, ensuring the lower-dimensional embedding closely reflects the structure of the original space.\n\nHyperparameter: Perplexity\nPerplexity controls the balance between local and global relationships in the data. Lower perplexity values emphasize local patterns (small clusters), whereas higher perplexity values account for broader structures.\n\n# Apply t-SNE\n\n# n_components=2 = reduces the data to 2D space\n# random_state=123 = for reproducibility\n# perplexity=10 = hyperparameter - controls the number of neighbours considered\n# it influences the balance between local and gloabl structure preservation\ntsne = TSNE(n_components=2, random_state=123, perplexity=10)\n\n# apply t-SNE transformation\nX_tsne = tsne.fit_transform(X)\n\n# Plot t-SNE results\nplt.figure(figsize=(10, 7))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"t-SNE Results (perplexity = 10)\", fontsize=14)\nplt.xlabel(\"t-SNE-1\", fontsize=14)\nplt.ylabel(\"t-SNE-2\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# higher perplexity\ntsne = TSNE(n_components=2, random_state=123, perplexity=30)\nX_tsne = tsne.fit_transform(X)\n\n# Plot t-SNE results\n# Plot t-SNE results\nplt.figure(figsize=(10, 7))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"t-SNE Results (perplexity = 30)\", fontsize=14)\nplt.xlabel(\"t-SNE-1\", fontsize=14)\nplt.ylabel(\"t-SNE-2\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n# higher preplexity\ntsne = TSNE(n_components=2, random_state=123, perplexity=50)\nX_tsne = tsne.fit_transform(X)\n\n# Plot t-SNE results\nplt.figure(figsize=(10, 7))\nplt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.7)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.title(\"t-SNE Results (perplexity = 50)\", fontsize=14)\nplt.xlabel(\"t-SNE-1\", fontsize=14)\nplt.ylabel(\"t-SNE-2\", fontsize=14)\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation:\n\nPerplexity = 10:\n\n\nAt a lower perplexity, the data is more dispersed with distinct local clusters.\nSmaller groups of points are clearly formed, but there are scattered points and fewer global patterns.\n\n\nPerplexity = 30:\n\n\nSeveral well-defined clusters with clear separation between them.\nThe structure of the data is more interpretable as it highlights both local and global trends.\n\n\nPerplexity = 50:\n\n\nAt a higher perplexity, the algorithm focuses more on global relationships in the data. There is a smoother, broader organization of the data, but fine-grained patters are less visible.\n\nInsights:\n\nThe variation in perplexity directly affects the way the algorithm balances local and global relationships.\nThe presence of clearly separated clusters across all three plots suggests that the features contain inherent groupings."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#pca-vs-t-sne",
    "href": "technical-details/unsupervised-learning/main.html#pca-vs-t-sne",
    "title": "Unsupervised Learning",
    "section": "PCA vs t-SNE",
    "text": "PCA vs t-SNE\nPCA is particularly effective at preserving the overall global structure of the data, highlighting large-scale patterns based on variance across features. However, because it relies on linear projections, PCA struggles to capture complex or non-linear relationships within the data.\nt-SNE is well-suited for capturing the local structure of data. Unlike PCA, it is a non-linear technique that identifies clusters and relationships that may not be apparent through linear methods.\nIn summary, PCA and t-SNE are complementary techniques. PCA is ideal for global variance-based analysis and dimensionality reduction, while t-SNE is better for exploring hidden local structures and clusters. Together, they provide a robust approach for understanding complex datasets."
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#kmeans",
    "href": "technical-details/unsupervised-learning/main.html#kmeans",
    "title": "Unsupervised Learning",
    "section": "KMeans",
    "text": "KMeans\nApplied to the data reduced to 2D space using the PCA algorithm.\nKMeans is a distance-based unsupervised clustering algorithm that partitions a dataset into kkk clusters by minimizing the intra-cluster variance, also known as inertia. The algorithm iteratively refines cluster centroids to find an optimal partition of the data. KMeans aims to ensure that data points within a cluster are as close as possible to the centroid, while centroids of different clusters are as far apart as possible.\nProcess:\n\nInitialize k centroids randomly.\nAssign each data point to the nearest centroid based on a distance metric, typically Euclidean distance.\nRecalculate the centroids as the mean position of all points assigned to each cluster.\nRepeat the assignment and centroid update steps until convergence, i.e., when centroids no longer change significantly or a predefined number of iterations is reached.\n\nEvaluation Metrics Used\n\nInertia (Within-Cluster Sum of Squares): Measures the compactness of clusters by summing the squared distances between points and their cluster centroids. Lower inertia indicates higher cohesion.\nSilhouette Score: Measures how well each data point fits within its assigned cluster versus how far it is from neighboring clusters. The silhouette score ranges from -1 to 1, where higher values indicate better-defined clusters.\n\n\n# evaluate using elbow method and silhouette score\ndef kmeans_elbow_silhouette(X, cluster_range):\n\n    # initialize lists to store results\n    inertia_scores = []\n    silhouette_scores = []\n\n    # iterate over the range of cluster values\n    for k in cluster_range:\n        # initialize and fit KMeans\n        kmeans = KMeans(n_clusters=k, n_init=10, random_state=5000)\n        labels = kmeans.fit_predict(X)\n\n        # calculate inertia\n        inertia_scores.append(kmeans.inertia_)\n\n        # calculate silhouette score if there are more than 1 cluster\n        if len(set(labels)) &gt; 1:\n            silhouette_scores.append(silhouette_score(X, labels))\n        else:\n            silhouette_scores.append(-1)  # Assign a low score for invalid clusters\n\n    # plot Elbow Curve and Silhouette Score\n    fig, ax1 = plt.subplots(figsize=(10, 6))\n\n    # Elbow Curve - Inertia\n    ax1.plot(cluster_range, inertia_scores, 'blue', marker='o', label='Inertia')\n    ax1.set_xlabel(\"Number of Clusters (k)\", fontsize=14)\n    ax1.set_ylabel(\"Inertia\", color=\"blue\", fontsize=14)\n    ax1.tick_params(axis=\"y\", labelcolor=\"blue\", labelsize=14)\n\n    # Silhouette Score Curve - on the secondary y-axis\n    ax2 = ax1.twinx()\n    ax2.plot(cluster_range, silhouette_scores, 'darkslateblue', label='Silhouette Score', marker='o')\n    ax2.set_ylabel(\"Silhouette Score\", color='darkslateblue', fontsize=14)\n    ax2.tick_params(axis=\"y\", labelcolor='darkslateblue', labelsize=14)\n\n    # Title and grid\n    plt.title(\"Elbow Curve and Silhouette Score vs Number of Clusters\", fontsize=16)\n    plt.grid(True)\n    fig.tight_layout()\n    plt.show()\n\n# range of clusters to evaluate\ncluster_range = range(2, 11)  \nkmeans_elbow_silhouette(X, cluster_range)\n\n\n\n\n\n\n\n\nInterpretation\n\nElbow Method:\n\n\nThe Inertia curve (blue line) decreases sharply as the number of clusters increases, indicating that adding clusters reduces intra-cluster distances.\nHowever, the reduction in inertia slows down noticeably around k=4, forming an “elbow” in the curve. This point indicates that increasing the number of clusters beyond k=4 provides diminishing returns in terms of variance reduction.\n\n\nSilhouette Score:\n\n\nThe Silhouette Score curve (dark slate blue) evaluates cluster cohesion and separation. The score peaks at around k=6, reaching approximately 0.72, which is a strong indication of well-defined clusters.\nHowever, the silhouette score for k=4 is also high (~0.70), and beyond k=6, the score declines, suggesting a decrease in clustering quality.\n\nConclusion: - Based on the Elbow Method and Silhouette Score, k=4 is chosen as the optimal number of clusters. - This decision balances minimizing intra-cluster distances (inertia) and maintaining well-separated, cohesive clusters, while ensuring computational efficiency and interpretability.\n\n# kmeans with optimnal k (= 4)\nkmeans = KMeans(n_clusters=4, n_init=10, random_state=123)\nkmeans_labels = kmeans.fit_predict(X_pca)\n\n# Visualize clustering results on PCA-transformed data\ncustom_colors = [\"lightskyblue\", \"cornflowerblue\", \"blue\", \"steelblue\", \"deepskyblue\"]\ncustom_cmap = ListedColormap(custom_colors)\n\nplt.figure(figsize=(10, 6))\nscatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=kmeans_labels, alpha=0.6, cmap=custom_cmap)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.xlabel('Principal Component 1', fontsize=14)\nplt.ylabel('Principal Component 2', fontsize=14)\nplt.title(f'KMeans Clustering Results Visualized with PCA (k=4)', fontsize=14)\nplt.colorbar(scatter, label=\"Cluster Labels\")\nplt.grid(True)\nplt.show()\n\n# Evaluate silhouette score\nsil_score = silhouette_score(X_pca, kmeans_labels)\nprint(f\"Silhouette Score for KMeans on PCA-transformed Data: {sil_score:.3f}\")\n\n\n\n\n\n\n\n\nSilhouette Score for KMeans on PCA-transformed Data: 0.700\n\n\nThe clusters are well-separated, suggesting that KMeans has effectively grouped the data points.\n\n# analysing clusters\ndf_transformed['Cluster'] = kmeans_labels\ndf_transformed.drop(columns=['Unnamed: 0'], inplace=True)\nprint(df_transformed.head())\n\n\n   Year  Round               RaceName     DriverID      Lap1      Lap2  \\\n0  2011      1  Australian Grand Prix  alguersuari  0.000000  0.229730   \n1  2011      1  Australian Grand Prix       alonso  0.174603  0.364865   \n2  2011      1  Australian Grand Prix     ambrosio  0.206349  0.513514   \n3  2011      1  Australian Grand Prix  barrichello  0.190476  0.310811   \n4  2011      1  Australian Grand Prix        buemi  0.222222  0.391892   \n\n       Lap3      Lap4  Lap5  Lap6  ...  Time6  Time7  Duration1  Duration2  \\\n0  0.479452  0.000000   0.0   0.0  ...    0.0    0.0   0.453661   0.428042   \n1  0.575342  0.000000   0.0   0.0  ...    0.0    0.0   0.392151   0.432766   \n2  0.000000  0.000000   0.0   0.0  ...    0.0    0.0   0.426017   0.462739   \n3  0.383562  0.512821   0.0   0.0  ...    0.0    0.0   0.398762   0.662386   \n4  0.000000  0.000000   0.0   0.0  ...    0.0    0.0   0.427417   0.404192   \n\n   Duration3  Duration4  Duration5  Duration6  Duration7  Cluster  \n0   0.457423   0.000000        0.0        0.0        0.0        3  \n1   0.419802   0.000000        0.0        0.0        0.0        3  \n2   0.000000   0.000000        0.0        0.0        0.0        0  \n3   0.293259   0.469108        0.0        0.0        0.0        1  \n4   0.000000   0.000000        0.0        0.0        0.0        0  \n\n[5 rows x 33 columns]\n\n\n\n# descriptive statistics - Laps\ncluster_summary = df_transformed.groupby('Cluster').describe()\n\nselected_features = ['Lap1', 'Lap2']\n\nfiltered_summary = cluster_summary[selected_features]\n\nfiltered_summary.T\n\n\n\n\n\n\n\n\nCluster\n0\n1\n2\n3\n\n\n\n\nLap1\ncount\n2023.000000\n409.000000\n1673.000000\n1013.000000\n\n\nmean\n0.218472\n0.101409\n0.361332\n0.147025\n\n\nstd\n0.123682\n0.103965\n0.166313\n0.101554\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.142857\n0.015873\n0.253968\n0.063492\n\n\n50%\n0.206349\n0.095238\n0.365079\n0.142857\n\n\n75%\n0.285714\n0.158730\n0.476190\n0.206349\n\n\nmax\n0.841270\n0.698413\n1.000000\n0.555556\n\n\nLap2\ncount\n2023.000000\n409.000000\n1673.000000\n1013.000000\n\n\nmean\n0.486132\n0.238188\n0.000000\n0.345655\n\n\nstd\n0.141812\n0.161187\n0.000000\n0.141362\n\n\nmin\n0.027027\n0.027027\n0.000000\n0.027027\n\n\n25%\n0.405405\n0.108108\n0.000000\n0.256757\n\n\n50%\n0.472973\n0.243243\n0.000000\n0.351351\n\n\n75%\n0.567568\n0.337838\n0.000000\n0.418919\n\n\nmax\n1.000000\n0.837838\n0.000000\n0.851351\n\n\n\n\n\n\n\n\n# descriptive statistics - Duration\ncluster_summary = df_transformed.groupby('Cluster').describe()\n\nselected_features = ['Duration2', 'Duration1']\n\nfiltered_summary = cluster_summary[selected_features]\n\nfiltered_summary.T\n\n\n\n\n\n\n\n\nCluster\n0\n1\n2\n3\n\n\n\n\nDuration2\ncount\n2023.000000\n409.000000\n1673.000000\n1013.000000\n\n\nmean\n0.412609\n0.359939\n0.000000\n0.404845\n\n\nstd\n0.107918\n0.152168\n0.000000\n0.116942\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.384980\n0.321482\n0.000000\n0.373274\n\n\n50%\n0.409424\n0.372697\n0.000000\n0.405452\n\n\n75%\n0.440264\n0.429564\n0.000000\n0.442862\n\n\nmax\n1.000000\n0.990551\n0.000000\n0.974559\n\n\nDuration1\ncount\n2023.000000\n409.000000\n1673.000000\n1013.000000\n\n\nmean\n0.405394\n0.381307\n0.420707\n0.387143\n\n\nstd\n0.098226\n0.137291\n0.079428\n0.126414\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.372940\n0.339512\n0.374256\n0.357289\n\n\n50%\n0.397548\n0.380361\n0.402523\n0.391071\n\n\n75%\n0.430698\n0.431769\n0.455364\n0.434923\n\n\nmax\n0.956469\n0.987722\n1.000000\n0.931372"
  },
  {
    "objectID": "technical-details/unsupervised-learning/main.html#introduction-1",
    "href": "technical-details/unsupervised-learning/main.html#introduction-1",
    "title": "Unsupervised Learning",
    "section": "Introduction",
    "text": "Introduction\nClustering techniques identify hidden patterns or natural groupings within data, providing critical insights without requiring labeled information. It is widely applied in data analysis to explore relationships, reduce dimensionality, and uncover actionable insights.\nClusters are characterized by two main properties:\nCohesion:\nMeasures how closely related data points are within the same cluster. High cohesion indicates that the points within a cluster are similar to each other. Typically evaluated using intra-cluster distances, where a lower distance implies higher cohesion.\nSeparation:\nMeasures how distinct or well-separated a cluster is from other clusters. High separation indicates clear boundaries between clusters. Typically evaluated using inter-cluster distances, where larger distances signify better separation.\nTypes of Clustering Methods\n\nDistance-Based Clustering\n\nRelies on distance measures (e.g., Euclidean distance) to form clusters.\nK-Means Clustering: Partitions data into k clusters by minimizing the variance (intra-cluster distance) within each cluster.Efficient for large datasets but assumes convex-shaped clusters and requires the number of clusters (k) in advance.\nHierarchical Clustering: Builds a hierarchy of clusters represented as a tree or dendrogram.\n\nAgglomerative (bottom-up): Starts with individual points and merges clusters iteratively.\nDivisive (top-down): Starts with one cluster and splits it iteratively.\n\n\nDensity-Based Clustering\n\nGroups points based on areas of high point density and identifies sparse regions as noise (outliers).\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise): Groups closely packed points into clusters while marking points in low-density regions as noise. Does not require specifying the number of clusters and works well for arbitrarily shaped clusters.\nOPTICS (Ordering Points To Identify the Clustering Structure): Similar to DBSCAN but can handle clusters of varying densities.\n\nModel-Based Clustering\n\nAssumes that data is generated from an underlying probabilistic model, often involving distributions.\nGaussian Mixture Models (GMM): Assumes data is generated from a mixture of several Gaussian distributions with unknown parameters. Clusters are modeled as probabilistic regions based on the Gaussian components.\n\nGrid-Based Clustering\n\nDivides the data space into a grid structure and performs clustering on the grid cells.\nCLIQUE (Clustering In QUEst): Efficiently handles high-dimensional data by partitioning the data space into a grid and identifying dense regions.\n\nSpectral Clustering\n\nUses graph theory and the eigenvalues of a similarity matrix to transform data into a lower-dimensional space.\nClustering algorithms like K-Means are then applied in this reduced space.\nEffective for non-convex and complex-shaped clusters.\n\n\nEvaluation Metrics\n\nInertia (Within-Cluster Sum of Squares): Measures the compactness of clusters by summing the squared distances between points and their cluster centroids. Lower inertia indicates higher cohesion.\nSilhouette Score: Measures how well each data point fits within its assigned cluster versus how far it is from neighboring clusters. The silhouette score ranges from -1 to 1, where higher values indicate better-defined clusters.\nDavies-Bouldin Index: Evaluates both intra-cluster distances (cohesion) and inter-cluster distances (separation). A lower Davies-Bouldin Index indicates better clustering.\nCalinski-Harabasz Index: Measures the ratio of inter-cluster separation to intra-cluster cohesion. Higher values indicate better clustering results."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#correlation-analysis",
    "href": "technical-details/supervised-learning/main.html#correlation-analysis",
    "title": "Supervised Learning",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nCorrelation analysis helps identify features that are strongly correlated with the target variable.\n\nplt.figure(figsize=(10, 6))\ncorrelations = X.corrwith(y)\ncorrelations.sort_values().plot(kind='barh', color='deepskyblue')\nplt.title(\"Correlation Between Features and Target\")\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"Correlation Coefficient\", fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.grid(axis='x')\nplt.show()"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#recursive-feature-elimination",
    "href": "technical-details/supervised-learning/main.html#recursive-feature-elimination",
    "title": "Supervised Learning",
    "section": "Recursive Feature Elimination",
    "text": "Recursive Feature Elimination\nRFE is a wrapper method that iteratively removes the least important features based on a model’s performance.\nProcess:\n\nTrain a machine learning model.\nRank features based on their importance or weights.\nRecursively eliminate the least significant feature(s) and retrain the model.\nStop when the desired number of features is selected.\n\n\n# inititlize logistic regression model\nmodel = LogisticRegression(max_iter=1000, random_state=123)\n# initialize RFE with the logistic regression model \nrfe = RFE(model, n_features_to_select=5)\n# fit RFE on data\nrfe.fit(X, y)\n# generate rankings for the features\n# 1 = most important \n# higher ranks indicate lower importance\nrfe_ranking = pd.Series(rfe.ranking_, index=features).sort_values()\n\nplt.figure(figsize=(10, 6))\nrfe_ranking.plot(kind='barh', color='deepskyblue')\nplt.title(\"Feature Importance by Recursive Feature Elimination (RFE)\", fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.xlabel(\"RFE Ranking (Lower is Better)\", fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.grid(axis='x')\nplt.show()"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#mutual-information",
    "href": "technical-details/supervised-learning/main.html#mutual-information",
    "title": "Supervised Learning",
    "section": "Mutual Information",
    "text": "Mutual Information\nMutual information measures the dependency between two variables. It identifies the most informative features by quantifying how much knowing one variable reduces uncertainty about the other\n\n# compute MI between each features and y\nmutual_info = mutual_info_classif(X, y, random_state=123)\nmutual_info_series = pd.Series(mutual_info, index=features).sort_values(ascending=False)\n\nplt.figure(figsize=(10, 6))\nmutual_info_series.plot(kind='barh', color='deepskyblue')\nplt.title(\"Feature Importance by Mutual Information\", fontsize=14)\nplt.yticks(fontsize=14)\nplt.xticks(fontsize=14)\nplt.ylabel(\"Feature\", fontsize=14)\nplt.xlabel(\"Mutual Information Score\", fontsize=14)\nplt.grid(axis='x')\nplt.show()\n\n\n\n\n\n\n\n\nSelected Features: grid,Duration1, Duration2, Lap1, Lap2, Lap3,Stop2, Stop3\nThese features have been consistenly selected as important by the Feature Selection Algorithms."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#decision-trees",
    "href": "technical-details/supervised-learning/main.html#decision-trees",
    "title": "Supervised Learning",
    "section": "Decision Trees",
    "text": "Decision Trees\nA Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks. It operates by recursively splitting the dataset based on the most informative features, represented as a tree-like structure of decisions. Each node in the tree represents a feature, branches represent decisions or splits, and the leaves represent the final outcome.\nProcess:\n\nDecision Trees identify the best feature for splitting the data at each step. Splits are made to maximize homogeneity within the resulting subsets.\n\nFor classification, splitting is often based on measures like Gini Impurity or Entropy (Information Gain)\nFor regression, the split minimizes the Mean Squared Error (MSE).\n\nRecursive Partitioning: The algorithm continues to split the data until a stopping condition is met, such as reaching a maximum depth, minimum samples in a node, or no further gain from splitting.\nPrediction:\n\nIn classification, predictions are based on the majority class in the leaf node.\nIn regression, predictions are based on the average value in the leaf node.\n\n\nHyperparameters:\n\nmax_depth: Limits the depth of the tree. A smaller depth reduces overfitting but may underfit the data.\nmin_samples_split: The minimum number of samples required to split a node. Increasing this value reduces overfitting by preventing small, irrelevant splits.\nmin_samples_leaf: Specifies the minimum number of samples a leaf node must contain. Larger values result in smoother decision boundaries.\ncriterion: Determines the measure of impurity.\nmax_features: Limits the number of features considered at each split, reducing overfitting and computational cost.\nmax_leaf_nodes: Limits the number of leaf nodes in the tree\n\nEvaluation Metrics:\n\nAccuracy: Measures the percentage of correctly predicted instances.\nPrecision: Fraction of true positive predictions out of all positive predictions. \\[ Precision = \\frac{TP}{TP + FP}\\]\nRecall (Sensitivity): Fraction of actual positives correctly identified. \\[ Recall = \\frac{TP}{TP + FN}\\]\nF1-Score: Harmonic mean of precision and recall, useful when dealing with imbalanced classes. \\[ F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\\]\nConfusion Matrix: Provides a matrix of predicted vs. actual class labels, offering insights into false positives, false negatives, etc.\nROC-AUC Score: Evaluates the trade-off between true positive rate and false positive rate.\n\nChallenges\n\nTrees tend to overfit when they grow too deep, capturing noise in the data.\nBias to Greedy Splits: Decision Trees use a greedy approach, which may lead to suboptimal splits.\nInstability: Small changes in the data can result in significant changes in the tree structure\n\n\ndf_transformed = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\n# create a new categortical variable - 1 if its a points finish\n#                                    - 0, otherwise\ndf_transformed['points_category'] = df_transformed['position'].apply(lambda x: 1 if x &lt;= 10 else 0)\n\n\n# Selected features\nX = df_transformed[['grid', 'Duration1', 'Duration2', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3']]\ny = df_transformed['points_category']  \n\n# Train-test split\n# splitting the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n# Initialize the Decision Tree Classifier\n# training with default values\ndt = DecisionTreeClassifier(random_state=42)\n\n# Fit the model\ndt.fit(X_train, y_train)\n\n# Predict on test set\ny_pred = dt.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report - Decision Tree:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(dt, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Decision Tree\")\nplt.show()\n\nClassification Report - Decision Tree:\n              precision    recall  f1-score   support\n\n           0       0.64      0.69      0.67       114\n           1       0.71      0.66      0.68       128\n\n    accuracy                           0.67       242\n   macro avg       0.67      0.67      0.67       242\nweighted avg       0.68      0.67      0.67       242\n\n\n\n\n\n\n\n\n\n\n\nThe model performs similarly for both classes, indicated by similar recall, and f-1 scores.\n67% accuracy indcates that the model can be imporoved further with hyperparameter tuning.\nThere is no significant class imbalance.\n\n\n# Hyperparameter Tuning\n# Define max_depth range\nmax_depth_range = range(1, 25)\n\n# Store metrics for each max_depth\ntrain_accuracies_y1 = []  # Accuracy for y=1\ntest_accuracies_y1 = []\ntrain_recalls_y0 = []     # Recall for y=0\ntest_recalls_y0 = []\n\ntrain_accuracies = []     # Overall accuracy\ntest_accuracies = []\ntrain_recalls = []        # Overall recall for y=1\ntest_recalls = []\n\nfor max_depth in max_depth_range:\n    # Initialize Decision Tree Classifier\n    dt = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n    dt.fit(X_train, y_train)\n    \n    # Predictions on training and test data\n    y_train_pred = dt.predict(X_train)\n    y_test_pred = dt.predict(X_test)\n    \n    # Calculate overall accuracy\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n    \n    # Calculate recall for y=1\n    train_recalls.append(recall_score(y_train, y_train_pred, pos_label=1))\n    test_recalls.append(recall_score(y_test, y_test_pred, pos_label=1))\n    \n    # Accuracy for y=1\n    train_accuracies_y1.append(accuracy_score(y_train[y_train == 1], y_train_pred[y_train == 1]))\n    test_accuracies_y1.append(accuracy_score(y_test[y_test == 1], y_test_pred[y_test == 1]))\n    \n    # Recall for y=0\n    train_recalls_y0.append(recall_score(y_train, y_train_pred, pos_label=0))\n    test_recalls_y0.append(recall_score(y_test, y_test_pred, pos_label=0))\n\n# Plotting all metrics in a (2, 2) grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Subplot 1: Overall Accuracy\naxes[0, 0].plot(max_depth_range, train_accuracies, 'o-', label='Training', color='blue')\naxes[0, 0].plot(max_depth_range, test_accuracies, 'o-', label='Test', color='deepskyblue')\naxes[0, 0].set_title(\"Accuracy\")\naxes[0, 0].set_xlabel(\"Max Depth\")\naxes[0, 0].set_ylabel(\"Accuracy\")\naxes[0, 0].legend()\naxes[0, 0].grid(True)\n\n# Subplot 2: Recall for y=1\naxes[1, 0].plot(max_depth_range, train_recalls, 'o-', label='Training', color='blue')\naxes[1, 0].plot(max_depth_range, test_recalls, 'o-', label='Test', color='deepskyblue')\naxes[1, 0].set_title(\"Recall for y=1\")\naxes[1, 0].set_xlabel(\"Max Depth\")\naxes[1, 0].set_ylabel(\"Recall\")\naxes[1, 0].legend()\naxes[1, 0].grid(True)\n\n# Subplot 3: Recall for y=0\naxes[1, 1].plot(max_depth_range, train_recalls_y0, 'o-', label='Training', color='blue')\naxes[1, 1].plot(max_depth_range, test_recalls_y0, 'o-', label='Test', color='deepskyblue')\naxes[1, 1].set_title(\"Recall for y=0\")\naxes[1, 1].set_xlabel(\"Max Depth\")\naxes[1, 1].set_ylabel(\"Recall\")\naxes[1, 1].legend()\naxes[1, 1].grid(True)\n\nfig.delaxes(axes[0, 1])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nAs the depth of the tree increases, the training accuracy of the model increases and the test accuracy of the model decreases. This is because the model starts to overfit the training data. To avoid overfitting of the model, we select the optimal number of layers when the test and training accuracy are high. In this case, we select the max depth to be 5. Beyond 5, the test accruacy significantly drops.\n\n# Hyper parameter tuning\n# fix max_depth = 5\n# define the parameter grid \nparam_grid = {\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 5]\n}\n\n# Perform Grid Search with max_depth fixed as 5 \ngrid_search = GridSearchCV(DecisionTreeClassifier(max_depth=5, random_state=123), \n                           param_grid, cv=5, scoring='accuracy')\n# fit the model to the training data\ngrid_search.fit(X_train, y_train)\n\n# results\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# evaluate the best model\nbest_dt = grid_search.best_estimator_\ny_pred_best = best_dt.predict(X_test)\n\n# evaluation metrics\nprint(\"Classification Report - Optimized Decision Tree with max_depth=5:\")\nprint(classification_report(y_test, y_pred_best))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(best_dt, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Decision Tree with max_depth=5\")\nplt.show()\n\nBest Parameters: {'min_samples_leaf': 2, 'min_samples_split': 5}\nClassification Report - Optimized Decision Tree with max_depth=5:\n              precision    recall  f1-score   support\n\n           0       0.81      0.75      0.78       114\n           1       0.79      0.84      0.82       128\n\n    accuracy                           0.80       242\n   macro avg       0.80      0.79      0.80       242\nweighted avg       0.80      0.80      0.80       242\n\n\n\n\n\n\n\n\n\n\n\nAfter hyperparameter tuning, we observe a significant increase in accuracy.\nThe precision for the turned model is also more balanced than the original model.\n\n\n# ROC Curve for the Initial Decision Tree\ny_proba_initial = dt.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_initial, tpr_initial, _ = roc_curve(y_test, y_proba_initial)\nroc_auc_initial = auc(fpr_initial, tpr_initial)\n\n# ROC Curve for the Optimized Decision Tree\ny_proba_optimized = best_dt.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_optimized, tpr_optimized, _ = roc_curve(y_test, y_proba_optimized)\nroc_auc_optimized = auc(fpr_optimized, tpr_optimized)\n\n# Plot both ROC curves\nplt.figure(figsize=(10, 6))\nplt.plot(fpr_initial, tpr_initial, label=f'Initial Decision Tree (AUC = {roc_auc_initial:.2f})', color='deepskyblue', lw=2)\nplt.plot(fpr_optimized, tpr_optimized, label=f'Optimized Decision Tree (AUC = {roc_auc_optimized:.2f})', color='blue', lw=2)\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  \nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves - Decision Tree')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe Receiver Operating Characteristic (ROC) curve is used to evaluate the trade-off between True Positive Rate (TPR) (sensitivity/recall) and False Positive Rate (FPR) across different threshold values.\nArea Under The Curve (AUC) represents the measure of separability. Higher the AUC, the better is the model at predicting the classes.\nThe optimized Decision Tree outperforms the initial model, as evidenced by the higher AUC (0.84 vs. 0.79). Hyperparameter tuning has led to improvements in the model’s performance."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest",
    "href": "technical-details/supervised-learning/main.html#random-forest",
    "title": "Supervised Learning",
    "section": "Random Forest",
    "text": "Random Forest\nRandom Forests combine the strengths of multiple decision trees to deliver a robust, versatile, and high-performing model for classification and regression tasks.\nProcess:\n\nRandom Forest builds multiple decision trees using bootstrap aggregation, where each tree is trained on a bootstrapped subset of the data.\nAt each node, a random subset of features is considered for splitting, introducing diversity among the trees and reducing overfitting.\nEach tree is grown independently to its maximum depth or based on stopping criteria like max_depth or min_samples_split.\nFor classification tasks, the final prediction is determined through a majority vote from all trees. For regression tasks, the final output is obtained by averaging the predictions of all trees.\nBy combining outputs from multiple decision trees, Random Forest reduces variance, improves generalization, and enhances model robustness.\n\n\ndf_transformed = pd.read_csv(\"../../data/processed-data/pitstop_with_positions.csv\")\ndf_transformed['points_category'] = df_transformed['position'].apply(lambda x: 1 if x &lt;= 10 else 0)\nX = df_transformed[['grid', 'Duration1', 'Duration2', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3']]\ny = df_transformed['points_category']  \n\n# Train-test split\n# splitting the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n# Initialize Random Forest Classifier\nrf = RandomForestClassifier(random_state=123)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Predict\ny_pred = rf.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(rf, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Random Forest\")\nplt.show()\n\n# Accuracy Score\nprint(\"Accuracy:\", accuracy_score(y_test, y_pred))\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.79      0.76      0.78       114\n           1       0.80      0.82      0.81       128\n\n    accuracy                           0.79       242\n   macro avg       0.79      0.79      0.79       242\nweighted avg       0.79      0.79      0.79       242\n\n\n\n\n\n\n\n\n\n\nAccuracy: 0.7933884297520661\n\n\nThe Random Forest Classifier, using its default hyperparameters, achieved an accuracy of approximately 0.80. Comparable to a Decision Tree after hyperparameter tuning, this highlights the superior performance and robustness of Random Forests, especially when handling large datasets, as they effectively reduce overfitting compared to a single decision tree.\n\n# hyperparameter tuning\nparam_grid = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],    \n    'min_samples_split': [2, 5, 10]\n}\n\n# grid search\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(random_state=123),\n                           param_grid=param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train, y_train)\n\n# results\nprint(\"Best Parameters:\", grid_search.best_params_)\n\n# evaluate with best parameters\nbest_rf = grid_search.best_estimator_\ny_pred_best = best_rf.predict(X_test)\n\n# Classification Report\nprint(\"Classification Report - Optimized Random Forest:\")\nprint(classification_report(y_test, y_pred_best))\n\nBest Parameters: {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 200}\nClassification Report - Optimized Random Forest:\n              precision    recall  f1-score   support\n\n           0       0.80      0.78      0.79       114\n           1       0.81      0.83      0.82       128\n\n    accuracy                           0.81       242\n   macro avg       0.81      0.80      0.80       242\nweighted avg       0.81      0.81      0.81       242\n\n\n\n\n# Confusion Matrix for Optimized Random Forest\nConfusionMatrixDisplay.from_estimator(best_rf, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Random Forest\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# ROC Curve for the Initial Random Forest\ny_proba_initial = rf.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_initial, tpr_initial, _ = roc_curve(y_test, y_proba_initial)\nroc_auc_initial = auc(fpr_initial, tpr_initial)\n\n# ROC Curve for the Optimized Random Forest\ny_proba_optimized = best_rf.predict_proba(X_test)[:, 1]  # Probability for the positive class\nfpr_optimized, tpr_optimized, _ = roc_curve(y_test, y_proba_optimized)\nroc_auc_optimized = auc(fpr_optimized, tpr_optimized)\n\n# Plot ROC Curves\nplt.figure(figsize=(10, 6))\nplt.plot(fpr_initial, tpr_initial, label=f'Initial Random Forest (AUC = {roc_auc_initial:.2f})', color='deepskyblue', lw=2)\nplt.plot(fpr_optimized, tpr_optimized, label=f'Optimized Random Forest (AUC = {roc_auc_optimized:.2f})', color='blue', lw=2)\nplt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=1)  # Diagonal line for random guessing\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curves - Random Forest')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe Random Forest Classifier has outperformed the Decision Tree in terms of accuracy, demonstrating a higher capability to classify race outcomes more effectively using the pit stop data.\nThis also suggests that race outcomes are significantly influenced by pit stop strategies, highlighting an important insight that can help predict potential race winners."
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#random-forest-classifier",
    "href": "technical-details/supervised-learning/main.html#random-forest-classifier",
    "title": "Supervised Learning",
    "section": "Random Forest Classifier",
    "text": "Random Forest Classifier\n\n# Create a new column with binned classes based on 'position'\n# multiclass categorical variable\ndef position_to_class(position):\n    if 1 &lt;= position &lt;= 5:\n        return 1\n    elif 6 &lt;= position &lt;= 10:\n        return 2\n    elif 11 &lt;= position &lt;= 15:\n        return 3\n    elif 16 &lt;= position &lt;= 20:\n        return 4\n    else:\n        return None  \n\ndf_transformed['position_class'] = df_transformed['position'].apply(position_to_class)\n\ndf_transformed = df_transformed.dropna(subset=['position_class'])\n\n# Define features and target\nX = df_transformed[['grid', 'Duration1', 'Duration2', 'Lap1', 'Lap2', 'Lap3', 'Stop2', 'Stop3']]\ny = df_transformed['position_class']\n\n\n# Train-test split\n# splitting the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)\n\n\n# Initialize Random Forest Classifier\nrf_multi = RandomForestClassifier(random_state=123)\n\n# Train the model\nrf_multi.fit(X_train, y_train)\n\n# Predict\ny_pred_multi = rf_multi.predict(X_test)\n\n# Evaluation Metrics\nprint(\"Classification Report - Multi-class Random Forest:\")\nprint(classification_report(y_test, y_pred_multi))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(rf_multi, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Multi-class Random Forest\")\nplt.show()\n\nClassification Report - Multi-class Random Forest:\n              precision    recall  f1-score   support\n\n           1       0.65      0.80      0.72        65\n           2       0.39      0.41      0.40        63\n           3       0.43      0.46      0.44        61\n           4       0.57      0.28      0.38        46\n\n    accuracy                           0.51       235\n   macro avg       0.51      0.49      0.48       235\nweighted avg       0.51      0.51      0.49       235\n\n\n\n\n\n\n\n\n\n\n\n# Hyperparameter tuning\nparam_grid_multi = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10]\n}\n\n# perform Grid Search\ngrid_search_multi = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n                                 param_grid=param_grid_multi, cv=5, scoring='accuracy')\ngrid_search_multi.fit(X_train, y_train)\n\n# results\nprint(\"Best Parameters:\", grid_search_multi.best_params_)\n\n# evaluate with best parameters\nbest_rf_multi = grid_search_multi.best_estimator_\ny_pred_best_multi = best_rf_multi.predict(X_test)\n\n# classification report\nprint(\"Classification Report - Optimized Multi-class Random Forest:\")\nprint(classification_report(y_test, y_pred_best_multi))\n\n# Confusion Matrix\nConfusionMatrixDisplay.from_estimator(best_rf_multi, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - Optimized Multi-class Random Forest\")\nplt.show()\n\nBest Parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\nClassification Report - Optimized Multi-class Random Forest:\n              precision    recall  f1-score   support\n\n           1       0.66      0.75      0.71        65\n           2       0.38      0.40      0.39        63\n           3       0.44      0.51      0.47        61\n           4       0.58      0.30      0.40        46\n\n    accuracy                           0.51       235\n   macro avg       0.52      0.49      0.49       235\nweighted avg       0.51      0.51      0.50       235"
  },
  {
    "objectID": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn",
    "href": "technical-details/supervised-learning/main.html#k-nearest-neighbors-knn",
    "title": "Supervised Learning",
    "section": "K-Nearest Neighbors (KNN)",
    "text": "K-Nearest Neighbors (KNN)\nKNN is a non-parametric and instance-based learning algorithm, meaning it makes no assumptions about the underlying data distribution and makes predictions based on the similarity of the input data to its neighbors.\nProcess:\n\nThe algorithm calculates the distance between the input (test) data point and all training data points.\nBased on the distance metric, KNN identifies the k-nearest neighbors to the test data point.\nPrediction:\n\nClassification: The algorithm assigns the class label that is the majority among the k-nearest neighbors (majority voting).\nRegression: The algorithm averages the values of the k-nearest neighbors to predict the outcome.\nHyperparameters: K – number of neighbors.\n\n\n\n# Range of k values to test\nk_values = range(1, 25)\n\n# Lists to store training and test accuracy\ntrain_accuracies = []\ntest_accuracies = []\n\nfor k in k_values:\n    # Initialize KNN with k neighbors\n    knn = KNeighborsClassifier(n_neighbors=k)\n    \n    # Train the model\n    knn.fit(X_train, y_train)\n    \n    # Predict on training and test data\n    y_train_pred = knn.predict(X_train)\n    y_test_pred = knn.predict(X_test)\n    \n    # Calculate accuracy for training and test data\n    train_accuracies.append(accuracy_score(y_train, y_train_pred))\n    test_accuracies.append(accuracy_score(y_test, y_test_pred))\n\n# Plot accuracy vs. k value\nplt.figure(figsize=(10, 6))\nplt.plot(k_values, train_accuracies, label=\"Training Accuracy\", marker='o', color='blue')\nplt.plot(k_values, test_accuracies, label=\"Test Accuracy\", marker='o', color='deepskyblue')\nplt.xticks(k_values)\nplt.xlabel(\"Number of Neighbors (k)\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy vs. Number of Neighbors (k)\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nOptimal number for k is 19. Beyond 19, the test accuracy starts to drop.\n\n# Initialize KNN with k=19\nknn = KNeighborsClassifier(n_neighbors=19)\n\n# Train the model\nknn.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = knn.predict(X_test)\n\n# Evaluate Performance\nprint(\"Classification Report - KNN (k=19):\")\nprint(classification_report(y_test, y_pred))\n\n# Plot Confusion Matrix\nConfusionMatrixDisplay.from_estimator(knn, X_test, y_test, cmap=\"Blues\")\nplt.title(\"Confusion Matrix - KNN (k=19)\")\nplt.show()\n\nClassification Report - KNN (k=19):\n              precision    recall  f1-score   support\n\n           1       0.68      0.80      0.74        65\n           2       0.41      0.52      0.46        63\n           3       0.44      0.39      0.42        61\n           4       0.40      0.22      0.28        46\n\n    accuracy                           0.51       235\n   macro avg       0.49      0.48      0.47       235\nweighted avg       0.49      0.51      0.49       235\n\n\n\n\n\n\n\n\n\n\nAcross models, the first class is consistently classified with the highest accuracy. This indicates that as the granularity of race results increases, classifying exact outcomes based solely on pit stop data becomes more challenging. While it is relatively easier to determine whether a driver will score points or not, accurately predicting the precise finishing position proves to be much more difficult."
  },
  {
    "objectID": "technical-details/supervised-learning/SupervisedLearning.html",
    "href": "technical-details/supervised-learning/SupervisedLearning.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nSupervised Learning is a branch of Machine Learning where models are trained on labeled data. The primary goal of supervised learning is to learn a mapping function that can predict outputs for unseen data accurately. This approach relies on historical data to identify patterns and relationships, enabling it to generalize well on new, unseen data.\nProcess:\n\nThe model is provided with input-output pairs from historical data. Each data point consists of:\n\nFeatures (X): Independent variables or predictors.\nLabels (Y): Target outputs or dependent variables.\n\nThe model learns the relationship between the input features and corresponding outputs using a loss function that measures prediction error. The goal is to minimize this error.\nOnce trained, the model can predict outputs for new inputs.\nPerformance is evaluated using metrics like accuracy, precision, recall, F1-score for classification tasks, or RMSE, MAE for regression tasks.\n\nTypes of Supervised Learning\n\nClassification:\n\n\nThe goal is to categorize inputs into discrete classes or categories.\nAlgorithms: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Random Forest, Naive Bayes, K-Nearest Neighbors (KNN).\n\n\nRegression:\n\n\nThe goal is to predict a continuous output based on input features.\nAlgorithms: Linear Regression, Polynomial Regression, Ridge and Lasso Regression, Support Vector Regression (SVR).\n\nChallenges:\n\nSupervised learning requires large volumes of labeled data, which can be expensive and time-consuming to obtain.\nModels may memorize training data instead of generalizing to new inputs. Regularization and cross-validation techniques help mitigate this.\nIn classification problems, imbalanced classes can affect performance.\n\nIn this project:\nSupervised Learning Algorithms are applied to classify race results and predict pit stop duration."
  },
  {
    "objectID": "technical-details/eda/WordClouds.html",
    "href": "technical-details/eda/WordClouds.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Analysis\nMax Verstappen\n\n\n\n\n\n\n\n\n\n\nLeading airlines in terms of flight volume are Southwest Airlines, Skywest Airlines, and Delta Airlines.\n\nHawaiian Airlines, Forntier Airlines, and Allegiant Air have a smaller footprint.\n\nSince the analysis is on data collected from December 2020, the pandemic could have influenced these numbers, specifically the smaller airlines."
  },
  {
    "objectID": "technical-details/data-cleaning/DataCleaning.html",
    "href": "technical-details/data-cleaning/DataCleaning.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nRaw data collected from various sources is rarely in a format ready for analysis. It often contains inconsistencies, missing values, duplicates, and irrelevant information, which can hinder the analytical process and lead to inaccurate or biased results. Data Cleaning is the process of transforming this messy data into a structured, consistent, and reliable format, suitable for extracting meaningful insights and applying models effectively.\nImportance:\n\nData cleaning ensures that the data is complete, accurate, and consistent, significantly enhancing the reliability of insights derived from the analysis and the performance of models built using the data. High-quality data forms the foundation of robust models and predictions.\nMissing values in the dataset can introduce bias or distort the analysis. Addressing these gaps through techniques like imputation (filling in missing values using statistical or logical methods) or removal helps maintain the integrity of results.\nDuplicates in the dataset can overrepresent certain patterns, while outliers may distort metrics and affect model accuracy. Identifying and appropriately handling these anomalies ensures the analysis remains valid and unbiased.\nWhen data is collected from multiple sources, differences in format, naming conventions, and measurement units can cause inconsistencies. Standardizing these elements across all datasets ensures that the data can be seamlessly integrated and analyzed as a whole.\n\nIn this project:\nData was collected from a variety of sources, Data Cleaning was an essential step. Each raw dataset underwent a tailored cleaning process designed to fit its specific structure and use case. These processes will be discussed in detail within the respective sections dedicates to each dataset."
  },
  {
    "objectID": "technical-details/eda/EDA.html",
    "href": "technical-details/eda/EDA.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The primary goal of Exploratory Data Analysis (EDA) is to gain a comprehensive understanding of the data, its quality, identify any underlying trends or relationships that may influence the analysis and modeling process.\nImportance:\n\nEDA helps detect outliers, and inconsistencies in the data, addressing these issues ensures that the analysis is based on accurate and clean data.\nThrough visualizations and statistical analysis, EDA identifies which features are most relevant to the questions and should be included in the model.\nEDA reveals relationships between variables, such as correlations, and trends, which can help discard certain features or create new derived features.\nBy analyzing the distribution of variables, we can identify central tendencies and variations. This helps us understand how the data is spread out and it it meets assumptions required by certain algorithms.\n\nTechniques:\n\nUnivariate Analysis:\n\nTo understand the distribution, and central tendency of a variable.\nVisualizations: Histograms, Box Plots, kernel Density plots.\nStatistical measures: Mean, Median, Mode, Variance, Standard Deviation.\n\nBivariate Analysis:\n\nTo understand the relationship between two variables\nCategorical-Categorical: Heatmaps.\nNumerical-Categorical: Box plots, Bar Graphs.\nNumerical-Numerical: Scatter plots, correlation coefficients.\n\nMultivariate Analysis:\n\nTo explore relationships between three or more variables simultaneously.\nVisualizations: Heatmaps, Scatter plots.\nDimensionality Reduction: PCA, t-SNE.\n\nData Distribution:\n\nHistograms and density plots are useful to detect skewness or multimodal distributions.\n\nStatistical Analysis:\n\nHypothesis Testing: t-tests, chi-square tests, ANOVA.\nCorrelation: Pearson, Spearman correlation coefficients\nSummary Statistics: Mean, variance, Quartiles.\n\n\nIn this project:\nThere are various datasets, and mutiple independent features such as: Driver Performance, Pit stop durations, race track features, overall performance of the car, which play an important role in determining the outcome of the race. Therefore, EDA is crucial to understand ow these factors influence race results."
  },
  {
    "objectID": "technical-details/eda/EDA.html#introduction",
    "href": "technical-details/eda/EDA.html#introduction",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "The primary goal of Exploratory Data Analysis (EDA) is to gain a comprehensive understanding of the data, its quality, identify any underlying trends or relationships that may influence the analysis and modeling process.\nImportance:\n\nEDA helps detect outliers, and inconsistencies in the data, addressing these issues ensures that the analysis is based on accurate and clean data.\nThrough visualizations and statistical analysis, EDA identifies which features are most relevant to the questions and should be included in the model.\nEDA reveals relationships between variables, such as correlations, and trends, which can help discard certain features or create new derived features.\nBy analyzing the distribution of variables, we can identify central tendencies and variations. This helps us understand how the data is spread out and it it meets assumptions required by certain algorithms.\n\nTechniques:\n\nUnivariate Analysis:\n\nTo understand the distribution, and central tendency of a variable.\nVisualizations: Histograms, Box Plots, kernel Density plots.\nStatistical measures: Mean, Median, Mode, Variance, Standard Deviation.\n\nBivariate Analysis:\n\nTo understand the relationship between two variables\nCategorical-Categorical: Heatmaps.\nNumerical-Categorical: Box plots, Bar Graphs.\nNumerical-Numerical: Scatter plots, correlation coefficients.\n\nMultivariate Analysis:\n\nTo explore relationships between three or more variables simultaneously.\nVisualizations: Heatmaps, Scatter plots.\nDimensionality Reduction: PCA, t-SNE.\n\nData Distribution:\n\nHistograms and density plots are useful to detect skewness or multimodal distributions.\n\nStatistical Analysis:\n\nHypothesis Testing: t-tests, chi-square tests, ANOVA.\nCorrelation: Pearson, Spearman correlation coefficients\nSummary Statistics: Mean, variance, Quartiles.\n\n\nIn this project:\nThere are various datasets, and mutiple independent features such as: Driver Performance, Pit stop durations, race track features, overall performance of the car, which play an important role in determining the outcome of the race. Therefore, EDA is crucial to understand ow these factors influence race results."
  },
  {
    "objectID": "technical-details/eda/points_driver.html",
    "href": "technical-details/eda/points_driver.html",
    "title": "DSAN-5000: Project",
    "section": "",
    "text": "Insights:\n\nA significant portion of the total championship points for most drivers has been earned while driving for top-performing teams such as Ferrari and Red Bull.\nThere is a clear correlation between the points scored in a season and the team a driver represented, emphasizing the importance of team performance in a driver’s success.\nObserving the career trajectories of drivers like Daniel Riccardo, Kimi Räikkönen, and Sebastian Vettel, we notice a pattern where they initially drove for top teams, followed by a move to midfield teams before retiring.\nSome drivers switched teams despite scoring a substantial number of championship points in the previous season. This highlights the strategic decisions made by teams in planning their driver line-ups and the drivers’ foresights regarding the potential improvements and competitiveness of cars across the grid."
  },
  {
    "objectID": "technical-details/unsupervised-learning/UnsupervisedLearning.html",
    "href": "technical-details/unsupervised-learning/UnsupervisedLearning.html",
    "title": "Introduction",
    "section": "",
    "text": "Unsupervised Learning is a branch of Machine Learning that involves using algorithms to analyze unlabeled data. Unlike supervised learning, which relies on input-output pairs, unsupervised learning does not have a predefined outcomes to predict. Instead, it focuses on discovering hidden patterns, clusters, or structures within the data without explicit instructions.\nUnsupervised Learning Algorithms:\n\nClustering: Grouping data based on their similarity\nDimensionality Reduction: reducing the number of features in the dataset while preserving key information.\nAssociation Rule Learning: Identifying relationships between variables in a dataset. In this project, Dimensionality reduction and Clustering algorithms are used, which will be explored in detail in their respective section.\n\nImportance\n\nUnsupervised learning algorithms can identify natural groupings in the data, revealing insights that might not be obvious to humans.\nTechniques like dimensionality reduction simplify data by reducing its features while retaining important information, making it easier to visualize and analyze.\nLabeling data could be a costly process, unsupervised learning provides a scalable approach to uncover patterns without requiring human intervention.\n\nIn this project:\n\n\nDimensionality reduction is applied to pit stop data to simplify the dataset and enhance the interpretability of subsequent analysis.\nClustering is employed to:\n\nAnalyze pit stop strategies and uncover potential groupings.\nGroup circuits based on their features."
  },
  {
    "objectID": "technical-details/unsupervised-learning/UnsupervisedLearning.html#principal-component-analysis-pca",
    "href": "technical-details/unsupervised-learning/UnsupervisedLearning.html#principal-component-analysis-pca",
    "title": "Introduction",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPCA transforms the data into a set of orthogonal components that capture the maximum variance in the data.\nProcess:\n\n\nThe data is standardized to ensure a consistent scale for all variables.\nCovariance matrix is computed to explore the dependencies between relationships.\nThe eigenvectors determine the directions (Principal Components), and the eigen values indicate the amount of variance captured.\nBased on the dimension, we want to reduce the data to, we select the eigenvectors that capture the most information."
  },
  {
    "objectID": "technical-details/llm-usage-log.html#data-collection",
    "href": "technical-details/llm-usage-log.html#data-collection",
    "title": "LLM usage log",
    "section": "Data Collection",
    "text": "Data Collection\n\nSearch for APIs and Websites for data - list of sources\n\nThis page can serve as a “catch-all” for LLM use cases that don’t involve content creation, such as reformatting your own ideas, commenting code that you wrote, or proofreading text, PDF summarization.\nLLM tools were used in the following way for the tasks below"
  },
  {
    "objectID": "technical-details/llm-usage-log.html#writing-1",
    "href": "technical-details/llm-usage-log.html#writing-1",
    "title": "LLM usage log",
    "section": "Writing:",
    "text": "Writing:\n\nReformating text from bulleted lists into proses\nProofreading\nText summarization for literature review"
  },
  {
    "objectID": "index.html#testing",
    "href": "index.html#testing",
    "title": "Introduction",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  },
  {
    "objectID": "index.html#f1-analysis",
    "href": "index.html#f1-analysis",
    "title": "Introduction",
    "section": "",
    "text": "Audio instructions:\nIf you want, you can listen to the instructions:\n\n\n\n Source: Text-to-speech conversion done with Amazon Polly on AWS \nNote: These audio instructions should not be included in your final submission or repository, once you are done wiht them, please delete the files and remove them from the website."
  }
]