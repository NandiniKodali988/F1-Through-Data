{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: true\n",
    "        embed-resources: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include DataCollection.qmd >}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sending HTTP requests\n",
    "import requests\n",
    "# for data manipulations \n",
    "import pandas as pd\n",
    "# for numerical computations and array manipulations\n",
    "import numpy as np\n",
    "# for parsing and manipulating JSON data\n",
    "import json\n",
    "# for interacting with the operating system\n",
    "import os\n",
    "# for pattern matching, cleaning and extracting text\n",
    "import re\n",
    "# for working with dates and times\n",
    "from datetime import datetime\n",
    "import time\n",
    "# for parsing and extracting data from HTML and XML documents\n",
    "from bs4 import BeautifulSoup\n",
    "# for reading and writing to csv files\n",
    "import csv\n",
    "# for car telemetry and results\n",
    "import fastf1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Race Information \n",
    "\n",
    "**Importance**: Contains information about the season, round, date, grand prix name, location, results,wikipedia url of the race, driver and constructor details. \n",
    "\n",
    "-  Season: The year of the race.\n",
    "- Round: The number of the race in the season.\n",
    "- Grand Prix Name: The official name of the race.\n",
    "- Results: Includes the finishing poistion and points earned. \n",
    "- Driver Details: Provides information about the driver's name, ID, nationality, and date of birth. \n",
    "- Constructor Details: Constructor refers to the team that builds and maintains the cars. It contains information about the constructor's name, ID, and nationality. \n",
    "  \n",
    "**Source**: ERGAST API\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funtion to get race results for a specifi year\n",
    "def get_race_results(url, year, offset, limit=1000):\n",
    "    full_url = f\"{url}/{year}/results.json?limit={limit}&offset={offset}\"\n",
    "    # GET request\n",
    "    result = requests.get(full_url)\n",
    "    # output = JSON object\n",
    "    return result.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 2023 season\n",
    "season_2023_json = get_race_results(url='http://ergast.com/api/f1', year=2023, offset=0)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('../../data/raw-data/race_data_2023.json', 'w') as outfile:\n",
    "    json.dump(season_2023_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting data from 2000 to 2022\n",
    "# function to loop through years and fetch the results\n",
    "def race_data(start_year, end_year, output_dr, url):\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # results for the current year\n",
    "        race_data = get_race_results(url, year, offset=0)\n",
    "        # save the output \n",
    "        output_file = os.path.join(output_dr, f\"race_data_{year}.json\")\n",
    "        # save the data to a JSON file\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(race_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call race_data()\n",
    "race_data(\n",
    "    start_year = 2000,\n",
    "    end_year = 2009,\n",
    "    output_dr = \"../../data/raw-data\",\n",
    "    url = 'http://ergast.com/api/f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Driver Standings\n",
    " After each round, drivers earn points based on their final position. These points are added to their overall tally, and the driver with the most points at the end of the season wins the World Driver’s Championship (WDC). \n",
    "\n",
    "**Importance**:  Contains total points earned by each driver in every season from 2000 to 2023. It is crucial for identifying trends in driver performance over the years.  \n",
    "\n",
    "**Source**: ERGAST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch driver standings for a specific year\n",
    "def driverstanding_info(url, season):\n",
    "    # construct the URL \n",
    "    full_url = f\"{url}/{season}/driverStandings.json\"\n",
    "    # GET request\n",
    "    response = requests.get(full_url)\n",
    "    return response.json()\n",
    "\n",
    "# Function to fetch and save all driver standings for the given seasons\n",
    "def driverstandings_info(start_year, end_year, output_file, url=\"http://ergast.com/api/f1\"):\n",
    "    # for storing driver standings data\n",
    "    driver_standings_data = {}\n",
    "    \n",
    "    # loop through each year \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        # results for the current year\n",
    "        data = driverstanding_info(url, year)\n",
    "        # add the data with year as key\n",
    "        driver_standings_data[year] = data\n",
    "    \n",
    "    # Save to output file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(driver_standings_data, outfile)\n",
    "\n",
    "# Call the function for seasons 2000–2023\n",
    "driverstandings_info(\n",
    "    start_year=2000,\n",
    "    end_year=2023,\n",
    "    output_file=\"../../data/raw-data/driver_standings/driver_standings_2000_2023.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Information\n",
    "\n",
    " The race tracks are referred to as circuits.  \n",
    "\n",
    "**Importance**: This data includes the circuit name, locality, country as well as its longitude and latitude. These values can be used for collecting weather information on the race day.\n",
    "\n",
    "**Source**: ERGAST API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit_info(output_file, url):\n",
    "    results = requests.get(url)\n",
    "    \n",
    "    # save to output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results.json(), f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_info(output_file='../../data/raw-data/circuit_data.json', url = \"http://ergast.com/api/f1/circuits.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News of Top 10 Drivers\n",
    "Standings in 2024 season as on 11-24-2024\n",
    "\n",
    "**Silly Season** in F1 refers to the period of speculation, rumors, and announcements surrounding driver lineups for the next season. This period typically begins during the latter half of the season, as drivers, teams, and sponsors negotiate deals for the future.  Headlines during the silly season often speculate on whether drivers will extend their contract, switch teams, or retire from the sport, creating a buzz that fuels media interest. \n",
    "\n",
    "**Importance**: Analyzing news coverage about drivers can provide insights into their career trajectories, and potential moves in the upcoming season. \n",
    "\n",
    "**Source**: NEWS API\n",
    "\n",
    "**Resources**: [NEWS-API DSAN 5000 Lecture Content](https://jfh.georgetown.domains/centralized-lecture-content/content/data-science/data-collection/share/API-newapi/news-api.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://newsapi.org/v2/everything?\"\n",
    "total_requests=2\n",
    "verbose=True\n",
    "\n",
    "API_KEY='86d4dac5a4864ece92da90bc31277e53'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to fetch news articles for a given topic\n",
    "def news_data(topic, API_KEY, total_requests=1, verbose=True):\n",
    "    baseURL = \"https://newsapi.org/v2/everything?\"\n",
    "\n",
    "    # API parameters\n",
    "    URLpost = {\n",
    "        'apiKey': API_KEY,      # API key\n",
    "        'q': '+'+topic,         # topic\n",
    "        'sotBy': 'relevancy',   # sort by relevance\n",
    "        'pageSize': 100,        # maximum articles per pages = 100\n",
    "        'page': 1               # start with page 1\n",
    "    }\n",
    "    # last name of the drives to avoid spaces in the file names\n",
    "    file_name = topic.split()[-1]\n",
    "    # initialize a list to store all articles\n",
    "    all_articles = []\n",
    "\n",
    "    # loop through the number of API requests\n",
    "    for request_num in range(total_requests):\n",
    "        # make the API request\n",
    "        response = requests.get(baseURL, params=URLpost)\n",
    "        response_data = response.json()\n",
    "        # extract artivles from the response\n",
    "        articles = response_data.get('articles', [])\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "        # increment the page number for next request\n",
    "        URLpost['page'] += 1\n",
    "\n",
    "\n",
    "    # output file path\n",
    "    output_dr = \"../../data/raw-data/News_Drivers\"\n",
    "    output_file = os.path.join(output_dr, f\"{file_name}_raw_text.json\")\n",
    "\n",
    "    # save to output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_articles, f, indent=4)\n",
    "    \n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Drivers as of Round 22 (Las Vegas Grand Prix)\n",
    "\n",
    "1. Max Verstappen\n",
    "2. Lando Norris\n",
    "3. Charles Leclerc\n",
    "4. Oscar Piastri\n",
    "5. Carlos Sainz\n",
    "6. George Russell\n",
    "7. Lewis Hamilton\n",
    "8. Sergio Perez\n",
    "9. Fernando Alonso\n",
    "10. Nico Hulkenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "text_data = news_data('Max Verstappen', API_KEY, total_requests=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = news_data('Lando Norris', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Charles Leclerc', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Oscar Piastri', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Carlos Sainz', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('George Russell', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Lewis Hamilton', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Sergio Perez', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Fernando Alonso', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Nico Hulkenberg', API_KEY, total_requests=1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data\n",
    "\n",
    "**Importance**: Weather conditions play a crucial role in race strategy, tire choices, and driver performance. \n",
    "\n",
    "**Source**: Wikipedia\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the url for each race is in the race data collected using ergast API\n",
    "race_df = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data = race_df[['season', 'raceName', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data = race_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>raceName</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>Bahrain Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Bahrain_Gran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010</td>\n",
       "      <td>Australian Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Australian_G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2010</td>\n",
       "      <td>Malaysian Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Malaysian_Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2010</td>\n",
       "      <td>Chinese Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Chinese_Gran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2010</td>\n",
       "      <td>Spanish Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Spanish_Gran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    season               raceName  \\\n",
       "0     2010     Bahrain Grand Prix   \n",
       "24    2010  Australian Grand Prix   \n",
       "48    2010   Malaysian Grand Prix   \n",
       "72    2010     Chinese Grand Prix   \n",
       "96    2010     Spanish Grand Prix   \n",
       "\n",
       "                                                  url  \n",
       "0   http://en.wikipedia.org/wiki/2010_Bahrain_Gran...  \n",
       "24  http://en.wikipedia.org/wiki/2010_Australian_G...  \n",
       "48  http://en.wikipedia.org/wiki/2010_Malaysian_Gr...  \n",
       "72  http://en.wikipedia.org/wiki/2010_Chinese_Gran...  \n",
       "96  http://en.wikipedia.org/wiki/2010_Spanish_Gran...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_weather_from_wikipedia(url):\n",
    "    # GET request\n",
    "    response = requests.get(url)\n",
    "    # parse the HTML content\n",
    "    bs = BeautifulSoup(response.text, 'html.parser')  \n",
    "    \n",
    "    # locate the infobox table\n",
    "    table = bs.find('table', {'class': 'infobox infobox-table vevent'})\n",
    "    if not table:\n",
    "        print(f\"No infobox found on the page: {url}\")\n",
    "        return \"Not Available\"\n",
    "    \n",
    "    # iterate through each row for the \"Weather\" field \n",
    "    for row in table.find_all('tr'):\n",
    "        # find the header cell with class 'infobox-label'\n",
    "        header = row.find('th', {'class': 'infobox-label'})  \n",
    "        # check if it contains \"Weather\"\n",
    "        if header and 'Weather' in header.text: \n",
    "            # find the corresponding data cell with class 'infobox-data'\n",
    "            data = row.find('td', {'class': 'infobox-data'})  \n",
    "            # remove any unnecessary spaces\n",
    "            if data:\n",
    "                return data.text.strip()  \n",
    "    \n",
    "# column in the dataframe to store weather information   \n",
    "race_data['weather'] = None\n",
    "\n",
    "# fetch weather information for each URL\n",
    "for index, row in race_data.iterrows():\n",
    "    url = row['url']\n",
    "    # for debugging purpose\n",
    "    #print(f\"Fetching weather for: {url}\")\n",
    "    \n",
    "    # get the weather information\n",
    "    weather = get_weather_from_wikipedia(url)\n",
    "    \n",
    "    # update the weather column\n",
    "    race_data.at[index, 'weather'] = weather\n",
    "\n",
    "# save to output file\n",
    "output_csv = \"../../data/raw-data/weather/race_data_with_weather.csv\"\n",
    "race_data.to_csv(output_csv, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circuit Features\n",
    "\n",
    "Around the F1 season, circuits vary widely in their features—some are known for tight, technical corners, others for long, high-speed straights, and a few for their narrow and challenging layouts. These unique characteristics influence car and driver performance significantly, with certain drivers or car designs excelling on specific track types.\n",
    "\n",
    "**Importance**: Analyzing racetrack features is crucial for understanding how different teams and drivers perform under varying conditions. This information can be used to classify tracks, which can further be studied to identify patterns and trends in race results. \n",
    "\n",
    "**Source**: fastf1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.makedirs('cache', exist_ok=True)\n",
    "#fastf1.Cache.enable_cache('cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an empty list\n",
    "track_data = []\n",
    "\n",
    "# function to fetch data for specific year and race\n",
    "def extract_track_features(year, race_name):\n",
    "    # load session data for the qualifying session\n",
    "    session = fastf1.get_session(year, race_name, 'Q') \n",
    "    session.load()\n",
    "\n",
    "    # fastest lap\n",
    "    fastest_lap = session.laps.pick_fastest()\n",
    "    telemetry = fastest_lap.get_telemetry()\n",
    "\n",
    "    # track Length\n",
    "    track_length = telemetry['Distance'].iloc[-1]  # Distance of the fastest lap\n",
    "\n",
    "    # max Speed\n",
    "    max_speed = telemetry['Speed'].max()\n",
    "\n",
    "    # average Speed\n",
    "    avg_speed = track_length / fastest_lap['LapTime'].total_seconds()\n",
    "\n",
    "    # percentage of Full Throttle (throttle >= 95%)\n",
    "    full_throttle = telemetry[telemetry['Throttle'] >= 95]\n",
    "    perc_full_throttle = (len(full_throttle) / len(telemetry)) * 100\n",
    "\n",
    "    # calcute the number of corners based on telemetry speed (< 100 km/h)\n",
    "    telemetry['is_corner'] = telemetry['Speed'] < 100\n",
    "    num_corners = (telemetry['is_corner'] & ~telemetry['is_corner'].shift(1, fill_value=False)).sum()\n",
    "\n",
    "    # calculate the number of straights based on telemetry speed (> 150 km/h)\n",
    "    telemetry['is_straight'] = telemetry['Speed'] > 150\n",
    "    num_straights = (telemetry['is_straight'] & ~telemetry['is_straight'].shift(1, fill_value=False)).sum()\n",
    "\n",
    "    return {\n",
    "        \"Year\": year,\n",
    "        \"Grand Prix\": race_name,\n",
    "        \"Track Length (m)\": track_length,\n",
    "        \"Max Speed (km/h)\": max_speed,\n",
    "        \"Full Throttle (%)\": perc_full_throttle,\n",
    "        \"Number of Corners\": num_corners,\n",
    "        \"Number of Straights\": num_straights\n",
    "    }\n",
    "\n",
    "# for test\n",
    "# data was extracted for seasons: 2018 - 2023\n",
    "# data is available from 2018 season onwards in fastf1\n",
    "year = 2023\n",
    "schedule = fastf1.get_event_schedule(year)\n",
    "\n",
    "# iterate through all races in the schedule\n",
    "for _, event in schedule.iterrows():  \n",
    "    if not pd.isna(event['Session1']):  \n",
    "        try:\n",
    "            track_features = extract_track_features(year, event['EventName'])\n",
    "            track_data.append(track_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for {event['EventName']} in {year}: {e}\")\n",
    "\n",
    "df_tracks = pd.DataFrame(track_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging all racetrack features into a single csv\n",
    "folder_path = \"../../data/raw-data/circuit_data/\"\n",
    "\n",
    "dataframes = []\n",
    "\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith('.csv'): \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        df = pd.read_csv(file_path)\n",
    "        dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "output_file = \"../../data/raw-data/circuit_data/merged_circuit_features.csv\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "merged_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pitstop data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data available from 2011\n",
    "\n",
    "# Function to fetch pitstop data for a specific race\n",
    "def get_pitstop_data(year, round_number):\n",
    "    url = f\"http://ergast.com/api/f1/{year}/{round_number}/pitstops.json?limit=1000\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200 and response.text.strip():\n",
    "        try:\n",
    "            return response.json()\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing JSON for {year} Round {round_number}: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for {year} Round {round_number}: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "# Function to fetch race schedule\n",
    "def get_race_schedule(year):\n",
    "    url = f\"http://ergast.com/api/f1/{year}.json\"\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json().get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n",
    "    else:\n",
    "        print(f\"Failed to fetch schedule for {year}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Function to extract and save pitstop data to CSV\n",
    "def fetch_and_save_pitstop_data(start_year, end_year, output_csv):\n",
    "    # Create the CSV file and write the header\n",
    "    with open(output_csv, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = [\"Year\", \"Round\", \"RaceName\", \"DriverID\", \"Lap\", \"Stop\", \"Time\", \"Duration\"]\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Loop through years and races\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            print(f\"Fetching data for year: {year}\")\n",
    "            races = get_race_schedule(year)\n",
    "            \n",
    "            for race in races:\n",
    "                round_number = race.get(\"round\")\n",
    "                race_name = race.get(\"raceName\")\n",
    "                print(f\"Processing {race_name} (Round {round_number}) in {year}\")\n",
    "\n",
    "                # Fetch pitstop data for this race\n",
    "                pitstop_data = get_pitstop_data(year, round_number)\n",
    "                if pitstop_data:\n",
    "                    races_list = pitstop_data.get(\"MRData\", {}).get(\"RaceTable\", {}).get(\"Races\", [])\n",
    "                    \n",
    "                    # Ensure there is race data\n",
    "                    if races_list:\n",
    "                        pitstops = races_list[0].get(\"PitStops\", [])\n",
    "                        \n",
    "                        # Write each pitstop to the CSV\n",
    "                        for pitstop in pitstops:\n",
    "                            writer.writerow({\n",
    "                                \"Year\": year,\n",
    "                                \"Round\": round_number,\n",
    "                                \"RaceName\": race_name,\n",
    "                                \"DriverID\": pitstop.get(\"driverId\"),\n",
    "                                \"Lap\": pitstop.get(\"lap\"),\n",
    "                                \"Stop\": pitstop.get(\"stop\"),\n",
    "                                \"Time\": pitstop.get(\"time\"),\n",
    "                                \"Duration\": pitstop.get(\"duration\")\n",
    "                            })\n",
    "                    else:\n",
    "                        print(f\"No race data available for {race_name} in {year}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_csv = \"../../data/raw-data/pitstop_data.csv\"\n",
    "fetch_and_save_pitstop_data(\n",
    "    start_year=2000,\n",
    "    end_year=2023,\n",
    "    output_csv=output_csv\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include closing.qmd >}} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
