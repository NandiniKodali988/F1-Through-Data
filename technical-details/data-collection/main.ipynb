{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "Ensure that the code is well-commented to enhance readability and understanding for others who may review or use it. If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "This page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Country     Population\n",
      "0                                  World  8,119,000,000\n",
      "1                                  China  1,409,670,000\n",
      "2                          1,404,910,000          17.3%\n",
      "3                          United States    335,893,238\n",
      "4                              Indonesia    281,603,800\n",
      "..                                   ...            ...\n",
      "235                   Niue (New Zealand)          1,681\n",
      "236                Tokelau (New Zealand)          1,647\n",
      "237                         Vatican City            764\n",
      "238  Cocos (Keeling) Islands (Australia)            593\n",
      "239                Pitcairn Islands (UK)             35\n",
      "\n",
      "[240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Send a request to Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the page content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the table containing the data (usually the first table for such lists)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Extract data from the table rows\n",
    "countries = []\n",
    "populations = []\n",
    "\n",
    "# Iterate over the table rows\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:\n",
    "        country = cells[1].text.strip()  # The country name is in the second column\n",
    "        population = cells[2].text.strip()  # The population is in the third column\n",
    "        countries.append(country)\n",
    "        populations.append(population)\n",
    "\n",
    "# Step 5: Create a DataFrame to store the results\n",
    "data = pd.DataFrame({\n",
    "    'Country': countries,\n",
    "    'Population': populations\n",
    "})\n",
    "\n",
    "# Display the scraped data\n",
    "print(data)\n",
    "\n",
    "# Optionally save to CSV\n",
    "data.to_csv('../../data/raw-data/countries_population.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include closing.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race Information for 2000-2023 Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_results(url, year, offset, limit=1000):\n",
    "    full_url = f\"{url}/{year}/results.json?limit={limit}&offset={offset}\"\n",
    "    result = requests.get(full_url)\n",
    "    return result.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 2023 season\n",
    "season_2023_json = get_race_results(url='http://ergast.com/api/f1', year=2023, offset=0)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('../../data/raw-data/race_data_2023.json', 'w') as outfile:\n",
    "    json.dump(season_2023_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting data from 2000 to 2022\n",
    "\n",
    "# function to loop through years and fetch the results\n",
    "def race_data(start_year, end_year, output_dr, url):\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        race_data = get_race_results(url, year, offset=0)\n",
    "        # save the output \n",
    "        output_file = os.path.join(output_dr, f\"race_data_{year}.json\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(race_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call race_data()\n",
    "race_data(\n",
    "    start_year = 2000,\n",
    "    end_year = 2009,\n",
    "    output_dr = \"../../data/raw-data\",\n",
    "    url = 'http://ergast.com/api/f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Standings for 2000-2023 Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driverstanding_info(url, season):\n",
    "    full_url = f\"{url}/{season}/driverStandings.json\"\n",
    "    response = requests.get(full_url)\n",
    "    return response.json()\n",
    "\n",
    "# Function to fetch and save all driver standings for the given seasons\n",
    "def driverstandings_info(start_year, end_year, output_file, url=\"http://ergast.com/api/f1\"):\n",
    "    driver_standings_data = {}\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        data = driverstanding_info(url, year)\n",
    "        driver_standings_data[year] = data\n",
    "    \n",
    "    # Save to output file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(driver_standings_data, outfile)\n",
    "\n",
    "# Call the function for seasons 2000–2023\n",
    "driverstandings_info(\n",
    "    start_year=2000,\n",
    "    end_year=2023,\n",
    "    output_file=\"../../data/raw-data/driver_standings/driver_standings_2000_2023.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Information for 2000-2023 Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit_info(output_file, url):\n",
    "    results = requests.get(url)\n",
    "    \n",
    "    # save to output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results.json(), f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_info(output_file='../../data/raw-data/circuit_data.json', url = \"http://ergast.com/api/f1/circuits.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News of Top 10 drivers in 2024 season (so far)\n",
    "- Using News-API\n",
    "- Resources: https://jfh.georgetown.domains/centralized-lecture-content/content/data-science/data-collection/share/API-newapi/news-api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://newsapi.org/v2/everything?\"\n",
    "total_requests=2\n",
    "verbose=True\n",
    "\n",
    "API_KEY='86d4dac5a4864ece92da90bc31277e53'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_data(topic, API_KEY, total_requests=1, verbose=True):\n",
    "    baseURL = \"https://newsapi.org/v2/everything?\"\n",
    "\n",
    "    # API parameters\n",
    "    URLpost = {\n",
    "        'apiKey': API_KEY,\n",
    "        'q': '+'+topic,\n",
    "        'sotBy': 'relevancy',\n",
    "        'pageSize': 100,\n",
    "        'page': 1\n",
    "    }\n",
    "    # last name of the drives to avoid spaces in the file names\n",
    "    file_name = topic.split()[-1]\n",
    "    all_articles = []\n",
    "\n",
    "    # make an API request \n",
    "    for request_num in range(total_requests):\n",
    "        response = requests.get(baseURL, params=URLpost)\n",
    "        response_data = response.json()\n",
    "\n",
    "        articles = response_data.get('articles', [])\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "        URLpost['page'] += 1\n",
    "\n",
    "\n",
    "    # output file path\n",
    "    output_dr = \"../../data/raw-data/News_Drivers\"\n",
    "    output_file = os.path.join(output_dr, f\"{file_name}_raw_text.json\")\n",
    "\n",
    "    # save to output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_articles, f, indent=4)\n",
    "    \n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top 10 Drivers as of Round 22 (Las Vegas Grad Prix)\n",
    "1. Max Verstappen\n",
    "2. Lando Norris\n",
    "3. Charles Leclerc\n",
    "4. Oscar Piastri\n",
    "5. Carlos Sainz\n",
    "6. George Russell\n",
    "7. Lewis Hamilton\n",
    "8. Sergio Perez\n",
    "9. Fernando Alonso\n",
    "10. Nico Hulkenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "text_data = news_data('Max Verstappen', API_KEY, total_requests=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = news_data('Lando Norris', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Charles Leclerc', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Oscar Piastri', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Carlos Sainz', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('George Russell', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Lewis Hamilton', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Sergio Perez', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Fernando Alonso', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Nico Hulkenberg', API_KEY, total_requests=1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
