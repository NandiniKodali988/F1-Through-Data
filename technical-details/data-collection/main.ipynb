{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Data Collection\"\n",
    "format:\n",
    "    html: \n",
    "        code-fold: false\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include instructions.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "{{< include overview.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include methods.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code \n",
    "\n",
    "Provide the source code used for this section of the project here.\n",
    "\n",
    "If you're using a package for code organization, you can import it at this point. However, make sure that the **actual workflow steps**—including data processing, analysis, and other key tasks—are conducted and clearly demonstrated on this page. The goal is to show the technical flow of your project, highlighting how the code is executed to achieve your results.\n",
    "\n",
    "Ensure that the code is well-commented to enhance readability and understanding for others who may review or use it. If relevant, link to additional documentation or external references that explain any complex components. This section should give readers a clear view of how the project is implemented from a technical perspective.\n",
    "\n",
    "This page is a technical narrative, NOT just a notebook with a collection of code cells, include in-line Prose, to describe what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "In the following code, we first utilized the requests library to retrieve the HTML content from the Wikipedia page. Afterward, we employed BeautifulSoup to parse the HTML and locate the specific table of interest by using the find function. Once the table was identified, we extracted the relevant data by iterating through its rows, gathering country names and their respective populations. Finally, we used Pandas to store the collected data in a DataFrame, allowing for easy analysis and visualization. The data could also be optionally saved as a CSV file for further use. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 Country     Population\n",
      "0                                  World  8,119,000,000\n",
      "1                                  China  1,409,670,000\n",
      "2                          1,404,910,000          17.3%\n",
      "3                          United States    335,893,238\n",
      "4                              Indonesia    281,603,800\n",
      "..                                   ...            ...\n",
      "235                   Niue (New Zealand)          1,681\n",
      "236                Tokelau (New Zealand)          1,647\n",
      "237                         Vatican City            764\n",
      "238  Cocos (Keeling) Islands (Australia)            593\n",
      "239                Pitcairn Islands (UK)             35\n",
      "\n",
      "[240 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Send a request to Wikipedia page\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_countries_and_dependencies_by_population'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Step 2: Parse the page content using BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Step 3: Find the table containing the data (usually the first table for such lists)\n",
    "table = soup.find('table', {'class': 'wikitable'})\n",
    "\n",
    "# Step 4: Extract data from the table rows\n",
    "countries = []\n",
    "populations = []\n",
    "\n",
    "# Iterate over the table rows\n",
    "for row in table.find_all('tr')[1:]:  # Skip the header row\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 1:\n",
    "        country = cells[1].text.strip()  # The country name is in the second column\n",
    "        population = cells[2].text.strip()  # The population is in the third column\n",
    "        countries.append(country)\n",
    "        populations.append(population)\n",
    "\n",
    "# Step 5: Create a DataFrame to store the results\n",
    "data = pd.DataFrame({\n",
    "    'Country': countries,\n",
    "    'Population': populations\n",
    "})\n",
    "\n",
    "# Display the scraped data\n",
    "print(data)\n",
    "\n",
    "# Optionally save to CSV\n",
    "data.to_csv('../../data/raw-data/countries_population.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{{< include closing.qmd >}} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Race Information for 2000-2023 Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_race_results(url, year, offset, limit=1000):\n",
    "    full_url = f\"{url}/{year}/results.json?limit={limit}&offset={offset}\"\n",
    "    result = requests.get(full_url)\n",
    "    return result.json()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for 2023 season\n",
    "season_2023_json = get_race_results(url='http://ergast.com/api/f1', year=2023, offset=0)\n",
    "\n",
    "# Save the data to a JSON file\n",
    "with open('../../data/raw-data/race_data_2023.json', 'w') as outfile:\n",
    "    json.dump(season_2023_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collecting data from 2000 to 2022\n",
    "\n",
    "# function to loop through years and fetch the results\n",
    "def race_data(start_year, end_year, output_dr, url):\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        race_data = get_race_results(url, year, offset=0)\n",
    "        # save the output \n",
    "        output_file = os.path.join(output_dr, f\"race_data_{year}.json\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(race_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call race_data()\n",
    "race_data(\n",
    "    start_year = 2000,\n",
    "    end_year = 2009,\n",
    "    output_dr = \"../../data/raw-data\",\n",
    "    url = 'http://ergast.com/api/f1'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Standings for 2000-2023 Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def driverstanding_info(url, season):\n",
    "    full_url = f\"{url}/{season}/driverStandings.json\"\n",
    "    response = requests.get(full_url)\n",
    "    return response.json()\n",
    "\n",
    "# Function to fetch and save all driver standings for the given seasons\n",
    "def driverstandings_info(start_year, end_year, output_file, url=\"http://ergast.com/api/f1\"):\n",
    "    driver_standings_data = {}\n",
    "    \n",
    "    for year in range(start_year, end_year + 1):\n",
    "        data = driverstanding_info(url, year)\n",
    "        driver_standings_data[year] = data\n",
    "    \n",
    "    # Save to output file\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(driver_standings_data, outfile)\n",
    "\n",
    "# Call the function for seasons 2000–2023\n",
    "driverstandings_info(\n",
    "    start_year=2000,\n",
    "    end_year=2023,\n",
    "    output_file=\"../../data/raw-data/driver_standings/driver_standings_2000_2023.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Circuit Information for 2000-2023 Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circuit_info(output_file, url):\n",
    "    results = requests.get(url)\n",
    "    \n",
    "    # save to output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(results.json(), f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "circuit_info(output_file='../../data/raw-data/circuit_data.json', url = \"http://ergast.com/api/f1/circuits.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### News of Top 10 drivers in 2024 season (so far)\n",
    "- Using News-API\n",
    "- Resources: https://jfh.georgetown.domains/centralized-lecture-content/content/data-science/data-collection/share/API-newapi/news-api.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseURL = \"https://newsapi.org/v2/everything?\"\n",
    "total_requests=2\n",
    "verbose=True\n",
    "\n",
    "API_KEY='86d4dac5a4864ece92da90bc31277e53'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_data(topic, API_KEY, total_requests=1, verbose=True):\n",
    "    baseURL = \"https://newsapi.org/v2/everything?\"\n",
    "\n",
    "    # API parameters\n",
    "    URLpost = {\n",
    "        'apiKey': API_KEY,\n",
    "        'q': '+'+topic,\n",
    "        'sotBy': 'relevancy',\n",
    "        'pageSize': 100,\n",
    "        'page': 1\n",
    "    }\n",
    "    # last name of the drives to avoid spaces in the file names\n",
    "    file_name = topic.split()[-1]\n",
    "    all_articles = []\n",
    "\n",
    "    # make an API request \n",
    "    for request_num in range(total_requests):\n",
    "        response = requests.get(baseURL, params=URLpost)\n",
    "        response_data = response.json()\n",
    "\n",
    "        articles = response_data.get('articles', [])\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "        URLpost['page'] += 1\n",
    "\n",
    "\n",
    "    # output file path\n",
    "    output_dr = \"../../data/raw-data/News_Drivers\"\n",
    "    output_file = os.path.join(output_dr, f\"{file_name}_raw_text.json\")\n",
    "\n",
    "    # save to output file\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(all_articles, f, indent=4)\n",
    "    \n",
    "    return all_articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 10 Drivers as of Round 22 (Las Vegas Grand Prix)\n",
    "\n",
    "1. Max Verstappen\n",
    "2. Lando Norris\n",
    "3. Charles Leclerc\n",
    "4. Oscar Piastri\n",
    "5. Carlos Sainz\n",
    "6. George Russell\n",
    "7. Lewis Hamilton\n",
    "8. Sergio Perez\n",
    "9. Fernando Alonso\n",
    "10. Nico Hulkenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing \n",
    "text_data = news_data('Max Verstappen', API_KEY, total_requests=1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = news_data('Lando Norris', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Charles Leclerc', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Oscar Piastri', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Carlos Sainz', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('George Russell', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Lewis Hamilton', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Sergio Perez', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Fernando Alonso', API_KEY, total_requests=1, verbose=True)\n",
    "text_data = news_data('Nico Hulkenberg', API_KEY, total_requests=1, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Weather data on the day of the race\n",
    "![](../../data/images/weather_wiki.png)[^1]\n",
    "\n",
    "[^1]: [2010 Bahrain Grand Prix](https://en.wikipedia.org/wiki/2010_Bahrain_Grand_Prix)\n",
    "\n",
    "The weather data will be fetched from the wiki page of each race. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the url for each race is in the race data collected using ergast API\n",
    "race_df = pd.read_csv(\"../../data/processed-data/all_race_results_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data = race_df[['season', 'raceName', 'url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_data = race_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>raceName</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>Bahrain Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Bahrain_Gran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2010</td>\n",
       "      <td>Australian Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Australian_G...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2010</td>\n",
       "      <td>Malaysian Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Malaysian_Gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2010</td>\n",
       "      <td>Chinese Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Chinese_Gran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2010</td>\n",
       "      <td>Spanish Grand Prix</td>\n",
       "      <td>http://en.wikipedia.org/wiki/2010_Spanish_Gran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    season               raceName  \\\n",
       "0     2010     Bahrain Grand Prix   \n",
       "24    2010  Australian Grand Prix   \n",
       "48    2010   Malaysian Grand Prix   \n",
       "72    2010     Chinese Grand Prix   \n",
       "96    2010     Spanish Grand Prix   \n",
       "\n",
       "                                                  url  \n",
       "0   http://en.wikipedia.org/wiki/2010_Bahrain_Gran...  \n",
       "24  http://en.wikipedia.org/wiki/2010_Australian_G...  \n",
       "48  http://en.wikipedia.org/wiki/2010_Malaysian_Gr...  \n",
       "72  http://en.wikipedia.org/wiki/2010_Chinese_Gran...  \n",
       "96  http://en.wikipedia.org/wiki/2010_Spanish_Gran...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching weather for: http://en.wikipedia.org/wiki/2010_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2010_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2010_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2010_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2010_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2006_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2006_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2006_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2006_San_Marino_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2006_European_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2007_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2007_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2007_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2007_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2007_Monaco_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2011_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2011_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2011_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2011_Turkish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2011_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2016_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2016_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2016_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2016_Russian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2016_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2000_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2000_Brazilian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2000_San_Marino_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2000_British_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2000_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2020_Austrian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2020_Styrian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2020_Hungarian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2020_British_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/70th_Anniversary_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2021_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2021_Emilia_Romagna_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2021_Portuguese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2021_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2021_Monaco_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2001_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2001_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2001_Brazilian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2001_San_Marino_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2001_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2017_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2017_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2017_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2017_Russian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2017_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2002_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2002_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2002_Brazilian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2002_San_Marino_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2002_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2014_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2014_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2014_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2014_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2014_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2022_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2022_Saudi_Arabian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2022_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2022_Emilia_Romagna_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2022_Miami_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2018_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2018_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2018_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2018_Azerbaijan_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2018_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2019_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2019_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2019_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2019_Azerbaijan_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2019_Spanish_Grand_Prix\n",
      "Fetching weather for: https://en.wikipedia.org/wiki/2023_Bahrain_Grand_Prix\n",
      "Fetching weather for: https://en.wikipedia.org/wiki/2023_Saudi_Arabian_Grand_Prix\n",
      "Fetching weather for: https://en.wikipedia.org/wiki/2023_Australian_Grand_Prix\n",
      "Fetching weather for: https://en.wikipedia.org/wiki/2023_Azerbaijan_Grand_Prix\n",
      "Fetching weather for: https://en.wikipedia.org/wiki/2023_Miami_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2015_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2015_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2015_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2015_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2015_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2015_Monaco_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2003_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2003_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2003_Brazilian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2003_San_Marino_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2003_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2008_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2008_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2008_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2008_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2008_Turkish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2004_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2004_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2004_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2004_San_Marino_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2004_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2012_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2012_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2012_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2012_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2012_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2013_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2013_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2013_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2013_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2013_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2005_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2005_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2005_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2005_San_Marino_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2005_Spanish_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2005_Monaco_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2009_Australian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2009_Malaysian_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2009_Chinese_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2009_Bahrain_Grand_Prix\n",
      "Fetching weather for: http://en.wikipedia.org/wiki/2009_Spanish_Grand_Prix\n",
      "Updated race data saved to: race_data_with_weather.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_weather_from_wikipedia(url):\n",
    "    response = requests.get(url)\n",
    "    bs = BeautifulSoup(response.text, 'html.parser')  \n",
    "    \n",
    "    # locate the infobox table\n",
    "    table = bs.find('table', {'class': 'infobox infobox-table vevent'})\n",
    "    if not table:\n",
    "        print(f\"No infobox found on the page: {url}\")\n",
    "        return \"Not Available\"\n",
    "    \n",
    "    # search for the \"Weather\" row in the table\n",
    "    for row in table.find_all('tr'):\n",
    "        # find the header cell with class 'infobox-label'\n",
    "        header = row.find('th', {'class': 'infobox-label'})  \n",
    "        # check if it contains \"Weather\"\n",
    "        if header and 'Weather' in header.text:  # Check if it contains \"Weather\"\n",
    "            # find the corresponding data cell with class 'infobox-data'\n",
    "            data = row.find('td', {'class': 'infobox-data'})  \n",
    "            \n",
    "            if data:\n",
    "                return data.text.strip()  \n",
    "    \n",
    "    \n",
    "race_data['weather'] = None\n",
    "\n",
    "# fetch weather information for each URL\n",
    "for index, row in race_data.iterrows():\n",
    "    url = row['url']\n",
    "    # for debuggin purpose\n",
    "    print(f\"Fetching weather for: {url}\")\n",
    "    \n",
    "    # get the weather information\n",
    "    weather = get_weather_from_wikipedia(url)\n",
    "    \n",
    "    # update the weather column\n",
    "    race_data.at[index, 'weather'] = weather\n",
    "\n",
    "# save to output file\n",
    "output_csv = \"../../data/raw-data/weather/race_data_with_weather.csv\"\n",
    "race_data.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Updated race data saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('cache', exist_ok=True)\n",
    "fastf1.Cache.enable_cache('cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_data = []\n",
    "\n",
    "def extract_track_features(year, race_name):\n",
    "    session = fastf1.get_session(year, race_name, 'Q') \n",
    "    session.load()\n",
    "\n",
    "    # Get the fastest lap\n",
    "    fastest_lap = session.laps.pick_fastest()\n",
    "    telemetry = fastest_lap.get_telemetry()\n",
    "\n",
    "    # Track Length\n",
    "    track_length = telemetry['Distance'].iloc[-1]  # Distance of the fastest lap\n",
    "\n",
    "    # Max Speed\n",
    "    max_speed = telemetry['Speed'].max()\n",
    "\n",
    "    # Average Speed\n",
    "    avg_speed = track_length / fastest_lap['LapTime'].total_seconds()\n",
    "\n",
    "    # Percentage of Full Throttle\n",
    "    full_throttle = telemetry[telemetry['Throttle'] >= 95]\n",
    "    perc_full_throttle = (len(full_throttle) / len(telemetry)) * 100\n",
    "\n",
    "    # Number of Corners\n",
    "    telemetry['is_corner'] = telemetry['Speed'] < 100\n",
    "    num_corners = (telemetry['is_corner'] & ~telemetry['is_corner'].shift(1, fill_value=False)).sum()\n",
    "\n",
    "    # Number of Straights\n",
    "    telemetry['is_straight'] = telemetry['Speed'] > 150\n",
    "    num_straights = (telemetry['is_straight'] & ~telemetry['is_straight'].shift(1, fill_value=False)).sum()\n",
    "\n",
    "    return {\n",
    "        \"Year\": year,\n",
    "        \"Grand Prix\": race_name,\n",
    "        \"Track Length (m)\": track_length,\n",
    "        \"Max Speed (km/h)\": max_speed,\n",
    "        \"Full Throttle (%)\": perc_full_throttle,\n",
    "        \"Number of Corners\": num_corners,\n",
    "        \"Number of Straights\": num_straights\n",
    "    }\n",
    "\n",
    "year = 2023\n",
    "schedule = fastf1.get_event_schedule(year)\n",
    "\n",
    "for _, event in schedule.iterrows():  \n",
    "    if not pd.isna(event['Session1']):  \n",
    "        try:\n",
    "            track_features = extract_track_features(year, event['EventName'])\n",
    "            track_data.append(track_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for {event['EventName']} in {year}: {e}\")\n",
    "\n",
    "df_tracks = pd.DataFrame(track_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
